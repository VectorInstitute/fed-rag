{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683e31db-0a58-43c7-bb70-efdbe655a735",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/VectorInstitute/fed-rag/blob/main/docs/notebooks/no_encode_rag_with_mcp.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "_(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c67b9-401e-4565-b082-cdc5abfbe1fe",
   "metadata": {},
   "source": [
    "# Build a NoEncode RAG System with an MCP Knowledge Store\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In traditional RAG systems, there are three components: a retriever, a knowledge store, and a generator. A user's query is encoded by the retriever and used to retrieve relevant knowledge chunks from the knowledge store that had previously been encoded by the retriever as well. The user query along with the retrieved knowledge chunks are passed to the LLM generator to finally respond to the original query.\n",
    "\n",
    "With NoEncode RAG systems, knowledge is still kept in a knowledge store and retrieved for responses to user queries, but there is no encoding step at all. Instead of pre-computing embeddings, NoEncode RAG systems query knowledge sources directly using natural language.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "**Traditional RAG:**\n",
    "- Documents → Embed → Vector Store\n",
    "- Query → Embed → Vector Search → Retrieve → Generate\n",
    "\n",
    "**NoEncode RAG:**\n",
    "- Knowledge Sources (MCP servers, APIs, databases)\n",
    "- Query → Direct Natural Language Query → Retrieve → Generate\n",
    "\n",
    "_**NOTE:** Knowledge sources may be traditional RAG systems themselves, and thus, these would involve encoding. However, the main RAG system does not handle encoding of queries or knowledge chunks at all._\n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "MCP provides a standardized way for AI systems to connect to external tools and data sources. In our NoEncode RAG system, MCP servers act as live knowledge sources that can be queried directly with natural language. An MCP knowledge store acts as the MCP client host that creates connections to these servers and retrieves context from them.\n",
    "\n",
    "### Outline\n",
    "\n",
    "In this cookbook, we will stand up two MCP knowledge sources, use them as part of an MCP knowledge store, and finally build an `AsyncNoEncodeRAGSystem` that allows us to query these sources.\n",
    "\n",
    "1. MCP Knowledge Source 1: an AWS Kendra Index MCP Server\n",
    "2. MCP Knowledge Source 2: a LlamaCloud MCP Server\n",
    "3. Create an MCP Knowledge Store (using our two built sources)\n",
    "4. Assemble a NoEncode RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d1bc6-9b84-490a-8697-d502c0f5821d",
   "metadata": {},
   "source": [
    "## MCP Knowledge Source 1: an AWS Kendra Index MCP Server\n",
    "\n",
    "Here, we make use of one the myriad of officially supported [AWS MCP servers](https://github.com/awslabs/mcp?tab=readme-ov-file#available-servers) offered by [AWS Labs](https://github.com/awslabs), namely: their [AWS Kendra Index MCP Server](https://github.com/awslabs/mcp/tree/main/src/amazon-kendra-index-mcp-server).\n",
    "\n",
    "AWS Kendra is an enterprise search service powered by machine learning. It can search across various data sources including documents, FAQs, knowledge bases, and websites, providing intelligent answers to natural language queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e12826b-9652-4ac7-a3a7-d565f216eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from mcp import StdioServerParameters\n",
    "from fed_rag.knowledge_stores.no_encode import MCPStdioKnowledgeSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea63aa3-1c3f-4953-a36b-72120c656912",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_params = StdioServerParameters(\n",
    "    command=\"docker\",\n",
    "    args=[\n",
    "        \"run\",\n",
    "        \"--rm\",\n",
    "        \"--interactive\",\n",
    "        \"--init\",  # important!\n",
    "        \"--env-file\",\n",
    "        f\"{os.getcwd()}/.env\",\n",
    "        \"awslabs/amazon-kendra-index-mcp-server:latest\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "mcp_source = MCPStdioKnowledgeSource(\n",
    "    name=\"awslabs.amazon-kendra-index-mcp-server\",\n",
    "    server_params=server_params,\n",
    "    tool_name=\"KendraQueryTool\",\n",
    "    query_param_name=\"query\",\n",
    "    tool_call_kwargs={\n",
    "        \"indexId\": \"572aca26-16be-44df-84d3-4d96d778f120\",\n",
    "        \"region\": \"ca-central-1\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ed95a-90ee-40fb-8c34-a20a4f3fcdfa",
   "metadata": {},
   "source": [
    "### Using the default converter of `~mcp.CallToolResult` to `~fed_rag.KnowledgeNode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7f783f-9b70-4960-9944-896f67398ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_tool_result = await mcp_source.retrieve(\"What is RAFT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6528d2-ec2f-4cce-8d4c-928ffa20d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results returned:  1 \n",
      "\n",
      "Text content of first returned node:\n",
      " {\"query\": \"What is RAFT?\", \"total_results_count\": 4, \"results\": [{\"id\": \"4fd36a75-4a08-4c9d-9d98-0e92049ece06-2f7adc0b-3250-47c9-976d-dc4c01eff138\", \"type\": \"ANSWER\", \"document_title\": \"raft.pdf\", \"document_uri\": \"https://fed-rag-mcp-cookbook.s3.ca-central-1.amazonaws.com/raft.pdf\", \"score\": \"HIGH\", \"excerpt\": \"In this paper, we present Retrieval Augmented\\nFine Tuning (RAFT), a training recipe which improves the model\\u2019s ability\\nto answer questions in \\\"open-book\\\" in-domain settings. In t\n"
     ]
    }
   ],
   "source": [
    "knowledge_nodes = mcp_source.call_tool_result_to_knowledge_nodes_list(\n",
    "    call_tool_result\n",
    ")\n",
    "\n",
    "print(\"Number of results returned: \", len(knowledge_nodes), \"\\n\")\n",
    "print(\n",
    "    \"Text content of first returned node:\\n\",\n",
    "    knowledge_nodes[0].text_content[:500],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab5fe3-c92a-4f3a-acf7-5d00f79ccab9",
   "metadata": {},
   "source": [
    "As we can see, this is not really the most ideal conversion. We should probably only pass the excerpt text content to the LLM generator. Thus, we should define our own converter function to extract only the text content.\n",
    "\n",
    "According to the [source code](https://github.com/awslabs/mcp/blob/36ae951bb8cfed234f69f6336fe2463eb4e08587/src/amazon-kendra-index-mcp-server/awslabs/amazon_kendra_index_mcp_server/server.py#L150) for this server, we see that a successful tool call of `KendraQueryTool` will yield a `CallToolResult` whose `text` attribute is a JSON string containing a `results` key. The value for `results` is a list of `result_items` each containing an `excerpt` field which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680ec887-bbb7-4794-b541-468ee478c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from typing import Any\n",
    "from mcp.types import CallToolResult\n",
    "from fed_rag.data_structures import KnowledgeNode\n",
    "\n",
    "\n",
    "def kendra_index_converter_fn(\n",
    "    result: CallToolResult, metadata: dict[str, Any] | None = None\n",
    ") -> list[KnowledgeNode]:\n",
    "    nodes = []\n",
    "    for c in result.content:\n",
    "        if c.type == \"text\":\n",
    "            data = json.loads(c.text)\n",
    "            for res in data[\"results\"]:\n",
    "                text_content = re.sub(r\"\\s+\", \" \", res[\"excerpt\"].strip())\n",
    "                nodes.append(\n",
    "                    KnowledgeNode(\n",
    "                        node_type=\"text\",\n",
    "                        text_content=text_content,\n",
    "                        metadata=metadata,\n",
    "                    )\n",
    "                )\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df90d4-a49f-4a45-b971-aee6f0405ec4",
   "metadata": {},
   "source": [
    "Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce2cdd94-d4d8-470e-b3c6-be72a04054c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results returned:  4 \n",
      "\n",
      "Text content of first returned node:\n",
      " In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t \n",
      "\n",
      "Text content of second returned node:\n",
      " 3 RAFT In this section, we present RAFT, a novel way of training LLMs for domain-specific open- book exams. We first introduce the classical technique of supervised fine-tuning, followed with the key takeaways from our experiments. Then, we introduce RAFT , a modified version of general instructio\n"
     ]
    }
   ],
   "source": [
    "knowledge_nodes = kendra_index_converter_fn(call_tool_result)\n",
    "\n",
    "print(\"Number of results returned: \", len(knowledge_nodes), \"\\n\")\n",
    "print(\n",
    "    \"Text content of first returned node:\\n\",\n",
    "    knowledge_nodes[0].text_content[:500],\n",
    "    \"\\n\",\n",
    ")\n",
    "print(\n",
    "    \"Text content of second returned node:\\n\",\n",
    "    knowledge_nodes[1].text_content[:500],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029904c-5362-4a1f-a915-9f4bced79a75",
   "metadata": {},
   "source": [
    "This is better than we had before with the default converter, so let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b4c2558-423b-4c50-a469-3ee131a85938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the converter function\n",
    "mcp_source = mcp_source.with_converter(kendra_index_converter_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c73fe-4a94-4a92-a977-1286a4671a00",
   "metadata": {},
   "source": [
    "## MCP Knowledge Source 2: a LlamaCloud MCP Server\n",
    "\n",
    "In this part of the cookbook, we'll stand up an MCP server using [LlamaCloud](https://docs.llamaindex.ai/en/stable/llama_cloud/)—an enterprise solution by LlamaIndex—by following their MCP [demo](https://github.com/run-llama/llamacloud-mcp?tab=readme-ov-file#llamacloud-as-an-mcp-server).\n",
    "\n",
    "LlamaCloud provides document parsing, indexing, and retrieval capabilities. By exposing these through an MCP server, we can query processed documents directly using natural language without managing our own document processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abf9af79-f756-4bb7-8d1b-d9bd0262c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_cloud_server_params = StdioServerParameters(\n",
    "    command=\"sh\",\n",
    "    args=[\n",
    "        \"-c\",\n",
    "        \"cd /home/nerdai/OSS/llamacloud-mcp && poetry install && exec poetry run python mcp-server.py\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "llama_cloud_mcp_source = MCPStdioKnowledgeSource(\n",
    "    name=\"llama-index-server\",\n",
    "    server_params=llama_cloud_server_params,\n",
    "    tool_name=\"LlamaCloudQueryTool\",\n",
    "    query_param_name=\"query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c75ad4-b7f1-494e-a3de-c27bcf01fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await llama_cloud_mcp_source.retrieve(\"What is RALT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72968ea9-a063-4a75-b64e-755723fabedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CallToolResult(meta=None, content=[TextContent(type='text', text='RALT stands for Retrieval-Augmented Language Model. It is a model that combines traditional language models with a retrieval mechanism to enhance performance on various natural language processing tasks. By incorporating a retrieval component, RALT can access external knowledge sources to improve its understanding and generation of text.\\n\\nHere is an example of how a Retrieval-Augmented Language Model (RALT) can be implemented using Python and the Hugging Face Transformers library:\\n\\n```python\\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\n\\n# Load the RALT model and tokenizer\\nmodel_name = \"your_RALT_model_name\"\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\n# Define a prompt\\nprompt = \"Your prompt here\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text using the RALT model\\noutput = model.generate(**inputs)\\n\\n# Decode the output tokens back to text\\ndecoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\\nprint(decoded_output)\\n```\\n\\nIn this code snippet, you can replace `\"your_RALT_model_name\"` with the specific RALT model you want to use. Then, you can provide a prompt, tokenize it, generate text using the RALT model, and decode the output to see the generated text based on the prompt.', annotations=None)], isError=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b647e6c5-5944-40b5-a68a-67b235cd30d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results returned:  1 \n",
      "\n",
      "Text content of first returned node:\n",
      " RALT stands for Retrieval-Augmented Language Model. It is a model that combines traditional language models with a retrieval mechanism to enhance performance on various natural language processing tasks. By incorporating a retrieval component, RALT can access external knowledge sources to improve its understanding and generation of text.\n",
      "\n",
      "Here is an example of how a Retrieval-Augmented Language Model (RALT) can be implemented using Python and the Hugging Face Transformers library:\n",
      "\n",
      "```python\n",
      "fro\n"
     ]
    }
   ],
   "source": [
    "knowledge_nodes = (\n",
    "    llama_cloud_mcp_source.call_tool_result_to_knowledge_nodes_list(res)\n",
    ")\n",
    "\n",
    "print(\"Number of results returned: \", len(knowledge_nodes), \"\\n\")\n",
    "print(\n",
    "    \"Text content of first returned node:\\n\",\n",
    "    knowledge_nodes[0].text_content[:500],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb002cfc-17eb-4dae-bb2c-9240f220866d",
   "metadata": {},
   "source": [
    "## Create an MCP Knowledge Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0264333-a34e-46ec-8c20-8b4faf6d8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.knowledge_stores.no_encode import MCPKnowledgeStore\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52b4957c-a70c-4d29-8151-ef7414db31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranker_callback(\n",
    "    nodes: list[KnowledgeNode], query: str\n",
    ") -> list[tuple[float, KnowledgeNode]]:\n",
    "    model = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L2\")\n",
    "    # Concatenate the query and all passages and predict the scores for the pairs [query, passage]\n",
    "    model_inputs = [[query, n.text_content] for n in nodes]\n",
    "    scores = model.predict(model_inputs)\n",
    "\n",
    "    # Sort the scores in decreasing order\n",
    "    results = [(score, node) for score, node in zip(scores, nodes)]\n",
    "    return sorted(results, key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "547a02e4-bcee-4d99-b14a-6ddd1ced685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_store = (\n",
    "    MCPKnowledgeStore()\n",
    "    .add_source(mcp_source)\n",
    "    .add_source(llama_cloud_mcp_source)\n",
    "    .with_reranker(reranker_callback)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885761e1-dc1a-4ef5-8b01-f33083ff01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await knowledge_store.retrieve(\"What is RAFT?\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c33e2f1c-5507-4779-ae8b-faabe11f9a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7772398,\n",
       "  KnowledgeNode(node_id='ca5afffd-ec50-4900-b25c-56f2e5b26e98', embedding=None, node_type=<NodeType.TEXT: 'text'>, text_content='...Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which...', image_content=None, metadata={'name': 'awslabs.amazon-kendra-index-mcp-server', 'tool_name': 'KendraQueryTool', 'query_param_name': 'query', 'tool_call_kwargs': {'indexId': '572aca26-16be-44df-84d3-4d96d778f120', 'region': 'ca-central-1'}, 'server_params': {'command': 'docker', 'args': ['run', '--rm', '--interactive', '--init', '--env-file', '/home/nerdai/Projects/fed-rag/docs/notebooks/.env', 'awslabs/amazon-kendra-index-mcp-server:latest'], 'env': None, 'cwd': None, 'encoding': 'utf-8', 'encoding_error_handler': 'strict'}})),\n",
       " (0.12683225,\n",
       "  KnowledgeNode(node_id='acf96074-f35c-448b-9868-8b132136331f', embedding=None, node_type=<NodeType.TEXT: 'text'>, text_content='In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t', image_content=None, metadata={'name': 'awslabs.amazon-kendra-index-mcp-server', 'tool_name': 'KendraQueryTool', 'query_param_name': 'query', 'tool_call_kwargs': {'indexId': '572aca26-16be-44df-84d3-4d96d778f120', 'region': 'ca-central-1'}, 'server_params': {'command': 'docker', 'args': ['run', '--rm', '--interactive', '--init', '--env-file', '/home/nerdai/Projects/fed-rag/docs/notebooks/.env', 'awslabs/amazon-kendra-index-mcp-server:latest'], 'env': None, 'cwd': None, 'encoding': 'utf-8', 'encoding_error_handler': 'strict'}}))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d4539-3e89-4632-b4f3-3c892b14f4a5",
   "metadata": {},
   "source": [
    "## Assemble a NoEncode RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d626460-d28f-488f-8bb1-738cf389e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.generators.huggingface import HFPretrainedModelGenerator\n",
    "import torch\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "generation_cfg = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    eos_token_id=151643,\n",
    "    bos_token_id=151643,\n",
    "    max_new_tokens=2048,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "    cache_implementation=\"offloaded\",\n",
    "    stop_strings=\"</response>\",\n",
    ")\n",
    "generator = HFPretrainedModelGenerator(\n",
    "    model_name=\"Qwen/Qwen2.5-3B\",\n",
    "    load_model_at_init=False,\n",
    "    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n",
    "    generation_config=generation_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9d24ee6-b734-489b-8750-39aea560b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag import AsyncNoEncodeRAGSystem, RAGConfig\n",
    "\n",
    "rag_config = RAGConfig(top_k=2)\n",
    "rag_system = AsyncNoEncodeRAGSystem(\n",
    "    knowledge_store=knowledge_store,\n",
    "    generator=generator,\n",
    "    rag_config=rag_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "223b69b6-4d05-49e7-9952-6f05c7257f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4c6190fa1e4eb9b9e32d29261195f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "res = await rag_system.query(query=\"What is RAFT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c86d1fcd-501b-43f7-911d-5578a02f8218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAFT stands for Retrieval-Augmented Fine-Tuning. It is a technique used in natural language processing that combines the power of retrieval-based methods with fine-tuning language models. This approach involves augmenting language models with retrieved information from external sources to enhance their performance on various NLP tasks. In RAFT, a language model is fine-tuned on a specific task while also incorporating information retrieved from external knowledge sources. This retrieved information helps the model to better understand the context of the task and make more informed predictions. RAFT can be implemented using Python and Hugging Face's Transformers library.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2291134b-b993-465e-98c0-c75e47e2963f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SourceNode(score=0.7772397994995117, node=KnowledgeNode(node_id='3f170eec-31a5-4afb-a92e-ae8f8414c476', embedding=None, node_type=<NodeType.TEXT: 'text'>, text_content='...Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which...', image_content=None, metadata={'name': 'awslabs.amazon-kendra-index-mcp-server', 'tool_name': 'KendraQueryTool', 'query_param_name': 'query', 'tool_call_kwargs': {'indexId': '572aca26-16be-44df-84d3-4d96d778f120', 'region': 'ca-central-1'}, 'server_params': {'command': 'docker', 'args': ['run', '--rm', '--interactive', '--init', '--env-file', '/home/nerdai/Projects/fed-rag/docs/notebooks/.env', 'awslabs/amazon-kendra-index-mcp-server:latest'], 'env': None, 'cwd': None, 'encoding': 'utf-8', 'encoding_error_handler': 'strict'}})),\n",
       " SourceNode(score=0.6162377595901489, node=KnowledgeNode(node_id='81a14c06-1a90-4391-bb63-2e4ac07d258a', embedding=None, node_type=<NodeType.TEXT: 'text'>, text_content='RAFT stands for Retrieval-Augmented Fine-Tuning. It is a technique used in natural language processing that combines the power of retrieval-based methods with fine-tuning language models. This approach involves augmenting language models with retrieved information from external sources to enhance their performance on various NLP tasks.\\n\\nIn RAFT, a language model is fine-tuned on a specific task while also incorporating information retrieved from external knowledge sources. This retrieved information helps the model to better understand the context of the task and make more informed predictions.\\n\\nHere is an example of how RAFT can be implemented using Python and Hugging Face\\'s Transformers library:\\n\\n```python\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\nfrom transformers import pipeline\\n\\n# Load the pre-trained RAFT model\\nmodel_name = \"username/raft-model\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\n\\n# Define the task and input text\\ntask = \"question_answering\"\\ntext = \"What is the capital of France?\"\\n\\n# Use the RAFT model for inference\\nraft_pipeline = pipeline(task, model=model, tokenizer=tokenizer)\\nresult = raft_pipeline(text)\\n\\nprint(result)\\n```\\n\\nIn this code snippet, we load a pre-trained RAFT model using Hugging Face\\'s Transformers library and then use it to perform question answering on the input text \"What is the capital of France?\". The RAFT model combines fine-tuning with retrieval-based methods to provide accurate answers based on the retrieved information.\\n\\nOverall, RAFT is a powerful technique that leverages external knowledge to enhance the performance of language models on a wide range of NLP tasks.', image_content=None, metadata={'name': 'llama-index-server', 'tool_name': 'LlamaCloudQueryTool', 'query_param_name': 'query', 'tool_call_kwargs': {}, 'server_params': {'command': 'sh', 'args': ['-c', 'cd /home/nerdai/OSS/llamacloud-mcp && poetry install && exec poetry run python mcp-server.py'], 'env': None, 'cwd': None, 'encoding': 'utf-8', 'encoding_error_handler': 'strict'}}))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.source_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
