{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683e31db-0a58-43c7-bb70-efdbe655a735",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/VectorInstitute/fed-rag/blob/main/docs/notebooks/no_encode_rag_with_mcp.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "_(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c67b9-401e-4565-b082-cdc5abfbe1fe",
   "metadata": {},
   "source": [
    "# Build a NoEncode RAG System with an MCP Knowledge Store\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In traditional RAG systems, there are three components: a retriever, a knowledge store, and a generator. A user's query is encoded by the retriever and used to retrieve relevant knowledge chunks from the knowledge store that had previously been encoded by the retriever as well. The user query along with the retrieved knowledge chunks are passed to the LLM generator to finally respond to the original query.\n",
    "\n",
    "With NoEncode RAG systems, knowledge is still kept in a knowledge store and retrieved for responses to user queries, but there is no encoding step at all. Instead of pre-computing embeddings, NoEncode RAG systems query knowledge sources directly using natural language.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "**Traditional RAG:**\n",
    "- Documents → Embed → Vector Store\n",
    "- Query → Embed → Vector Search → Retrieve → Generate\n",
    "\n",
    "**NoEncode RAG:**\n",
    "- Knowledge Sources (MCP servers, APIs, databases)\n",
    "- Query → Direct Natural Language Query → Retrieve → Generate\n",
    "\n",
    "_**NOTE:** Knowledge sources may be traditional RAG systems themselves, and thus, these would involve encoding. However, the main RAG system does not handle encoding of queries or knowledge chunks at all._\n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "MCP provides a standardized way for AI systems to connect to external tools and data sources. In our NoEncode RAG system, MCP servers act as live knowledge sources that can be queried directly with natural language. An MCP knowledge store acts as the MCP client host that creates connections to these servers and retrieves context from them.\n",
    "\n",
    "### Outline\n",
    "\n",
    "In this cookbook, we will stand up two MCP knowledge sources, use them as part of an MCP knowledge store, and finally build an `AsyncNoEncodeRAGSystem` that allows us to query these sources.\n",
    "\n",
    "1. MCP Knowledge Source 1: an AWS Kendra Index MCP Server\n",
    "2. MCP Knowledge Source 2: a LlamaCloud MCP Server\n",
    "3. Create an MCP Knowledge Store (using our two built sources)\n",
    "4. Assemble a NoEncode RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d1bc6-9b84-490a-8697-d502c0f5821d",
   "metadata": {},
   "source": [
    "## MCP Knowledge Source 1: an AWS Kendra Index MCP Server\n",
    "\n",
    "Here, we make use of one the myriad of officially supported [AWS MCP servers](https://github.com/awslabs/mcp?tab=readme-ov-file#available-servers) offered by [AWS Labs](https://github.com/awslabs), namely: their [AWS Kendra Index MCP Server](https://github.com/awslabs/mcp/tree/main/src/amazon-kendra-index-mcp-server).\n",
    "\n",
    "AWS Kendra is an enterprise search service powered by machine learning. It can search across various data sources including documents, FAQs, knowledge bases, and websites, providing intelligent answers to natural language queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a875a-5ad8-4da8-b24e-1d02e1c676b3",
   "metadata": {},
   "source": [
    "### Pre-requisite Steps\n",
    "\n",
    "#### Create a Kendra Index\n",
    "\n",
    "To be able to use this MCP server, you need to create a new Kendra Index. Add a S3 data connector to it that has the [RAFT paper](https://arxiv.org/pdf/2403.10131)—make sure to sync your index so that its ready to be queried with the RAFT paper. Finally, fill in the information below for regarding your Kendra Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c21b19-316c-43af-ba7a-cd20069d97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info regarding your kendra index which needs to be passed to the MCP tool call\n",
    "kendra_index_info = {\n",
    "    \"indexId\": \"572aca26-16be-44df-84d3-4d96d778f120\",\n",
    "    \"region\": \"ca-central-1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86292b85-9bb3-48d5-b3f9-60885c3fdaf7",
   "metadata": {},
   "source": [
    "#### Build the Kendra Index MCP Server Docker image\n",
    "\n",
    "With our Kendra index in hand, we now are able to build a local MCP server that would interact with it. To do this, we take the following steps\n",
    "\n",
    "1. Clone the `awslabs/mcp` Github repo:\n",
    "\n",
    "```sh\n",
    "git clone https://github.com/awslabs/mcp.gi\n",
    "```\n",
    "\n",
    "2. cd into the Kendra index src directory:\n",
    "\n",
    "```sh\n",
    "cd mcp/src/amazon-kendra-index-mcp-server\n",
    "```\n",
    "\n",
    "3. Locally build the Docker image\n",
    "\n",
    "```sh\n",
    "docker build -t awslabs/amazon-kendra-index-mcp-server .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c55ae-077a-4670-9b68-a3746142b88e",
   "metadata": {},
   "source": [
    "#### Configure AWS Credentials\n",
    "\n",
    "Create a `.env` file in the same directory as this notebook, with your AWS credentials:\n",
    "\n",
    "```sh\n",
    "# .env file\n",
    "AWS_ACCESS_KEY_ID=...\n",
    "AWS_SECRET_ACCESS_KEY=...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d44ae2-2a58-401e-a18e-d57046e16bac",
   "metadata": {},
   "source": [
    "### Build the MCP Stdio Knowledge Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e12826b-9652-4ac7-a3a7-d565f216eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mcp import StdioServerParameters\n",
    "from fed_rag.knowledge_stores.no_encode import MCPStdioKnowledgeSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea63aa3-1c3f-4953-a36b-72120c656912",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_params = StdioServerParameters(\n",
    "    command=\"docker\",\n",
    "    args=[\n",
    "        \"run\",\n",
    "        \"--rm\",\n",
    "        \"--interactive\",\n",
    "        \"--init\",  # important to have in Jupyter Notebook\n",
    "        \"--env-file\",\n",
    "        f\"{os.getcwd()}/.env\",\n",
    "        \"awslabs/amazon-kendra-index-mcp-server:latest\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "mcp_source = MCPStdioKnowledgeSource(\n",
    "    name=\"awslabs.amazon-kendra-index-mcp-server\",\n",
    "    server_params=server_params,\n",
    "    tool_name=\"KendraQueryTool\",\n",
    "    query_param_name=\"query\",\n",
    "    tool_call_kwargs=kendra_index_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c868b09-3fc0-4387-b49e-42cdce0e9268",
   "metadata": {},
   "source": [
    "Let's test out our new MCP source by invoking the `retrieve()` method with a specific query. This will return an `~mcp.CallToolResult` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7f783f-9b70-4960-9944-896f67398ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_tool_result = await mcp_source.retrieve(\"What is RAFT?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ed95a-90ee-40fb-8c34-a20a4f3fcdfa",
   "metadata": {},
   "source": [
    "#### Converting MCP Call Tool Results to Knowledge Nodes\n",
    "\n",
    "MCP tool results are automatically converted to `KnowledgeNode` objects using a default converter in `MCPStdioKnowledgeSource`. This generic converter works for basic use cases but may not extract all valuable information from server-specific responses. Implement a custom converter to optimize knowledge extraction for your particular MCP server. Let's see the default converter in action first and determine if we need to create our own converter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6528d2-ec2f-4cce-8d4c-928ffa20d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of knowledge nodes created:  1 \n",
      "\n",
      "Text content of first created node:\n",
      " {\"query\": \"What is RAFT?\", \"total_results_count\": 4, \"results\": [{\"id\": \"a7439424-ac28-4f3c-9d94-f1c041e57541-8698aea8-20ac-49f2-9a79-298992bd1bd9\", \"type\": \"ANSWER\", \"document_title\": \"raft.pdf\", \"document_uri\": \"https://fed-rag-mcp-cookbook.s3.ca-central-1.amazonaws.com/raft.pdf\", \"score\": \"HIGH\", \"excerpt\": \"In this paper, we present Retrieval Augmented\\nFine Tuning (RAFT), a training recipe which improves the model\\u2019s ability\\nto answer questions in \\\"open-book\\\" in-domain settings. In t\n"
     ]
    }
   ],
   "source": [
    "# using the default converter function\n",
    "knowledge_nodes = mcp_source.call_tool_result_to_knowledge_nodes_list(\n",
    "    call_tool_result\n",
    ")\n",
    "\n",
    "print(\"Number of knowledge nodes created: \", len(knowledge_nodes), \"\\n\")\n",
    "print(\n",
    "    \"Text content of first created node:\\n\",\n",
    "    knowledge_nodes[0].text_content[:500],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab5fe3-c92a-4f3a-acf7-5d00f79ccab9",
   "metadata": {},
   "source": [
    "As we can see, this is not really the most ideal conversion. We should probably only pass the `excerpt` text content to the LLM generator. Thus, we should define our own converter function to extract only the text content.\n",
    "\n",
    "According to the [source code](https://github.com/awslabs/mcp/blob/36ae951bb8cfed234f69f6336fe2463eb4e08587/src/amazon-kendra-index-mcp-server/awslabs/amazon_kendra_index_mcp_server/server.py#L150) for this server, we see that a successful tool call of `KendraQueryTool` will return a `CallToolResult` whose `text` attribute is a JSON string containing a `results` key. The value for `results` is a list of `result_items` each containing an `excerpt` field, which is ultimately what we want to pass to the LLM generator.\n",
    "\n",
    "Let's create a custom converter function to do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "680ec887-bbb7-4794-b541-468ee478c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from typing import Any\n",
    "from mcp.types import CallToolResult\n",
    "from fed_rag.data_structures import KnowledgeNode\n",
    "\n",
    "\n",
    "# signature of a converter function\n",
    "def kendra_index_converter_fn(\n",
    "    result: CallToolResult, metadata: dict[str, Any] | None = None\n",
    ") -> list[KnowledgeNode]:\n",
    "    nodes = []\n",
    "    for c in result.content:\n",
    "        if c.type == \"text\":  # only use ~mcp.TextContent\n",
    "            data = json.loads(c.text)\n",
    "            for res in data[\"results\"]:\n",
    "                # take only the content in the \"excerpt\" key\n",
    "                text_content = re.sub(r\"\\s+\", \" \", res[\"excerpt\"].strip())\n",
    "                nodes.append(\n",
    "                    # create the knowledge node\n",
    "                    KnowledgeNode(\n",
    "                        node_type=\"text\",\n",
    "                        text_content=text_content,\n",
    "                        metadata=metadata,\n",
    "                    )\n",
    "                )\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df90d4-a49f-4a45-b971-aee6f0405ec4",
   "metadata": {},
   "source": [
    "Let's test out our custom converter as a standalone function on the previously obtained `call_tool_result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2cdd94-d4d8-470e-b3c6-be72a04054c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of knowledge nodes created:  4 \n",
      "\n",
      "Text content of first created node:\n",
      " In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t \n",
      "\n",
      "Text content of second created node:\n",
      " 3 RAFT In this section, we present RAFT, a novel way of training LLMs for domain-specific open- book exams. We first introduce the classical technique of supervised fine-tuning, followed with the key takeaways from our experiments. Then, we introduce RAFT , a modified version of general instructio\n"
     ]
    }
   ],
   "source": [
    "knowledge_nodes = kendra_index_converter_fn(call_tool_result)\n",
    "\n",
    "print(\"Number of knowledge nodes created: \", len(knowledge_nodes), \"\\n\")\n",
    "print(\n",
    "    \"Text content of first created node:\\n\",\n",
    "    knowledge_nodes[0].text_content[:500],\n",
    "    \"\\n\",\n",
    ")\n",
    "print(\n",
    "    \"Text content of second created node:\\n\",\n",
    "    knowledge_nodes[1].text_content[:500],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029904c-5362-4a1f-a915-9f4bced79a75",
   "metadata": {},
   "source": [
    "This much improved and should work better when passing down as context to the LLM generator. We can update our `mcp_source` to use this converter function easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4c2558-423b-4c50-a469-3ee131a85938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the converter function\n",
    "mcp_source = mcp_source.with_converter(kendra_index_converter_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c73fe-4a94-4a92-a977-1286a4671a00",
   "metadata": {},
   "source": [
    "## MCP Knowledge Source 2: a LlamaCloud MCP Server\n",
    "\n",
    "In this part of the cookbook, we'll stand up an MCP server using [LlamaCloud](https://docs.llamaindex.ai/en/stable/llama_cloud/)—an enterprise solution by LlamaIndex—by following their MCP [demo](https://github.com/run-llama/llamacloud-mcp?tab=readme-ov-file#llamacloud-as-an-mcp-server).\n",
    "\n",
    "LlamaCloud provides document parsing, indexing, and retrieval capabilities. By exposing these through an MCP server, we can query processed documents directly using natural language without managing our own document processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98cdfa-57eb-40cf-bbc6-ac5ce6651fb1",
   "metadata": {},
   "source": [
    "### Pre-requisite Steps\n",
    "\n",
    "The steps below follow from the setup instructions listed in the Github repo for the [llamacloud-mcp demo](https://github.com/run-llama/llamacloud-mcp).\n",
    "\n",
    "This requires the creation of a new LlamaCloud account. If you don't have one, then you create one by visiting <https://cloud.llamaindex.ai/>.\n",
    "\n",
    "#### Create a LlamaCloud Index\n",
    "\n",
    "Login to LlamaCloud with your account and navigate to \"Tool\" > \"Index\" in the left side-bar. Click the \"Create Index\" button to create a new index. After creating the new index, upload the [RA-DIT paper](https://arxiv.org/pdf/2310.01352).\n",
    "\n",
    "__NOTE__: You'll need to supply information on your new index in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d069d-0579-47b6-9fd1-a3bed542f83d",
   "metadata": {},
   "source": [
    "#### Create a local MCP server to connect with the LlamaCloud Index\n",
    "\n",
    "1. Clone the `llamacloud-mcp` demo Github repo:\n",
    "\n",
    "```sh\n",
    "https://github.com/run-llama/llamacloud-mcp.git\n",
    "```\n",
    "\n",
    "2. cd into `llamacloud-mcp` directory\n",
    "\n",
    "```sh\n",
    "cd llamacloud-mcp\n",
    "```\n",
    "\n",
    "3. Update the `mcp-server.py`\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mcp = FastMCP('llama-index-server')\n",
    "\n",
    "@mcp.tool(name='LlamaCloudQueryTool')\n",
    "def llama_index_query(query: str) -> str:\n",
    "    \"\"\"Search the llama-index documentation for the given query.\"\"\"\n",
    "\n",
    "    index = LlamaCloudIndex(\n",
    "        name=\"<your-llamacloud-index-name>\", \n",
    "        project_name=\"Default\",  # change this if you didn't use default project name\n",
    "        organization_id=\"<your-llamacloud-org-id>\",\n",
    "        api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    response = index.as_query_engine().query(query + \" Be verbose and include code examples.\")\n",
    "\n",
    "    return str(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")\n",
    "```\n",
    "\n",
    "4. Create a `.env` file in the `llamacloud-mcp` directory\n",
    "\n",
    "```sh\n",
    "# .env\n",
    "LLAMA_CLOUD_API_KEY=<your-llamacloud-api-key>\n",
    "OPENAI_API_KEY=<your-openai-api-key>\n",
    "```\n",
    "\n",
    "__NOTE__: This llamacloud-mcp demo builds a `~llama_index.QueryEngine()` which by default uses an OpenAI LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0f7b9-1a14-45f8-9e7d-6ffc9d9a82f3",
   "metadata": {},
   "source": [
    "### Build the MCP Stdio Knowledge Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120d2d32-21e2-410d-b1e1-3d118336b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to your actual path\n",
    "path_to_llamacloud_mcp = \"/home/nerdai/OSS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf9af79-f756-4bb7-8d1b-d9bd0262c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_cloud_server_params = StdioServerParameters(\n",
    "    command=\"sh\",\n",
    "    args=[\n",
    "        \"-c\",\n",
    "        f\"cd {path_to_llamacloud_mcp}/llamacloud-mcp && poetry install && exec poetry run python mcp-server.py\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "llama_cloud_mcp_source = MCPStdioKnowledgeSource(\n",
    "    name=\"llama-index-server\",\n",
    "    server_params=llama_cloud_server_params,\n",
    "    tool_name=\"LlamaCloudQueryTool\",\n",
    "    query_param_name=\"query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c75ad4-b7f1-494e-a3de-c27bcf01fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await llama_cloud_mcp_source.retrieve(\"What is RALT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72968ea9-a063-4a75-b64e-755723fabedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CallToolResult(meta=None, content=[TextContent(type='text', text='RALT stands for Retrieval-Augmented Language Model. It refers to a type of language model that incorporates a retrieval mechanism to enhance its performance on various natural language processing tasks. By retrieving relevant information from a knowledge source, such as a large text corpus or a database, the language model can better understand and generate responses to queries.\\n\\nHere is an example of how a Retrieval-Augmented Language Model (RALT) can be implemented using Python with the Hugging Face Transformers library:\\n\\n```python\\nfrom transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\\n\\n# Initialize the RAG tokenizer\\ntokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\\n\\n# Initialize the RAG retriever\\nretriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")\\n\\n# Initialize the RAG token model for generation\\nmodel = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\\n\\n# Query input\\nquery = \"What is the capital of France?\"\\n\\n# Encode the query\\ninput_dict = tokenizer(query, return_tensors=\"pt\")\\n\\n# Generate a response using the RALT model\\noutput = model.generate(input_ids=input_dict[\"input_ids\"], attention_mask=input_dict[\"attention_mask\"])\\n\\n# Decode and print the generated response\\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\\nprint(response)\\n```\\n\\nIn this code snippet, we first load the RAG tokenizer, retriever, and token model for generation. We then provide a query, encode it using the tokenizer, generate a response using the RALT model, and finally decode and print the generated response. This demonstrates how a Retrieval-Augmented Language Model can be used to answer questions by leveraging retrieved information.', annotations=None)], isError=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b647e6c5-5944-40b5-a68a-67b235cd30d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results returned:  1 \n",
      "\n",
      "Text content of first returned node:\n",
      " RALT stands for Retrieval-Augmented Language Model. It refers to a type of language model that incorporates a retrieval mechanism to enhance its performance on various natural language processing tasks. By retrieving relevant information from a knowledge source, such as a large text corpus or a database, the language model can better understand and generate responses to queries.\n",
      "\n",
      "Here is an example of how a Retrieval-Augmented Language Model (RALT) can be implemented using Python with the Huggin\n"
     ]
    }
   ],
   "source": [
    "knowledge_nodes = (\n",
    "    llama_cloud_mcp_source.call_tool_result_to_knowledge_nodes_list(res)\n",
    ")\n",
    "\n",
    "print(\"Number of results returned: \", len(knowledge_nodes), \"\\n\")\n",
    "print(\n",
    "    \"Text content of first returned node:\\n\",\n",
    "    knowledge_nodes[0].text_content[:500],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb002cfc-17eb-4dae-bb2c-9240f220866d",
   "metadata": {},
   "source": [
    "## Create an MCP Knowledge Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0264333-a34e-46ec-8c20-8b4faf6d8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.knowledge_stores.no_encode import MCPKnowledgeStore\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f2ba9a-d9ed-4be3-8c86-16261e88ba49",
   "metadata": {},
   "source": [
    "### Define a ReRanker\n",
    "\n",
    "When `MCPKnowledgeStore` retrieves knowledge from multiple MCP sources, you can provide a `reranker_callback` function to rank and filter results by relevance. This optimization step ensures downstream components receive only the highest-quality, most contextually relevant information for a given query.\n",
    "\n",
    "Below we'll use a `sentence_transformer.CrossEncoder` to rerank the nodes from our two MCP sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52b4957c-a70c-4d29-8151-ef7414db31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranker_callback(\n",
    "    nodes: list[KnowledgeNode], query: str\n",
    ") -> list[tuple[float, KnowledgeNode]]:\n",
    "    model = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L2\")\n",
    "    # Concatenate the query and all passages and predict the scores for the pairs [query, passage]\n",
    "    model_inputs = [[query, n.text_content] for n in nodes]\n",
    "    scores = model.predict(model_inputs)\n",
    "\n",
    "    # Sort the scores in decreasing order\n",
    "    results = [(score, node) for score, node in zip(scores, nodes)]\n",
    "    return sorted(results, key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "547a02e4-bcee-4d99-b14a-6ddd1ced685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the re-ranker to the knowledge store is easy to do!\n",
    "knowledge_store = (\n",
    "    MCPKnowledgeStore()\n",
    "    .add_source(mcp_source)\n",
    "    .add_source(llama_cloud_mcp_source)\n",
    "    .with_reranker(reranker_callback)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "885761e1-dc1a-4ef5-8b01-f33083ff01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await knowledge_store.retrieve(\"What is RAFT?\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33e2f1c-5507-4779-ae8b-faabe11f9a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.float32(0.91412175),\n",
       "  KnowledgeNode(node_id='a4509097-bc9c-4a85-bbbc-85b327e07bdf', embedding=None, node_type=<NodeType.TEXT: 'text'>, text_content='RAFT stands for Retrieval-Augmented Fine-Tuning, a technique used to enhance the performance of large language models on knowledge-intensive natural language processing tasks. It involves incorporating retrieved information from external sources into the training process of language models to improve their understanding and performance on complex tasks.\\n\\nHere is an example of how RAFT can be implemented using Python code:\\n\\n```python\\nfrom transformers import RAFT, AutoTokenizer\\n\\n# Load the RAFT model and tokenizer\\nmodel = RAFT.from_pretrained(\\'model_name\\')\\ntokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\\n\\n# Define your input text\\ninput_text = \"Your input text here.\"\\n\\n# Tokenize the input text\\ninput_ids = tokenizer(input_text, return_tensors=\\'pt\\')[\\'input_ids\\']\\n\\n# Retrieve relevant information using the RAFT model\\nretrieved_info = model.retrieve(input_ids)\\n\\n# Incorporate the retrieved information into the fine-tuning process\\n# Further train your model using the retrieved information to improve performance on specific tasks\\n```\\n\\nIn summary, RAFT is a method that leverages retrieved information to fine-tune language models, enhancing their ability to tackle knowledge-intensive tasks effectively.', image_content=None, metadata={'name': 'llama-index-server', 'tool_name': 'LlamaCloudQueryTool', 'query_param_name': 'query', 'tool_call_kwargs': {}, 'server_params': {'command': 'sh', 'args': ['-c', 'cd /home/nerdai/OSS/llamacloud-mcp && poetry install && exec poetry run python mcp-server.py'], 'env': None, 'cwd': None, 'encoding': 'utf-8', 'encoding_error_handler': 'strict'}})),\n",
       " (np.float32(0.7772403),\n",
       "  KnowledgeNode(node_id='7ac7451f-0307-465f-8c25-113f2e43e9f3', embedding=None, node_type=<NodeType.TEXT: 'text'>, text_content='...Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which...', image_content=None, metadata={'name': 'awslabs.amazon-kendra-index-mcp-server', 'tool_name': 'KendraQueryTool', 'query_param_name': 'query', 'tool_call_kwargs': {'indexId': '572aca26-16be-44df-84d3-4d96d778f120', 'region': 'ca-central-1'}, 'server_params': {'command': 'docker', 'args': ['run', '--rm', '--interactive', '--init', '--env-file', '/home/nerdai/Projects/fed-rag/docs/notebooks/.env', 'awslabs/amazon-kendra-index-mcp-server:latest'], 'env': None, 'cwd': None, 'encoding': 'utf-8', 'encoding_error_handler': 'strict'}}))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d4539-3e89-4632-b4f3-3c892b14f4a5",
   "metadata": {},
   "source": [
    "## Assemble a NoEncode RAG System\n",
    "\n",
    "Now that we have built our `MCPKnowledgeStore`, we can assemble our NoEncode RAG system. Recall that with NoEncode RAG systems, we forego the encoding step, and thus, we don't require a retriever model as we did with traditional RAG systems—all we need is a generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d626460-d28f-488f-8bb1-738cf389e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.generators.huggingface import HFPretrainedModelGenerator\n",
    "import torch\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "generation_cfg = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    eos_token_id=151643,\n",
    "    bos_token_id=151643,\n",
    "    max_new_tokens=2048,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "    cache_implementation=\"offloaded\",\n",
    "    stop_strings=\"</response>\",\n",
    ")\n",
    "generator = HFPretrainedModelGenerator(\n",
    "    model_name=\"Qwen/Qwen2.5-3B\",\n",
    "    load_model_at_init=False,\n",
    "    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n",
    "    generation_config=generation_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9d24ee6-b734-489b-8750-39aea560b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag import AsyncNoEncodeRAGSystem, RAGConfig\n",
    "\n",
    "rag_config = RAGConfig(top_k=3)\n",
    "rag_system = AsyncNoEncodeRAGSystem(\n",
    "    knowledge_store=knowledge_store,\n",
    "    generator=generator,\n",
    "    rag_config=rag_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "223b69b6-4d05-49e7-9952-6f05c7257f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f16259048d8479785f2a6c04e5c146a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "res = await rag_system.query(query=\"What is RAFT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c86d1fcd-501b-43f7-911d-5578a02f8218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAFT stands for Retrieval-Augmented Fine-Tuning, a technique used to enhance the performance of large language models on knowledge-intensive natural language processing tasks. It involves fine-tuning a language model with in-context retrieval augmentation, where relevant information is retrieved from external sources to assist the model in generating accurate responses.\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "# final RAG response\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2291134b-b993-465e-98c0-c75e47e2963f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE NODE 0:\n",
      "SCORE: 0.7772402763366699\n",
      "SOURCE: awslabs.amazon-kendra-index-mcp-server\n",
      "TEXT: ...Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which...\n",
      "\n",
      "\n",
      "SOURCE NODE 1:\n",
      "SCORE: 0.12683217227458954\n",
      "SOURCE: awslabs.amazon-kendra-index-mcp-server\n",
      "TEXT: In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t\n",
      "\n",
      "\n",
      "SOURCE NODE 2:\n",
      "SCORE: 0.0769425481557846\n",
      "SOURCE: llama-index-server\n",
      "TEXT: RAFT stands for Retrieval-Augmented Fine-Tuning, a technique used to enhance the performance of large language models on knowledge-intensive natural language processing tasks. It involves fine-tuning a language model with in-context retrieval augmentation, where relevant information is retrieved from external sources to assist the model in generating accurate responses.\n",
      "\n",
      "Here is an example of how RAFT can be implemented using Python code with the Hugging Face Transformers library:\n",
      "\n",
      "```python\n",
      "fro\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a peak at the retrieved source nodes from MCP knowledge store\n",
    "for ix, sn in enumerate(res.source_nodes):\n",
    "    print(\n",
    "        f\"SOURCE NODE {ix}:\\nSCORE: {sn.score}\\nSOURCE: {sn.metadata['name']}\\nTEXT: {sn.text_content[:500]}\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165e60b-945c-4b8f-9b6f-b5d796bd8ad1",
   "metadata": {},
   "source": [
    "## In Summary\n",
    "\n",
    "In this comprehensive notebook, we covered how to bring in context from MCP servers (or sources). More specifically, we went through:\n",
    "\n",
    "- How to build and interact with an `MCPStdioKnowledgeSource`\n",
    "- How to build and interact with an `MCPKnowledgeStore` that is connected to these sources\n",
    "- How to then assemble a NoEncode RAG system that combines the `MCPKnowledgeStore` with a chosen generator LLM\n",
    "- How to define and use a reranker callback to better prioritize the retrieved knowledge nodes from the multiple MCP sources\n",
    "\n",
    "## What's Next\n",
    "\n",
    "After assembling the `NoEncodeRAGSystem`, you can use it with any `GeneratorTrainers` (e.g., [HuggingFaceTrainerForRALT](http://127.0.0.1:8000/api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.ralt.HuggingFaceTrainerForRALT)) to fine-tune the RAG system to better adapt to your MCP knowledge store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec3cab8a-9944-43f9-b893-ceab2af46e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from fed_rag.trainers.huggingface import HuggingFaceTrainerForRALT\n",
    "\n",
    "# define a train dataset\n",
    "train_dataset = Dataset.from_dict(\n",
    "    # examples from Commonsense QA\n",
    "    {\n",
    "        \"query\": [\n",
    "            \"The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\",\n",
    "            \"Sammy wanted to go to where the people were.  Where might he go?\",\n",
    "        ],\n",
    "        \"response\": [\n",
    "            \"ignore\",\n",
    "            \"populated areas\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# use a smaller generator to avoid OOM\n",
    "generator = HFPretrainedModelGenerator(\n",
    "    model_name=\"Qwen/Qwen2.5-0.5B\",\n",
    "    load_model_at_init=False,\n",
    "    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n",
    "    generation_config=generation_cfg,\n",
    ")\n",
    "rag_system.generator = generator\n",
    "\n",
    "# the trainer object\n",
    "generator_trainer = HuggingFaceTrainerForRALT(\n",
    "    rag_system=rag_system.to_sync(),  # trainers only work with sync objects\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dea521d3-1c6c-4617-9c33-453e49bfff42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = generator_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6397a2ed-f5c2-42f1-b83d-959f7f4a8c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8815449078877767"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
