{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683e31db-0a58-43c7-bb70-efdbe655a735",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/VectorInstitute/fed-rag/blob/main/docs/notebooks/no_encode_rag_with_mcp.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "_(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c67b9-401e-4565-b082-cdc5abfbe1fe",
   "metadata": {},
   "source": [
    "# Build a NoEncode RAG System with an MCP Knowledge Store\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In traditional RAG systems, there are three components: a retriever, a knowledge store, and a generator. A user's query is encoded by the retriever and used to retrieve relevant knowledge chunks from the knowledge store that had previously been encoded by the retriever as well. The user query along with the retrieved knowledge chunks are passed to the LLM generator to finally respond to the original query.\n",
    "\n",
    "With NoEncode RAG systems, knowledge is still kept in a knowledge store and retrieved for responses to user queries, but there is no encoding step at all. Instead of pre-computing embeddings, NoEncode RAG systems query knowledge sources directly using natural language.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "**Traditional RAG:**\n",
    "- Documents → Embed → Vector Store\n",
    "- Query → Embed → Vector Search → Retrieve → Generate\n",
    "\n",
    "**NoEncode RAG:**\n",
    "- Knowledge Sources (MCP servers, APIs, databases)\n",
    "- Query → Direct Natural Language Query → Retrieve → Generate\n",
    "\n",
    "_**NOTE:** Knowledge sources may be traditional RAG systems themselves, and thus, these would involve encoding. However, the main RAG system does not handle encoding of queries or knowledge chunks at all._\n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "MCP provides a standardized way for AI systems to connect to external tools and data sources. In our NoEncode RAG system, MCP servers act as live knowledge sources that can be queried directly with natural language. An MCP knowledge store acts as the MCP client host that creates connections to these servers and retrieves context from them.\n",
    "\n",
    "### Outline\n",
    "\n",
    "In this cookbook, we will stand up two MCP knowledge sources, use them as part of an MCP knowledge store, and finally build an `AsyncNoEncodeRAGSystem` that allows us to query these sources.\n",
    "\n",
    "1. MCP Knowledge Source 1: an AWS Kendra Index MCP Server\n",
    "2. MCP Knowledge Source 2: a LlamaCloud MCP Server\n",
    "3. Create an MCP Knowledge Store (using our two built sources)\n",
    "4. Assemble a NoEncode RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d1bc6-9b84-490a-8697-d502c0f5821d",
   "metadata": {},
   "source": [
    "## MCP Knowledge Source 1: an AWS Kendra Index MCP Server\n",
    "\n",
    "Here, we make use of one the myriad of officially supported [AWS MCP servers](https://github.com/awslabs/mcp?tab=readme-ov-file#available-servers) offered by [AWS Labs](https://github.com/awslabs), namely: their [AWS Kendra Index MCP Server](https://github.com/awslabs/mcp/tree/main/src/amazon-kendra-index-mcp-server).\n",
    "\n",
    "AWS Kendra is an enterprise search service powered by machine learning. It can search across various data sources including documents, FAQs, knowledge bases, and websites, providing intelligent answers to natural language queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12826b-9652-4ac7-a3a7-d565f216eb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7c73fe-4a94-4a92-a977-1286a4671a00",
   "metadata": {},
   "source": [
    "## MCP Knowledge Source 2: a LlamaCloud MCP Server\n",
    "\n",
    "In this part of the cookbook, we'll stand up an MCP server using [LlamaCloud](https://docs.llamaindex.ai/en/stable/llama_cloud/)—an enterprise solution by LlamaIndex—by following their MCP [demo](https://github.com/run-llama/llamacloud-mcp?tab=readme-ov-file#llamacloud-as-an-mcp-server).\n",
    "\n",
    "LlamaCloud provides document parsing, indexing, and retrieval capabilities. By exposing these through an MCP server, we can query processed documents directly using natural language without managing our own document processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9af79-f756-4bb7-8d1b-d9bd0262c8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb002cfc-17eb-4dae-bb2c-9240f220866d",
   "metadata": {},
   "source": [
    "## Create an MCP Knowledge Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0264333-a34e-46ec-8c20-8b4faf6d8a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "769d4539-3e89-4632-b4f3-3c892b14f4a5",
   "metadata": {},
   "source": [
    "## Assemble a NoEncode RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d626460-d28f-488f-8bb1-738cf389e29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
