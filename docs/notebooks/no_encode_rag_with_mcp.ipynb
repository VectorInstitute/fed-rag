{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683e31db-0a58-43c7-bb70-efdbe655a735",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/VectorInstitute/fed-rag/blob/main/docs/notebooks/no_encode_rag_with_mcp.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "_(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c67b9-401e-4565-b082-cdc5abfbe1fe",
   "metadata": {},
   "source": [
    "# Build a NoEncode RAG System with an MCP Knowledge Store\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In traditional RAG systems, there are three components: a retriever, a knowledge store, and a generator. A user's query is encoded by the retriever and used to retrieve relevant knowledge chunks from the knowledge store that had previously been encoded by the retriever as well. The user query along with the retrieved knowledge chunks are passed to the LLM generator to finally respond to the original query.\n",
    "\n",
    "With NoEncode RAG systems, knowledge is still kept in a knowledge store and retrieved for responses to user queries, but there is no encoding step at all. Instead of pre-computing embeddings, NoEncode RAG systems query knowledge sources directly using natural language.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "**Traditional RAG:**\n",
    "- Documents → Embed → Vector Store\n",
    "- Query → Embed → Vector Search → Retrieve → Generate\n",
    "\n",
    "**NoEncode RAG:**\n",
    "- Knowledge Sources (MCP servers, APIs, databases)\n",
    "- Query → Direct Natural Language Query → Retrieve → Generate\n",
    "\n",
    "_**NOTE:** Knowledge sources may be traditional RAG systems themselves, and thus, these would involve encoding. However, the main RAG system does not handle encoding of queries or knowledge chunks at all._\n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "MCP provides a standardized way for AI systems to connect to external tools and data sources. In our NoEncode RAG system, MCP servers act as live knowledge sources that can be queried directly with natural language. An MCP knowledge store acts as the MCP client host that creates connections to these servers and retrieves context from them.\n",
    "\n",
    "### Outline\n",
    "\n",
    "In this cookbook, we will stand up two MCP knowledge sources, use them as part of an MCP knowledge store, and finally build an `AsyncNoEncodeRAGSystem` that allows us to query these sources.\n",
    "\n",
    "1. MCP Knowledge Source 1: an AWS Kendra Index MCP Server\n",
    "2. MCP Knowledge Source 2: a LlamaCloud MCP Server\n",
    "3. Create an MCP Knowledge Store (using our two built sources)\n",
    "4. Assemble a NoEncode RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d1bc6-9b84-490a-8697-d502c0f5821d",
   "metadata": {},
   "source": [
    "## MCP Knowledge Source 1: an AWS Kendra Index MCP Server\n",
    "\n",
    "Here, we make use of one the myriad of officially supported [AWS MCP servers](https://github.com/awslabs/mcp?tab=readme-ov-file#available-servers) offered by [AWS Labs](https://github.com/awslabs), namely: their [AWS Kendra Index MCP Server](https://github.com/awslabs/mcp/tree/main/src/amazon-kendra-index-mcp-server).\n",
    "\n",
    "AWS Kendra is an enterprise search service powered by machine learning. It can search across various data sources including documents, FAQs, knowledge bases, and websites, providing intelligent answers to natural language queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e12826b-9652-4ac7-a3a7-d565f216eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from mcp import StdioServerParameters\n",
    "from fed_rag.knowledge_stores.no_encode import MCPStdioKnowledgeSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea63aa3-1c3f-4953-a36b-72120c656912",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_params = StdioServerParameters(\n",
    "    command=\"docker\",\n",
    "    args=[\n",
    "        \"run\",\n",
    "        \"--rm\",\n",
    "        \"--interactive\",\n",
    "        \"--init\",  # important!\n",
    "        \"--env-file\",\n",
    "        f\"{os.getcwd()}/.env\",\n",
    "        \"awslabs/amazon-kendra-index-mcp-server:latest\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "mcp_source = MCPStdioKnowledgeSource(\n",
    "    name=\"awslabs.amazon-kendra-index-mcp-server\",\n",
    "    server_params=server_params,\n",
    "    tool_name=\"KendraQueryTool\",\n",
    "    query_param_name=\"query\",\n",
    "    tool_call_kwargs={\n",
    "        \"indexId\": \"572aca26-16be-44df-84d3-4d96d778f120\",\n",
    "        \"region\": \"ca-central-1\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ed95a-90ee-40fb-8c34-a20a4f3fcdfa",
   "metadata": {},
   "source": [
    "### Using the default converter of `~mcp.CallToolResult` to `~fed_rag.KnowledgeNode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7f783f-9b70-4960-9944-896f67398ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_tool_result = await mcp_source.retrieve(\"What is RAFT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6528d2-ec2f-4cce-8d4c-928ffa20d0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"query\": \"What is RAFT?\", \"total_results_count\": 4, \"results\": [{\"id\": \"48f58734-f2c8-4a65-9cce-3540b914fb52-c413e6f3-def2-4ee1-96b7-c4844fb735f5\", \"type\": \"ANSWER\", \"document_title\": \"raft.pdf\", \"document_uri\": \"https://fed-rag-mcp-cookbook.s3.ca-central-1.amazonaws.com/raft.pdf\", \"score\": \"HIGH\", \"excerpt\": \"In this paper, we present Retrieval Augmented\\\\nFine Tuning (RAFT), a training recipe which improves the model\\\\u2019s ability\\\\nto answer questions in \\\\\"open-book\\\\\" in-domain settings. In t'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_node = mcp_source.call_tool_result_to_knowledge_node(\n",
    "    call_tool_result\n",
    ")\n",
    "knowledge_node.text_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab5fe3-c92a-4f3a-acf7-5d00f79ccab9",
   "metadata": {},
   "source": [
    "As we can see, this is not really the most ideal conversion. We should probably only pass the excerpt text content to the LLM generator. Thus, we should define our own converter function to extract only the text content.\n",
    "\n",
    "According to the source code for this server, we see that a successful tool call of `KendraQueryTool` will yield a `CallToolResult` whose `text` attribute is a JSON string containing a `results` key. The value for `results` is a list of `result_items` each containing an `excerpt` field which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680ec887-bbb7-4794-b541-468ee478c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from typing import Any\n",
    "from mcp.types import CallToolResult\n",
    "from fed_rag.data_structures import KnowledgeNode\n",
    "\n",
    "\n",
    "def kendra_index_converter_fn(\n",
    "    result: CallToolResult, metadata: dict[str, Any] | None = None\n",
    ") -> KnowledgeNode:\n",
    "    excerpts = []\n",
    "    for c in result.content:\n",
    "        if c.type == \"text\":\n",
    "            data = json.loads(c.text)\n",
    "            for res in data[\"results\"]:\n",
    "                excerpts.append(re.sub(r\"\\s+\", \" \", res[\"excerpt\"].strip()))\n",
    "    return KnowledgeNode(\n",
    "        node_type=\"text\", text_content=\"\\n\\n<<CHUNK_SEP>>\\n\\n\".join(excerpts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df90d4-a49f-4a45-b971-aee6f0405ec4",
   "metadata": {},
   "source": [
    "Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce2cdd94-d4d8-470e-b3c6-be72a04054c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t\n",
      "\n",
      "<<CHUNK_SEP>>\n",
      "\n",
      "3 RAFT In this section, we present RAFT, a novel way of training LLMs for domain-specific open- book exams. We first introduce the classical technique of supervised fine-tuning, followed with the key takeaways from our experiments. Then, we introduce RAFT , a modified version of general instructio\n",
      "\n",
      "<<CHUNK_SEP>>\n",
      "\n",
      ", 2023; Xu 9 Preprint, Under Review et al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning dataset for RAG and train a model to perform well on these tasks. In particular, in their settings, at test time, the domain or documents can be different tha\n",
      "\n",
      "<<CHUNK_SEP>>\n",
      "\n",
      "...Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which...\n"
     ]
    }
   ],
   "source": [
    "print(kendra_index_converter_fn(call_tool_result).text_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029904c-5362-4a1f-a915-9f4bced79a75",
   "metadata": {},
   "source": [
    "This is better than we had before with the default converter, so let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b4c2558-423b-4c50-a469-3ee131a85938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the converter function\n",
    "mcp_source = mcp_source.with_converter(kendra_index_converter_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c73fe-4a94-4a92-a977-1286a4671a00",
   "metadata": {},
   "source": [
    "## MCP Knowledge Source 2: a LlamaCloud MCP Server\n",
    "\n",
    "In this part of the cookbook, we'll stand up an MCP server using [LlamaCloud](https://docs.llamaindex.ai/en/stable/llama_cloud/)—an enterprise solution by LlamaIndex—by following their MCP [demo](https://github.com/run-llama/llamacloud-mcp?tab=readme-ov-file#llamacloud-as-an-mcp-server).\n",
    "\n",
    "LlamaCloud provides document parsing, indexing, and retrieval capabilities. By exposing these through an MCP server, we can query processed documents directly using natural language without managing our own document processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237356f3-a651-49ec-8acd-d81ed9ef3890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9af79-f756-4bb7-8d1b-d9bd0262c8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb002cfc-17eb-4dae-bb2c-9240f220866d",
   "metadata": {},
   "source": [
    "## Create an MCP Knowledge Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0264333-a34e-46ec-8c20-8b4faf6d8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.knowledge_stores.no_encode import MCPKnowledgeStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547a02e4-bcee-4d99-b14a-6ddd1ced685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_store = MCPKnowledgeStore().add_source(mcp_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "885761e1-dc1a-4ef5-8b01-f33083ff01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await knowledge_store.retrieve(\"What is RAFT\", top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d4539-3e89-4632-b4f3-3c892b14f4a5",
   "metadata": {},
   "source": [
    "## Assemble a NoEncode RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d626460-d28f-488f-8bb1-738cf389e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.generators.huggingface import HFPretrainedModelGenerator\n",
    "import torch\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "generation_cfg = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    eos_token_id=151643,\n",
    "    bos_token_id=151643,\n",
    "    max_new_tokens=2048,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "    cache_implementation=\"offloaded\",\n",
    "    stop_strings=\"</response>\",\n",
    ")\n",
    "generator = HFPretrainedModelGenerator(\n",
    "    model_name=\"Qwen/Qwen2.5-0.5B\",\n",
    "    load_model_at_init=False,\n",
    "    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n",
    "    generation_config=generation_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d24ee6-b734-489b-8750-39aea560b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag import AsyncNoEncodeRAGSystem, RAGConfig\n",
    "\n",
    "rag_config = RAGConfig(top_k=2)\n",
    "rag_system = AsyncNoEncodeRAGSystem(\n",
    "    knowledge_store=knowledge_store,\n",
    "    generator=generator,\n",
    "    rag_config=rag_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "223b69b6-4d05-49e7-9952-6f05c7257f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "res = await rag_system.query(query=\"What is RAFT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c86d1fcd-501b-43f7-911d-5578a02f8218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Here's a concise response:\n",
      "\n",
      "**What is RAFT?**\n",
      "\n",
      "RAFT is a training recipe that improves the performance of language models on tasks like answering questions in \"open-book\" in-domain settings. It involves training the model to ignore documents that don't help in answering the question, similar to how a fine-tuned model might ignore irrelevant information during inference.\n",
      "\n",
      "**Context**\n",
      "\n",
      "In this paper, we present RAFT, a training recipe that improves the model's ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which is a key takeaway from our experiments.\n",
      "\n",
      "**Summary**\n",
      "\n",
      "In this section, we introduce RAFT, a novel way of training LLMs for domain-specific open- book exams. We first present the classical technique of supervised fine-tuning, followed by the key takeaways from our experiments. Then, we introduce RAFT, a modified version of general instruction to fine-tune (GIFT), which is a technique used to fine-tune models for specific tasks.\n",
      "\n",
      "**Key Takeaways**\n",
      "\n",
      "1. **Fine-tuning**: RAFT focuses on fine-tuning the model for a specific task, such as answering questions in \"open-book\" in-domain settings.\n",
      "2. **Domain specificity**: The model is trained to ignore documents that don't help in answering the question, similar to how a fine-tuned model might ignore irrelevant information during inference.\n",
      "3. **Context**: This paper introduces RAFT, a training recipe that improves the model's ability to answer questions in \"open-book\" in-domain settings.\n",
      "4. **Incorporation**: The model is trained to ignore documents that don't help in answering the question, which is a key takeaway from our experiments.\n",
      "5. **Supervised fine-tuning**: RAFT is a modification of supervised fine-tuning, which involves training the model on a fine-tuned dataset that includes both training and test data.\n",
      "\n",
      "**References**\n",
      "\n",
      "* Xu, J., et al. (2023). Retrieval Augmented Fine Tuning (RAFT): A Training Recipe for Language Models in Open-Book Exams. arXiv preprint arXiv:2307.02165.\n",
      "* Liu, L., et al. (2024). Fine Tuning for Language Models in Open-Book Exams. arXiv preprint arXiv:2403.04591.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291134b-b993-465e-98c0-c75e47e2963f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
