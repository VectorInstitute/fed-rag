{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683e31db-0a58-43c7-bb70-efdbe655a735",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/VectorInstitute/fed-rag/blob/main/docs/notebooks/no_encode_rag_with_mcp.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "_(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c67b9-401e-4565-b082-cdc5abfbe1fe",
   "metadata": {},
   "source": [
    "# Build a NoEncode RAG System with an MCP Knowledge Store\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In traditional RAG systems, there are three components: a retriever, a knowledge store, and a generator. A user's query is encoded by the retriever and used to retrieve relevant knowledge chunks from the knowledge store that had previously been encoded by the retriever as well. The user query along with the retrieved knowledge chunks are passed to the LLM generator to finally respond to the original query.\n",
    "\n",
    "With NoEncode RAG systems, knowledge is still kept in a knowledge store and retrieved for responses to user queries, but there is no encoding step at all. Instead of pre-computing embeddings, NoEncode RAG systems query knowledge sources directly using natural language.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "**Traditional RAG:**\n",
    "- Documents → Embed → Vector Store\n",
    "- Query → Embed → Vector Search → Retrieve → Generate\n",
    "\n",
    "**NoEncode RAG:**\n",
    "- Knowledge Sources (MCP servers, APIs, databases)\n",
    "- Query → Direct Natural Language Query → Retrieve → Generate\n",
    "\n",
    "_**NOTE:** Knowledge sources may be traditional RAG systems themselves, and thus, these would involve encoding. However, the main RAG system does not handle encoding of queries or knowledge chunks at all._\n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "MCP provides a standardized way for AI systems to connect to external tools and data sources. In our NoEncode RAG system, MCP servers act as live knowledge sources that can be queried directly with natural language. An MCP knowledge store acts as the MCP client host that creates connections to these servers and retrieves context from them.\n",
    "\n",
    "### Outline\n",
    "\n",
    "In this cookbook, we will stand up two MCP knowledge sources, use them as part of an MCP knowledge store, and finally build an `AsyncNoEncodeRAGSystem` that allows us to query these sources.\n",
    "\n",
    "1. MCP Knowledge Source 1: an AWS Kendra Index MCP Server\n",
    "2. MCP Knowledge Source 2: a LlamaCloud MCP Server\n",
    "3. Create an MCP Knowledge Store (using our two built sources)\n",
    "4. Assemble a NoEncode RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d1bc6-9b84-490a-8697-d502c0f5821d",
   "metadata": {},
   "source": [
    "## MCP Knowledge Source 1: an AWS Kendra Index MCP Server\n",
    "\n",
    "Here, we make use of one the myriad of officially supported [AWS MCP servers](https://github.com/awslabs/mcp?tab=readme-ov-file#available-servers) offered by [AWS Labs](https://github.com/awslabs), namely: their [AWS Kendra Index MCP Server](https://github.com/awslabs/mcp/tree/main/src/amazon-kendra-index-mcp-server).\n",
    "\n",
    "AWS Kendra is an enterprise search service powered by machine learning. It can search across various data sources including documents, FAQs, knowledge bases, and websites, providing intelligent answers to natural language queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a875a-5ad8-4da8-b24e-1d02e1c676b3",
   "metadata": {},
   "source": [
    "### Pre-requisite Steps\n",
    "\n",
    "#### Create a Kendra Index\n",
    "\n",
    "To be able to use this MCP server, you need to create a new Kendra Index. Add a S3 data connector to it that has the [RAFT paper](https://arxiv.org/pdf/2403.10131)—make sure to sync your index so that its ready to be queried with the RAFT paper. Finally, fill in the information below for regarding your Kendra Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c21b19-316c-43af-ba7a-cd20069d97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info regarding your kendra index which needs to be passed to the MCP tool call\n",
    "kendra_index_info = {\n",
    "    \"indexId\": \"572aca26-16be-44df-84d3-4d96d778f120\",\n",
    "    \"region\": \"ca-central-1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86292b85-9bb3-48d5-b3f9-60885c3fdaf7",
   "metadata": {},
   "source": [
    "#### Build the Kendra Index MCP Server Docker image\n",
    "\n",
    "With our Kendra index in hand, we now are able to build a local MCP server that would interact with it. To do this, we take the following steps\n",
    "\n",
    "1. Clone the `awslabs/mcp` Github repo:\n",
    "\n",
    "```sh\n",
    "git clone https://github.com/awslabs/mcp.gi\n",
    "```\n",
    "\n",
    "2. cd into the Kendra index src directory:\n",
    "\n",
    "```sh\n",
    "cd mcp/src/amazon-kendra-index-mcp-server\n",
    "```\n",
    "\n",
    "3. Locally build the Docker image\n",
    "\n",
    "```sh\n",
    "docker build -t awslabs/amazon-kendra-index-mcp-server .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c55ae-077a-4670-9b68-a3746142b88e",
   "metadata": {},
   "source": [
    "#### Configure AWS Credentials\n",
    "\n",
    "Create a `.env` file in the same directory as this notebook, with your AWS credentials:\n",
    "\n",
    "```sh\n",
    "# .env file\n",
    "AWS_ACCESS_KEY_ID=...\n",
    "AWS_SECRET_ACCESS_KEY=...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d44ae2-2a58-401e-a18e-d57046e16bac",
   "metadata": {},
   "source": [
    "### Build the MCP Stdio Knowledge Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e12826b-9652-4ac7-a3a7-d565f216eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mcp import StdioServerParameters\n",
    "from fed_rag.knowledge_stores.no_encode import MCPStdioKnowledgeSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea63aa3-1c3f-4953-a36b-72120c656912",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_params = StdioServerParameters(\n",
    "    command=\"docker\",\n",
    "    args=[\n",
    "        \"run\",\n",
    "        \"--rm\",\n",
    "        \"--interactive\",\n",
    "        \"--init\",  # important to have in Jupyter Notebook\n",
    "        \"--env-file\",\n",
    "        f\"{os.getcwd()}/.env\",\n",
    "        \"awslabs/amazon-kendra-index-mcp-server:latest\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "mcp_source = MCPStdioKnowledgeSource(\n",
    "    name=\"awslabs.amazon-kendra-index-mcp-server\",\n",
    "    server_params=server_params,\n",
    "    tool_name=\"KendraQueryTool\",\n",
    "    query_param_name=\"query\",\n",
    "    tool_call_kwargs=kendra_index_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c868b09-3fc0-4387-b49e-42cdce0e9268",
   "metadata": {},
   "source": [
    "Let's test out our new MCP source by invoking the `retrieve()` method with a specific query. This will return an `~mcp.CallToolResult` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7f783f-9b70-4960-9944-896f67398ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_tool_result = await mcp_source.retrieve(\"What is RAFT?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ed95a-90ee-40fb-8c34-a20a4f3fcdfa",
   "metadata": {},
   "source": [
    "#### Converting MCP Call Tool Results to Knowledge Nodes\n",
    "\n",
    "MCP tool results are automatically converted to `KnowledgeNode` objects using a default converter in `MCPStdioKnowledgeSource`. This generic converter works for basic use cases but may not extract all valuable information from server-specific responses. Implement a custom converter to optimize knowledge extraction for your particular MCP server. Let's see the default converter in action first and determine if we need to create our own converter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6528d2-ec2f-4cce-8d4c-928ffa20d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of knowledge nodes created:  1 \n",
      "\n",
      "Text content of first created node:\n",
      " {\"query\": \"What is RAFT?\", \"total_results_count\": 4, \"results\": [{\"id\": \"1881517b-ad6c-4c37-8fd1-f71018fe3f7c-02036f99-420a-457b-8bb3-edd7114d173d\", \"type\": \"ANSWER\", \"document_title\": \"raft.pdf\", \"document_uri\": \"https://fed-rag-mcp-cookbook.s3.ca-central-1.amazonaws.com/raft.pdf\", \"score\": \"HIGH\", \"excerpt\": \"In this paper, we present Retrieval Augmented\\nFine Tuning (RAFT), a training recipe which improves the model\\u2019s ability\\nto answer questions in \\\"open-book\\\" in-domain settings. In t\n"
     ]
    }
   ],
   "source": [
    "# using the default converter function\n",
    "knowledge_nodes = mcp_source.call_tool_result_to_knowledge_nodes_list(\n",
    "    call_tool_result\n",
    ")\n",
    "\n",
    "print(\"Number of knowledge nodes created: \", len(knowledge_nodes), \"\\n\")\n",
    "print(\n",
    "    \"Text content of first created node:\\n\",\n",
    "    knowledge_nodes[0].text_content[:500],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab5fe3-c92a-4f3a-acf7-5d00f79ccab9",
   "metadata": {},
   "source": [
    "As we can see, this is not really the most ideal conversion. We should probably only pass the `excerpt` text content to the LLM generator. Thus, we should define our own converter function to extract only the text content.\n",
    "\n",
    "According to the [source code](https://github.com/awslabs/mcp/blob/36ae951bb8cfed234f69f6336fe2463eb4e08587/src/amazon-kendra-index-mcp-server/awslabs/amazon_kendra_index_mcp_server/server.py#L150) for this server, we see that a successful tool call of `KendraQueryTool` will return a `CallToolResult` whose `text` attribute is a JSON string containing a `results` key. The value for `results` is a list of `result_items` each containing an `excerpt` field, which is ultimately what we want to pass to the LLM generator.\n",
    "\n",
    "Let's create a custom converter function to do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680ec887-bbb7-4794-b541-468ee478c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from typing import Any\n",
    "from mcp.types import CallToolResult\n",
    "from fed_rag.data_structures import KnowledgeNode\n",
    "\n",
    "\n",
    "# signature of a converter function\n",
    "def kendra_index_converter_fn(\n",
    "    result: CallToolResult, metadata: dict[str, Any] | None = None\n",
    ") -> list[KnowledgeNode]:\n",
    "    nodes = []\n",
    "    for c in result.content:\n",
    "        if c.type == \"text\":  # only use ~mcp.TextContent\n",
    "            data = json.loads(c.text)\n",
    "            for res in data[\"results\"]:\n",
    "                # take only the content in the \"excerpt\" key\n",
    "                text_content = re.sub(r\"\\s+\", \" \", res[\"excerpt\"].strip())\n",
    "                nodes.append(\n",
    "                    # create the knowledge node\n",
    "                    KnowledgeNode(\n",
    "                        node_type=\"text\",\n",
    "                        text_content=text_content,\n",
    "                        metadata=metadata,\n",
    "                    )\n",
    "                )\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df90d4-a49f-4a45-b971-aee6f0405ec4",
   "metadata": {},
   "source": [
    "Let's test out our custom converter as a standalone function on the previously obtained `call_tool_result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce2cdd94-d4d8-470e-b3c6-be72a04054c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of knowledge nodes created:  4 \n",
      "\n",
      "Text content of first created node:\n",
      " In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t \n",
      "\n",
      "Text content of second created node:\n",
      " 3 RAFT In this section, we present RAFT, a novel way of training LLMs for domain-specific open- book exams. We first introduce the classical technique of supervised fine-tuning, followed with the key takeaways from our experiments. Then, we introduce RAFT , a modified version of general instructio\n"
     ]
    }
   ],
   "source": [
    "knowledge_nodes = kendra_index_converter_fn(call_tool_result)\n",
    "\n",
    "print(\"Number of knowledge nodes created: \", len(knowledge_nodes), \"\\n\")\n",
    "print(\n",
    "    \"Text content of first created node:\\n\",\n",
    "    knowledge_nodes[0].text_content[:500],\n",
    "    \"\\n\",\n",
    ")\n",
    "print(\n",
    "    \"Text content of second created node:\\n\",\n",
    "    knowledge_nodes[1].text_content[:500],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029904c-5362-4a1f-a915-9f4bced79a75",
   "metadata": {},
   "source": [
    "This much improved and should work better when passing down as context to the LLM generator. We can update our `mcp_source` to use this converter function easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4c2558-423b-4c50-a469-3ee131a85938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the converter function\n",
    "mcp_source = mcp_source.with_converter(kendra_index_converter_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c73fe-4a94-4a92-a977-1286a4671a00",
   "metadata": {},
   "source": [
    "## MCP Knowledge Source 2: a LlamaCloud MCP Server\n",
    "\n",
    "In this part of the cookbook, we'll stand up an MCP server using [LlamaCloud](https://docs.llamaindex.ai/en/stable/llama_cloud/)—an enterprise solution by LlamaIndex—by following their MCP [demo](https://github.com/run-llama/llamacloud-mcp?tab=readme-ov-file#llamacloud-as-an-mcp-server).\n",
    "\n",
    "LlamaCloud provides document parsing, indexing, and retrieval capabilities. By exposing these through an MCP server, we can query processed documents directly using natural language without managing our own document processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98cdfa-57eb-40cf-bbc6-ac5ce6651fb1",
   "metadata": {},
   "source": [
    "### Pre-requisite Steps\n",
    "\n",
    "The steps below follow from the setup instructions listed in the Github repo for the [llamacloud-mcp demo](https://github.com/run-llama/llamacloud-mcp).\n",
    "\n",
    "This requires the creation of a new LlamaCloud account. If you don't have one, then you create one by visiting <https://cloud.llamaindex.ai/>.\n",
    "\n",
    "#### Create a LlamaCloud Index\n",
    "\n",
    "Login to LlamaCloud with your account and navigate to \"Tool\" > \"Index\" in the left side-bar. Click the \"Create Index\" button to create a new index. After creating the new index, upload the [RA-DIT paper](https://arxiv.org/pdf/2310.01352).\n",
    "\n",
    "__NOTE__: You'll need to supply information on your new index in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d069d-0579-47b6-9fd1-a3bed542f83d",
   "metadata": {},
   "source": [
    "#### Create a local MCP server to connect with the LlamaCloud Index\n",
    "\n",
    "1. Clone the `llamacloud-mcp` demo Github repo:\n",
    "\n",
    "```sh\n",
    "https://github.com/run-llama/llamacloud-mcp.git\n",
    "```\n",
    "\n",
    "2. cd into `llamacloud-mcp` directory\n",
    "\n",
    "```sh\n",
    "cd llamacloud-mcp\n",
    "```\n",
    "\n",
    "3. Update the `mcp-server.py`\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mcp = FastMCP('llama-index-server')\n",
    "\n",
    "@mcp.tool(name='LlamaCloudQueryTool')\n",
    "def llama_index_query(query: str) -> str:\n",
    "    \"\"\"Search the llama-index documentation for the given query.\"\"\"\n",
    "\n",
    "    index = LlamaCloudIndex(\n",
    "        name=\"<your-llamacloud-index-name>\", \n",
    "        project_name=\"Default\",  # change this if you didn't use default project name\n",
    "        organization_id=\"<your-llamacloud-org-id>\",\n",
    "        api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    response = index.as_query_engine().query(query + \" Be verbose and include code examples.\")\n",
    "\n",
    "    return str(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")\n",
    "```\n",
    "\n",
    "4. Create a `.env` file in the `llamacloud-mcp` directory\n",
    "\n",
    "```sh\n",
    "# .env\n",
    "LLAMA_CLOUD_API_KEY=<your-llamacloud-api-key>\n",
    "OPENAI_API_KEY=<your-openai-api-key>\n",
    "```\n",
    "\n",
    "__NOTE__: This llamacloud-mcp demo builds a `~llama_index.QueryEngine()` which by default uses an OpenAI LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0f7b9-1a14-45f8-9e7d-6ffc9d9a82f3",
   "metadata": {},
   "source": [
    "### Build the MCP Stdio Knowledge Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120d2d32-21e2-410d-b1e1-3d118336b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to your actual path\n",
    "path_to_llamacloud_mcp = \"/home/nerdai/OSS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf9af79-f756-4bb7-8d1b-d9bd0262c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_cloud_server_params = StdioServerParameters(\n",
    "    command=\"sh\",\n",
    "    args=[\n",
    "        \"-c\",\n",
    "        f\"cd {path_to_llamacloud_mcp}/llamacloud-mcp && poetry install && exec poetry run python mcp-server.py\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "llama_cloud_mcp_source = MCPStdioKnowledgeSource(\n",
    "    name=\"llama-index-server\",\n",
    "    server_params=llama_cloud_server_params,\n",
    "    tool_name=\"LlamaCloudQueryTool\",\n",
    "    query_param_name=\"query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0c75ad4-b7f1-494e-a3de-c27bcf01fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await llama_cloud_mcp_source.retrieve(\"What is RALT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72968ea9-a063-4a75-b64e-755723fabedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CallToolResult(meta=None, content=[TextContent(type='text', text='RALT stands for Retrieval-Augmented Language Model. It is a model that combines traditional language models with a retrieval mechanism to enhance performance on various natural language processing tasks. By incorporating a retrieval component, RALT models can access external knowledge sources to improve their understanding and generation capabilities.\\n\\nHere is an example of how a RALT model can be implemented using Python and the Hugging Face Transformers library:\\n\\n```python\\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\n\\n# Load the RALT model and tokenizer\\nmodel_name = \"your_RALT_model_name\"\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\n# Define a prompt for the RALT model\\nprompt = \"Your prompt here\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate output from the RALT model\\noutput = model.generate(**inputs)\\n\\n# Decode the output tokens back to text\\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\\nprint(output_text)\\n```\\n\\nBy utilizing a RALT model, you can leverage both the power of language modeling and the richness of external knowledge sources to improve the performance of various NLP tasks.', annotations=None)], isError=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b647e6c5-5944-40b5-a68a-67b235cd30d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results returned:  1 \n",
      "\n",
      "Text content of first returned node:\n",
      " RALT stands for Retrieval-Augmented Language Model. It is a model that combines traditional language models with a retrieval mechanism to enhance performance on various natural language processing tasks. By incorporating a retrieval component, RALT models can access external knowledge sources to improve their understanding and generation capabilities.\n",
      "\n",
      "Here is an example of how a RALT model can be implemented using Python and the Hugging Face Transformers library:\n",
      "\n",
      "```python\n",
      "from transformers im\n"
     ]
    }
   ],
   "source": [
    "knowledge_nodes = (\n",
    "    llama_cloud_mcp_source.call_tool_result_to_knowledge_nodes_list(res)\n",
    ")\n",
    "\n",
    "print(\"Number of results returned: \", len(knowledge_nodes), \"\\n\")\n",
    "print(\n",
    "    \"Text content of first returned node:\\n\",\n",
    "    knowledge_nodes[0].text_content[:500],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb002cfc-17eb-4dae-bb2c-9240f220866d",
   "metadata": {},
   "source": [
    "## Create an MCP Knowledge Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0264333-a34e-46ec-8c20-8b4faf6d8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.knowledge_stores.no_encode import MCPKnowledgeStore\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f2ba9a-d9ed-4be3-8c86-16261e88ba49",
   "metadata": {},
   "source": [
    "### Define a ReRanker\n",
    "\n",
    "When `MCPKnowledgeStore` retrieves knowledge from multiple MCP sources, you can provide a `reranker_callback` function to rank and filter results by relevance. This optimization step ensures downstream components receive only the highest-quality, most contextually relevant information for a given query.\n",
    "\n",
    "Below we'll use a `sentence_transformer.CrossEncoder` to rerank the nodes from our two MCP sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52b4957c-a70c-4d29-8151-ef7414db31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranker_callback(\n",
    "    nodes: list[KnowledgeNode], query: str\n",
    ") -> list[tuple[float, KnowledgeNode]]:\n",
    "    model = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L2\")\n",
    "    # Concatenate the query and all passages and predict the scores for the pairs [query, passage]\n",
    "    model_inputs = [[query, n.text_content] for n in nodes]\n",
    "    scores = model.predict(model_inputs)\n",
    "\n",
    "    # Sort the scores in decreasing order\n",
    "    results = [(score, node) for score, node in zip(scores, nodes)]\n",
    "    return sorted(results, key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "547a02e4-bcee-4d99-b14a-6ddd1ced685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the re-ranker to the knowledge store is easy to do!\n",
    "knowledge_store = (\n",
    "    MCPKnowledgeStore()\n",
    "    .add_source(mcp_source)\n",
    "    .add_source(llama_cloud_mcp_source)\n",
    "    .with_reranker(reranker_callback)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885761e1-dc1a-4ef5-8b01-f33083ff01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await knowledge_store.retrieve(\"What is RAFT?\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c33e2f1c-5507-4779-ae8b-faabe11f9a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7772398,\n",
       "  KnowledgeNode(node_id='4a032b54-4037-4ba8-ad6c-7cc9f810bcc0', embedding=None, node_type=<NodeType.TEXT: 'text'>, text_content='...Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which...', image_content=None, metadata={'name': 'awslabs.amazon-kendra-index-mcp-server', 'tool_name': 'KendraQueryTool', 'query_param_name': 'query', 'tool_call_kwargs': {'indexId': '572aca26-16be-44df-84d3-4d96d778f120', 'region': 'ca-central-1'}, 'server_params': {'command': 'docker', 'args': ['run', '--rm', '--interactive', '--init', '--env-file', '/home/nerdai/Projects/fed-rag/docs/notebooks/.env', 'awslabs/amazon-kendra-index-mcp-server:latest'], 'env': None, 'cwd': None, 'encoding': 'utf-8', 'encoding_error_handler': 'strict'}})),\n",
       " (0.18298462,\n",
       "  KnowledgeNode(node_id='1fe637c3-f69f-4d3e-abb6-f80cd20ed1af', embedding=None, node_type=<NodeType.TEXT: 'text'>, text_content='RAFT stands for Retrieval-Augmented Fine-Tuning. It is a technique used in natural language processing that combines retrieval-based methods with fine-tuning of large language models to enhance performance on various tasks. \\n\\nIn RAFT, a large language model is fine-tuned on a specific task while also incorporating information retrieved from external sources. This retrieval augmentation helps the model to access relevant external knowledge during inference, improving its ability to generate accurate responses.\\n\\nHere is an example of how RAFT can be implemented using a Python code snippet:\\n\\n```python\\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\n\\n# Load the pre-trained model and tokenizer\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"model_name\")\\ntokenizer = AutoTokenizer.from_pretrained(\"model_name\")\\n\\n# Fine-tune the model using RAFT\\n# Include retrieval-augmented data during fine-tuning\\n\\n# Generate predictions using the fine-tuned model\\ninput_text = \"Input text for inference\"\\nretrieved_info = \"Information retrieved from external source\"\\ninput_data = tokenizer(input_text, retrieved_info, return_tensors=\"pt\", padding=True, truncation=True)\\noutput = model.generate(**input_data)\\n\\n# Decode the output\\ndecoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\\nprint(decoded_output)\\n```\\n\\nIn this code snippet, the model is fine-tuned using RAFT by incorporating retrieved information during training. During inference, the model uses both the input text and retrieved information to generate predictions. The final output is decoded and printed for further analysis.', image_content=None, metadata={'name': 'llama-index-server', 'tool_name': 'LlamaCloudQueryTool', 'query_param_name': 'query', 'tool_call_kwargs': {}, 'server_params': {'command': 'sh', 'args': ['-c', 'cd /home/nerdai/OSS/llamacloud-mcp && poetry install && exec poetry run python mcp-server.py'], 'env': None, 'cwd': None, 'encoding': 'utf-8', 'encoding_error_handler': 'strict'}}))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d4539-3e89-4632-b4f3-3c892b14f4a5",
   "metadata": {},
   "source": [
    "## Assemble a NoEncode RAG System\n",
    "\n",
    "Now that we have built our `MCPKnowledgeStore`, we can assemble our NoEncode RAG system. Recall that with NoEncode RAG systems, we forego the encoding step, and thus, we don't require a retriever model as we did with traditional RAG systems—all we need is a generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d626460-d28f-488f-8bb1-738cf389e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.generators.huggingface import HFPretrainedModelGenerator\n",
    "import torch\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "generation_cfg = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    eos_token_id=151643,\n",
    "    bos_token_id=151643,\n",
    "    max_new_tokens=2048,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "    cache_implementation=\"offloaded\",\n",
    "    stop_strings=\"</response>\",\n",
    ")\n",
    "generator = HFPretrainedModelGenerator(\n",
    "    model_name=\"Qwen/Qwen2.5-3B\",\n",
    "    load_model_at_init=False,\n",
    "    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n",
    "    generation_config=generation_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9d24ee6-b734-489b-8750-39aea560b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag import AsyncNoEncodeRAGSystem, RAGConfig\n",
    "\n",
    "rag_config = RAGConfig(top_k=2)\n",
    "rag_system = AsyncNoEncodeRAGSystem(\n",
    "    knowledge_store=knowledge_store,\n",
    "    generator=generator,\n",
    "    rag_config=rag_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "223b69b6-4d05-49e7-9952-6f05c7257f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582f6dd0a390427f943fd004f831f3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "res = await rag_system.query(query=\"What is RAFT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c86d1fcd-501b-43f7-911d-5578a02f8218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAFT stands for Retrieval-Augmented Fine-Tuning, a training recipe that enhances a language model's ability to answer questions in \"open-book\" in-domain settings. It involves fine-tuning the model using a combination of in-context retrieval augmentation and instruction tuning methods. By incorporating relevant information from external sources, RAFT improves the model's understanding of the context and generates more accurate responses.\n"
     ]
    }
   ],
   "source": [
    "# final RAG response\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2291134b-b993-465e-98c0-c75e47e2963f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE NODE 0:\n",
      "SCORE: 0.7772397994995117\n",
      "SOURCE: awslabs.amazon-kendra-index-mcp-server\n",
      "TEXT: ...Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which...\n",
      "\n",
      "\n",
      "SOURCE NODE 1:\n",
      "SCORE: 0.6331754326820374\n",
      "SOURCE: llama-index-server\n",
      "TEXT: RAFT stands for Retrieval-Augmented Fine-Tuning. It is a technique used in natural language processing to enhance language models by incorporating retrieved information during the fine-tuning process. This approach involves leveraging external knowledge sources to improve the model's performance on various tasks.\n",
      "\n",
      "In RAFT, the language model is fine-tuned using a combination of in-context retrieval augmentation and instruction tuning methods. By retrieving relevant information from external sour\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a peak at the retrieved source nodes from MCP knowledge store\n",
    "for ix, sn in enumerate(res.source_nodes):\n",
    "    print(\n",
    "        f\"SOURCE NODE {ix}:\\nSCORE: {sn.score}\\nSOURCE: {sn.metadata['name']}\\nTEXT: {sn.text_content[:500]}\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165e60b-945c-4b8f-9b6f-b5d796bd8ad1",
   "metadata": {},
   "source": [
    "## In Summary\n",
    "\n",
    "In this comprehensive notebook, we covered how to bring in context from MCP servers (or sources). More specifically, we went through:\n",
    "\n",
    "- How to build and interact with an `MCPStdioKnowledgeSource`\n",
    "- How to build and interact with an `MCPKnowledgeStore` that is connected to these sources\n",
    "- How to then assemble a NoEncode RAG system that combines the `MCPKnowledgeStore` with a chosen generator LLM\n",
    "- How to define and use a reranker callback to better prioritize the retrieved knowledge nodes from the multiple MCP sources\n",
    "\n",
    "## What's Next\n",
    "\n",
    "After assembling the `NoEncodeRAGSystem`, you can use it with any `GeneratorTrainers` (e.g., [HuggingFaceTrainerForRALT](http://127.0.0.1:8000/api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.ralt.HuggingFaceTrainerForRALT)) to fine-tune the RAG system to better adapt to your MCP knowledge store."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
