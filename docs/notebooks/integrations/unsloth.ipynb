{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d948fa7-0eb5-4c35-b8aa-9ffb4e02c612",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/VectorInstitute/fed-rag/blob/main/docs/notebooks/integrations/qdrant.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "_(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52948a7-260f-4122-b327-990b7c96b2bf",
   "metadata": {},
   "source": [
    "# ðŸ¦¥ Using Unsloth FastModels as your RAG Generator Model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As of `v0.0.20`, the `fed-rag` library includes seamless integration with Unsloth.ai, a popular open-source library that dramatically accelerates fine-tuning workflows. This integration allows you to use `~unsloth.FastLanguageModel` instances as generator models in your RAG system while fully leveraging Unsloth's efficient fine-tuning capabilities.\n",
    "\n",
    "In this notebook, we demonstrate how to define a `UnslothFastModelGenerator`, integrate it into a RAG system, and fine-tune it using our `GeneratorTrainers`.\n",
    "\n",
    "NOTE: This notebook takes inspiration from Unsloth's [cookbook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb), for fine-tuning Gemma3 4Bâ€”we'll use that exact same model as our generator in our RAG system. The key difference is that we're fine-tuning the model specifically for retrieval-augmented generation tasks using our `fed-rag` framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82f731f-ea9a-4f41-908a-b2549f087be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install fed-rag[huggingface,unsloth] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e06be98a-d756-46fa-8497-5e4c761c2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
    "# !uv pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c1ca60-28d8-41d2-bc14-532a8983ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90816595-0bf2-4017-8faf-935621ec08fb",
   "metadata": {},
   "source": [
    "## Creating an `UnslothFastModelGenerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2b284b-c0cb-487c-b043-ca47340ad86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.generators import UnslothFastModelGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a4468b-0533-4ebb-bc1f-5dcd8cd9baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "generation_cfg = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    eos_token_id=[1, 106],\n",
    "    bos_token_id=2,\n",
    "    max_new_tokens=2048,\n",
    "    pad_token_id=0,\n",
    "    top_p=0.95,\n",
    "    top_k=64,\n",
    "    temperature=0.6,\n",
    "    cache_implementation=\"offloaded\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc9ec36c-fd1b-43b5-94aa-0fd28c26c60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Gemma3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "unsloth_load_kwargs = {\n",
    "    \"max_seq_length\": 2048,  # Choose any for long context!\n",
    "    \"load_in_4bit\": True,\n",
    "    \"load_in_8bit\": False,  # [NEW!] A bit more accurate, uses 2x memory\n",
    "    \"full_finetuning\": False,  # [NEW!] We have full finetuning now!\n",
    "}\n",
    "generator = UnslothFastModelGenerator(\n",
    "    model_name=\"unsloth/gemma-3-4b-it\",\n",
    "    load_model_kwargs=unsloth_load_kwargs,\n",
    "    generation_config=generation_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b06a90e-edf7-49e6-a3f5-d400bc27e443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e3ceb-dc74-4272-938e-f2efa6242f56",
   "metadata": {},
   "source": [
    "### Give it a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab669a6-2869-4c02-b1a3-a7615ddefa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant. Given the user's query, provide a succinct\n",
      "and accurate response. If context is provided, use it in your answer if it helps\n",
      "you to create the most accurate response.\n",
      "\n",
      "<query>\n",
      "What is a Tulip?\n",
      "</query>\n",
      "\n",
      "<context>\n",
      "\n",
      "</context>\n",
      "\n",
      "<response>\n",
      "\n",
      "A tulip is a flowering plant in the genus *Tulipa*, native to Central Asia and Turkey. They are known for their cup-shaped flowers and are often associated with spring.\n",
      "</response>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = generator.generate(query=\"What is a Tulip?\", context=\"\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008dbce-7ca3-4cfe-8cf4-229948f110e4",
   "metadata": {},
   "source": [
    "In Unsloth's Gemma 3 (4B) [cookbook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb), they demonstrate how to use `~transformers.TextStreamer` to stream generation output in real-time rather than waiting for completion. We can apply the same technique here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2cfad0c-330e-4ad0-853d-7f631c23b3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porsche is a German automobile manufacturer known for its high-performance sports cars, luxury vehicles, and SUVs. The company was founded in 1931 by Ferdinand Porsche.\n",
      "\n",
      "</response>\n",
      "<end_of_turn>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nYou are a helpful assistant. Given the user's query, provide a succinct\\nand accurate response. If context is provided, use it in your answer if it helps\\nyou to create the most accurate response.\\n\\n<query>\\nWhat is a Porshe?\\n</query>\\n\\n<context>\\n\\n</context>\\n\\n<response>\\n\\nPorsche is a German automobile manufacturer known for its high-performance sports cars, luxury vehicles, and SUVs. The company was founded in 1931 by Ferdinand Porsche.\\n\\n</response>\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "generator.generate(\n",
    "    query=\"What is a Porshe?\",\n",
    "    context=\"\",\n",
    "    streamer=TextStreamer(generator.tokenizer.unwrapped, skip_prompt=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457ca76-6715-4b48-82dd-0e10fe06ece4",
   "metadata": {},
   "source": [
    "## Let's Build the Rest of our RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4245e0-1106-4112-a0de-591c36de2441",
   "metadata": {},
   "source": [
    "### Define our Retriever and Knowledge Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "092698c5-ed6d-4e9f-95c1-1c56f7ce11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from fed_rag import RAGSystem, RAGConfig\n",
    "from fed_rag.retrievers.huggingface import (\n",
    "    HFSentenceTransformerRetriever,\n",
    ")\n",
    "from fed_rag.knowledge_stores import InMemoryKnowledgeStore\n",
    "from fed_rag.data_structures import KnowledgeNode, NodeType\n",
    "\n",
    "\n",
    "QUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\"\n",
    "CONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\"\n",
    "PRETRAINED_MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# Retriever\n",
    "retriever = HFSentenceTransformerRetriever(\n",
    "    query_model_name=QUERY_ENCODER_NAME,\n",
    "    context_model_name=CONTEXT_ENCODER_NAME,\n",
    "    load_model_at_init=False,\n",
    ")\n",
    "\n",
    "# Knowledge store\n",
    "knowledge_store = InMemoryKnowledgeStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b040a-34d3-4e4d-a3ab-9dad5706b620",
   "metadata": {},
   "source": [
    "### Add some knowledge to the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "349199a9-e3e6-4e57-9e19-962050b1bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = [\n",
    "    \"Retrieval-Augmented Generation (RAG) combines retrieval with generation.\",\n",
    "    \"LLMs can hallucinate information when they lack context.\",\n",
    "]\n",
    "knowledge_nodes = [\n",
    "    KnowledgeNode(\n",
    "        node_type=\"text\",\n",
    "        embedding=retriever.encode_context(ct).tolist(),\n",
    "        text_content=ct,\n",
    "    )\n",
    "    for ct in text_chunks\n",
    "]\n",
    "knowledge_store.load_nodes(knowledge_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f169e3eb-2bf1-4b84-ba23-75b2baa834f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_store.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ece457-7071-4602-99ef-0403f10f4ed1",
   "metadata": {},
   "source": [
    "### Assemble the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e2f1a7c-32d5-462c-8c99-58af59e68f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG system\n",
    "rag_system = RAGSystem(\n",
    "    retriever=retriever,\n",
    "    generator=generator,\n",
    "    knowledge_store=knowledge_store,\n",
    "    rag_config=RAGConfig(top_k=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25157614-c52c-4767-a2cd-bad8e46ea906",
   "metadata": {},
   "source": [
    "## Time to Fine-tune!\n",
    "\n",
    "Now, that we have our RAG system defined, let's proceed with fine-tuning the generator with the [RALT](https://vectorinstitute.github.io/fed-rag/getting_started/tutorials/ralt/) method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5072db-bcc1-4583-86c6-083f7c7f0340",
   "metadata": {},
   "source": [
    "### Let's first add our LoRA adapters\n",
    "\n",
    "In order to do so, we use the `to_peft()` method, which under the hood will call the `FastModel.get_peft_model()` to build the `PeftModel`, and then set it as this generators model. In other words, the underlying model is currently a `PreTrainedModel`, but after executing the next cell, it will be a `PeftModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9762f273-e253-4914-8833-fa41c322f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `base_model.model.vision_tower.vision_model` require gradients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UnslothFastModelGenerator(model_name='unsloth/gemma-3-4b-it', generation_config=GenerationConfig {\n",
       "  \"bos_token_id\": 2,\n",
       "  \"cache_implementation\": \"hybrid\",\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    1,\n",
       "    106\n",
       "  ],\n",
       "  \"max_new_tokens\": 2048,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_k\": 64,\n",
       "  \"top_p\": 0.95\n",
       "}\n",
       ", load_model_kwargs={'max_seq_length': 2048, 'load_in_4bit': True, 'load_in_8bit': False, 'full_finetuning': False})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.to_peft(\n",
    "    finetune_vision_layers=False,  # Turn off for just text!\n",
    "    finetune_language_layers=True,  # Should leave on!\n",
    "    finetune_attention_modules=True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules=True,  # SHould leave on always!\n",
    "    r=8,  # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha=8,  # Recommended alpha == r at least\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b0072c2-73f1-499d-bd03-f00df2359fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1cb105e-6ee2-4ee8-81ed-fa5498a81d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "isinstance(generator.model, PeftModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed54412-3d2e-4ecf-9c8e-fd0107433a54",
   "metadata": {},
   "source": [
    "### The Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7f863f3-c2a9-4c1a-b4b4-2f9aaafa2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(\n",
    "    # examples from Commonsense QA\n",
    "    {\n",
    "        \"query\": [\n",
    "            \"The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\",\n",
    "            \"Sammy wanted to go to where the people were.  Where might he go?\",\n",
    "            \"To locate a choker not located in a jewelry box or boutique where would you go?\",\n",
    "            \"Google Maps and other highway and street GPS services have replaced what?\",\n",
    "            \"The fox walked from the city into the forest, what was it looking for?\",\n",
    "        ],\n",
    "        \"response\": [\n",
    "            \"ignore\",\n",
    "            \"populated areas\",\n",
    "            \"jewelry store\",\n",
    "            \"atlas\",\n",
    "            \"natural habitat\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ef95c-c0f6-4a46-a7bb-40aefed71dab",
   "metadata": {},
   "source": [
    "Since, Unsloth essentially applies efficiencies to the training processes of `~transformer.PreTrainedModels` as well as `~peft.PeftModels`, we can make full use of our HuggingFace generator trainer classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bfc80a9-b72b-40c7-9c9f-1aa67bb30fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.trainers.huggingface.ralt import HuggingFaceTrainerForRALT\n",
    "\n",
    "# the trainer object\n",
    "generator_trainer = HuggingFaceTrainerForRALT(\n",
    "    rag_system=rag_system,\n",
    "    train_dataset=train_dataset,\n",
    "    # training_arguments=...  # Optional ~transformers.TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97d556f0-f6eb-4252-a0aa-07928d6611d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5 | Num Epochs = 3 | Total steps = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 16,394,240/4,000,000,000 (0.41% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "result = generator_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93d40c16-4219-470f-9168-cf711c744a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainResult(loss=3.530837059020996)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
