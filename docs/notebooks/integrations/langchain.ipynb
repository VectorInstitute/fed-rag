{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78004a67-2dea-4da4-a87f-819fde322364",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/VectorInstitute/fed-rag/blob/main/docs/notebooks/integrations/llama_index.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "_(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31024c3-3f4f-4909-94c2-ef5bc7de1f5a",
   "metadata": {},
   "source": [
    "# Using LlamaIndex for Inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "After fine-tuning your RAG system to achieve desired performance, you'll want to\n",
    "deploy it for inference. While FedRAG's `RAGSystem` provides complete inference\n",
    "capabilities out of the box, you may need additional features for production deployments\n",
    "or want to leverage the ecosystem of existing RAG frameworks.\n",
    "\n",
    "FedRAG offers a seamless integration into [LangChain](https://github.com/langchain-ai/langchain) through our bridges system, giving you the best of both worlds: FedRAG's fine-tuning capabilities combined\n",
    "with the extensive inference features of LangChain.\n",
    "\n",
    "In this example, we demonstrate how you can convert a `RAGSystem` into a tuple consisting of `~langchain_core.vectorstores.VectorStore` and `~langchain_core.language_models.BaseLLM`. The former can then be transformed into a `~langchain_core.vectorestores.VectorStoreRetriever` using the `as_retriever()` method, enabling the creation of a complete QA pipeline whith LangChain's LCEL.\n",
    "\n",
    "__NOTE:__\n",
    "Streaming and async functionalities are not yet supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248dec17-5225-4d16-8aad-00c0101c7a4b",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad8f32-d63a-4b9f-8406-79130df4945e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: fed-rag[huggingface,llama-index]\n"
     ]
    }
   ],
   "source": [
    "# If running in a Google Colab, the first attempt at installing fed-rag may fail,\n",
    "# though for reasons unknown to me yet, if you try a second time, it magically works...\n",
    "!pip install fed-rag[huggingface,langchain] -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db0e246-6757-4c67-98b8-347173d186b7",
   "metadata": {},
   "source": [
    "## Setup â€” The RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edf7eae4-e5c8-4f97-8a82-a8c4ddbaea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "\n",
    "from fed_rag import RAGSystem, RAGConfig\n",
    "from fed_rag.generators.huggingface import HFPretrainedModelGenerator\n",
    "from fed_rag.retrievers.huggingface import (\n",
    "    HFSentenceTransformerRetriever,\n",
    ")\n",
    "from fed_rag.knowledge_stores import InMemoryKnowledgeStore\n",
    "from fed_rag.data_structures import KnowledgeNode\n",
    "\n",
    "\n",
    "QUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\"\n",
    "CONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\"\n",
    "PRETRAINED_MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# Retriever\n",
    "retriever = HFSentenceTransformerRetriever(\n",
    "    query_model_name=QUERY_ENCODER_NAME,\n",
    "    context_model_name=CONTEXT_ENCODER_NAME,\n",
    "    load_model_at_init=False,\n",
    ")\n",
    "\n",
    "# Generator\n",
    "generation_cfg = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    eos_token_id=151643,\n",
    "    bos_token_id=151643,\n",
    "    max_new_tokens=2048,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "    cache_implementation=\"offloaded\",\n",
    "    stop_strings=\"</response>\",\n",
    ")\n",
    "generator = HFPretrainedModelGenerator(\n",
    "    model_name=PRETRAINED_MODEL_NAME,\n",
    "    load_model_at_init=False,\n",
    "    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n",
    "    generation_config=generation_cfg,\n",
    ")\n",
    "\n",
    "# Knowledge store\n",
    "knowledge_store = InMemoryKnowledgeStore()\n",
    "\n",
    "\n",
    "# Create the RAG system\n",
    "rag_system = RAGSystem(\n",
    "    retriever=retriever,\n",
    "    generator=generator,\n",
    "    knowledge_store=knowledge_store,\n",
    "    rag_config=RAGConfig(top_k=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2f188-5a6c-4092-834e-b45d321a1eba",
   "metadata": {},
   "source": [
    "### Add some knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c13ca5a-83da-44af-9bc9-c75274856248",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = [\n",
    "    \"Retrieval-Augmented Generation (RAG) combines retrieval with generation.\",\n",
    "    \"LLMs can hallucinate information when they lack context.\",\n",
    "]\n",
    "knowledge_nodes = [\n",
    "    KnowledgeNode(\n",
    "        node_type=\"text\",\n",
    "        embedding=retriever.encode_context(ct).tolist(),\n",
    "        text_content=ct,\n",
    "    )\n",
    "    for ct in text_chunks\n",
    "]\n",
    "knowledge_store.load_nodes(knowledge_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0389b3-da82-4e37-915a-1326a4573b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_system.knowledge_store.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df7a0b-b0f2-45b3-9cc8-c9d7d8997056",
   "metadata": {},
   "source": [
    "## Using the Bridge\n",
    "\n",
    "Converting your RAG system to LangChain objects is seamless since the bridge\n",
    "functionality is already built into the `RAGSystem` class. The `RAGSystem` inherits\n",
    "from `LangChainBridgeMixin`, which provides the `to_langchain()` method for\n",
    "effortless conversion.\n",
    "\n",
    "__NOTE__: The `to_langchain()` method returns a tuple consisting of `FedRAGVectorStore` and `FedRAGLLM` objects, which are custom implementation of the `~langchain_core.vectorstores.VectorStore` and `~langchain_core.language_models.BaseLLM` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87648b84-00a2-4bb1-b30e-07b5b9a090b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: LLMs can hallucinate information when they lack context., Score: 0.5453173017158688\n",
      "Content: Retrieval-Augmented Generation (RAG) combines retrieval with generation., Score: 0.5065647377479774\n",
      "--------------------------------------------------------------------------------\n",
      "Content: LLMs can hallucinate information when they lack context.\n",
      "Content: Retrieval-Augmented Generation (RAG) combines retrieval with generation.\n",
      "--------------------------------------------------------------------------------\n",
      "Autonomous agents are systems that can perform tasks without requiring human input. They use algorithms and data to make decisions and execute actions automatically. For example, a self-driving car or a robot that can navigate a room without human assistance.\n",
      "</response>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the LangChain objects\n",
    "vector_store, llm = rag_system.to_langchain()\n",
    "\n",
    "# Search the vectore store directly\n",
    "query = \"What happens if LLMs lack context?\"\n",
    "results = vector_store.similarity_search_with_score(query, k=2)\n",
    "for doc, score in results:\n",
    "    print(f\"Content: {doc.page_content}, Score: {score}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Or, convert it to a retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "results = retriever.invoke(query)\n",
    "for doc in results:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Or, create a complete RAG chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from fed_rag.base.generator import DEFAULT_PROMPT_TEMPLATE\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vector_store.as_retriever() | format_docs,\n",
    "        \"query\": RunnablePassthrough(),\n",
    "    }\n",
    "    | PromptTemplate.from_template(DEFAULT_PROMPT_TEMPLATE)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = qa_chain.invoke(\"What are autonomous agents?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270a3335-66ec-47a3-ac2f-1592a6496b72",
   "metadata": {},
   "source": [
    "### Modifying Knowledge\n",
    "\n",
    "In addition to querying the bridged index, you can also make changes to the\n",
    "underlying KnowledgeStore using LangChains's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39707b5a-82e1-4977-be72-96ce9f6714d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eceb02ba-a206-441c-b268-c5ef664a32c5',\n",
       " 'c5da9720-e3ce-4b91-92bf-203e3953a408']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = vector_store.add_texts(\n",
    "    texts=[\"some arbitrary text\", \"some other arbitrary text\"],\n",
    "    metadatas=[{\"source\": \"fed-rag\"}, {\"source\": \"fed-rag\"}],\n",
    ")\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff1bb880-a526-4dd7-8751-06f08689092d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that what we added above is indeed in the knowledge store\n",
    "rag_system.knowledge_store.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2febaa2d-d28f-46d2-8de7-741fe28f322f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also delete nodes\n",
    "vector_store.delete(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abc736ec-6dff-4c54-b145-8c65592a68c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that what we deleted above is indeed removed from the knowledge store\n",
    "rag_system.knowledge_store.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d26a2-ae85-425d-81d3-bb613f25e8ad",
   "metadata": {},
   "source": [
    "## Bridge Metadata\n",
    "\n",
    "To view the metadata of the LangChain bridge, you can access the class attribute\n",
    "`bridge` of the `RAGSystem` class, which is a dictionary object that contains the `BridgeMetadata` for all of the installed bridges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49206bb2-1fec-491d-9099-aa3b74fd181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama-index-core': {'bridge_version': '0.1.0', 'framework': 'llama-index-core', 'compatible_versions': {'min': '0.12.35'}, 'method_name': 'to_llamaindex'}, 'langchain-core': {'bridge_version': '0.1.0', 'framework': 'langchain-core', 'compatible_versions': {'min': '0.3.62'}, 'method_name': 'to_langchain'}}\n",
      "{'bridge_version': '0.1.0', 'framework': 'langchain-core', 'compatible_versions': {'min': '0.3.62'}, 'method_name': 'to_langchain'}\n"
     ]
    }
   ],
   "source": [
    "# see available bridges\n",
    "print(RAGSystem.bridges)\n",
    "\n",
    "# see the LangChain bridge metadata\n",
    "print(RAGSystem.bridges[\"langchain-core\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2979fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
