{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31024c3-3f4f-4909-94c2-ef5bc7de1f5a",
   "metadata": {},
   "source": [
    "# Using LlamaIndex for Inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "After fine-tuning your RAG system to achieve desired performance, you'll want to\n",
    "deploy it for inference. While FedRAG's `RAGSystem` provides complete inference\n",
    "capabilities out of the box, you may need additional features for production deployments\n",
    "or want to leverage the ecosystem of existing RAG frameworks.\n",
    "\n",
    "FedRAG offers a seamless integration into [LlamaIndex](https://github.com/run-llama/llama_index) through our bridges system,\n",
    "giving you the best of both worlds: FedRAG's fine-tuning capabilities combined\n",
    "with the extensive inference features of LlamaIndex.\n",
    "\n",
    "In this example, we demonstrate how you can convert a `RAGSystem` to a\n",
    "`~llama_index.BaseManagedIndex` from which you can obtain `~llama_index.QueryEngine`\n",
    "as well as `~llama_index.Retriever`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248dec17-5225-4d16-8aad-00c0101c7a4b",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad8f32-d63a-4b9f-8406-79130df4945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in a Google Colab, the first attempt at installing fed-rag may fail,\n",
    "# though for reasons unknown to me yet, if you try a second time, it magically works...\n",
    "!pip install fed-rag[huggingface,llama-index] -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db0e246-6757-4c67-98b8-347173d186b7",
   "metadata": {},
   "source": [
    "## Setup â€” The RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf7eae4-e5c8-4f97-8a82-a8c4ddbaea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "from fed_rag import RAGSystem, RAGConfig\n",
    "from fed_rag.generators.huggingface import HFPretrainedModelGenerator\n",
    "from fed_rag.retrievers.huggingface import (\n",
    "    HFSentenceTransformerRetriever,\n",
    ")\n",
    "from fed_rag.knowledge_stores import InMemoryKnowledgeStore\n",
    "from fed_rag.types import KnowledgeNode, NodeType\n",
    "\n",
    "\n",
    "QUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\"\n",
    "CONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\"\n",
    "PRETRAINED_MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# Retriever\n",
    "retriever = HFSentenceTransformerRetriever(\n",
    "    query_model_name=QUERY_ENCODER_NAME,\n",
    "    context_model_name=CONTEXT_ENCODER_NAME,\n",
    "    load_model_at_init=False,\n",
    ")\n",
    "\n",
    "# Generator\n",
    "generation_cfg = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    eos_token_id=151643,\n",
    "    bos_token_id=151643,\n",
    "    max_new_tokens=2048,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "    cache_implementation=\"offloaded\",\n",
    "    stop_strings=\"</response>\",\n",
    ")\n",
    "generator = HFPretrainedModelGenerator(\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B\",\n",
    "    load_model_at_init=False,\n",
    "    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n",
    "    generation_config=generation_cfg,\n",
    ")\n",
    "\n",
    "# Knowledge store\n",
    "knowledge_store = InMemoryKnowledgeStore()\n",
    "\n",
    "\n",
    "# Create the RAG system\n",
    "rag_system = RAGSystem(\n",
    "    retriever=retriever,\n",
    "    generator=generator,\n",
    "    knowledge_store=knowledge_store,\n",
    "    rag_config=RAGConfig(top_k=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2f188-5a6c-4092-834e-b45d321a1eba",
   "metadata": {},
   "source": [
    "### Add some knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c13ca5a-83da-44af-9bc9-c75274856248",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = [\n",
    "    \"Retrieval-Augmented Generation (RAG) combines retrieval with generation.\",\n",
    "    \"LLMs can hallucinate information when they lack context.\",\n",
    "]\n",
    "knowledge_nodes = [\n",
    "    KnowledgeNode(\n",
    "        node_type=\"text\",\n",
    "        embedding=retriever.encode_context(ct).tolist(),\n",
    "        text_content=ct,\n",
    "    )\n",
    "    for ct in text_chunks\n",
    "]\n",
    "knowledge_store.load_nodes(knowledge_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0389b3-da82-4e37-915a-1326a4573b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_system.knowledge_store.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df7a0b-b0f2-45b3-9cc8-c9d7d8997056",
   "metadata": {},
   "source": [
    "## Using the Bridge\n",
    "\n",
    "Converting your RAG system to a LlamaIndex object is seamless since the bridge\n",
    "functionality is already built into the `RAGSystem` class. The `RAGSystem` inherits\n",
    "from `LlamaIndexBridgeMixin`, which provides the `to_llamaindex()` method for\n",
    "effortless conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87648b84-00a2-4bb1-b30e-07b5b9a090b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant. Given the user's question, provide a succinct\n",
      "and accurate response. If context is provided, use it in your answer if it helps\n",
      "you to create the most accurate response.\n",
      "\n",
      "<question>\n",
      "Context information is below.\n",
      "---------------------\n",
      "LLMs can hallucinate information when they lack context.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What happens if LLMs lack context?\n",
      "Answer: \n",
      "</question>\n",
      "\n",
      "<context>\n",
      "\n",
      "</context>\n",
      "\n",
      "<response>\n",
      "\n",
      "Assistant: If LLMs lack context, they may hallucinate information, which means they generate incorrect or irrelevant information that is not based on the available context. This can lead to inaccurate results and potentially harm the user's decision-making process. To avoid this, it is important to provide LLMs with relevant and accurate context before using them. \n",
      "\n",
      "Score: 0.5453173113645673, Content: Node ID: 9eaf96fe-c784-4cac-b423-518e063a936b\n",
      "Text: LLMs can hallucinate information when they lack context.\n"
     ]
    }
   ],
   "source": [
    "# Create a llamaindex object\n",
    "index = rag_system.to_llamaindex()\n",
    "\n",
    "# Use it like any other LlamaIndex object to get a query engine\n",
    "query = \"What happens if LLMs lack context?\"\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query)\n",
    "print(response, \"\\n\")\n",
    "\n",
    "# Or, get a retriever\n",
    "retriever = index.as_retriever()\n",
    "results = retriever.retrieve(query)\n",
    "for node in results:\n",
    "    print(f\"Score: {node.score}, Content: {node.node}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517fc89-29c0-438c-9213-e6bebc1c204a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
