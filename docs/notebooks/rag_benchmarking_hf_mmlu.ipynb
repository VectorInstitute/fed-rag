{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683e31db-0a58-43c7-bb70-efdbe655a735",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/VectorInstitute/fed-rag/blob/main/docs/notebooks/rag_benchmarking_hf_mmlu.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "_(NOTE: if running on Colab, you will need to supply a HFToken. Also, you'll need to change the runtime to a L4.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930123f4-a6d3-4a0d-953f-2bd84cb415c4",
   "metadata": {},
   "source": [
    "# Benchmarking RAG Systems with FedRAG and Gemma 3n\n",
    "\n",
    "In this note book, we demonstrate how one benchmark a RAG system with the `fed-rag` library with Googleâ€™s Gemma 3n model on the MMLU dataset.\n",
    "\n",
    "We will walk through the following steps:\n",
    "\n",
    "1. Build your `RAGSystem` to be benchmarked (with a retriever, knowledge store, and LLM generator)\n",
    "2. Create a `Benchmarker` object to manage evaluation runs\n",
    "3. Choose your `Benchmark (MMLU)` and run it using the `Benchmarker`\n",
    "\n",
    "In this notebook, we'll make use of the `huggingface-evals` extra which will allow us to utilize the benchmarks defined in the `fed_rag.evals.benchmarks.huggingface` module.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- A Hugging Face account with access to Gemma 3n, and your Hugging Face API token (see cell below for setup).\n",
    "\n",
    "- (Colab) Set your runtime to a GPU instance with sufficient RAM (L4 or better recommended)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497c18e-0c8d-4e24-9faa-b19bed277087",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d8fc1-90a3-4f8d-a68a-cd6a43572913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest fed-rag, transformers, and timm.\n",
    "!pip install fed-rag[huggingface,huggingface-evals]\n",
    "\n",
    "# Update libraries needed for Gemma3n.\n",
    "!pip install --upgrade transformers timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587dc55e-d6f2-40f2-a3d2-64e304e6193d",
   "metadata": {},
   "source": [
    "## Build the RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e11c5b-bd44-43c2-8150-0b3be22946de",
   "metadata": {},
   "source": [
    "### Knowledge Store and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c807325-2ac6-417a-a0f7-7b2ae3762b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.knowledge_stores.in_memory import InMemoryKnowledgeStore\n",
    "from fed_rag.retrievers.huggingface.hf_sentence_transformer import (\n",
    "    HFSentenceTransformerRetriever,\n",
    ")\n",
    "\n",
    "knowledge_store = InMemoryKnowledgeStore()\n",
    "\n",
    "retriever = HFSentenceTransformerRetriever(\n",
    "    query_model_name=\"nthakur/dragon-plus-query-encoder\",\n",
    "    context_model_name=\"nthakur/dragon-plus-context-encoder\",\n",
    "    load_model_at_init=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e5aac-6615-4df1-a0a9-62d3da3a0195",
   "metadata": {},
   "source": [
    "### Let's Add Some Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfff24c-1286-4db7-a144-b2104b9320ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small sample from the Dec 2021 Wikipedia dump\n",
    "text_chunks = [\n",
    "    {\n",
    "        \"id\": \"140\",\n",
    "        \"title\": \"History of marine biology\",\n",
    "        \"section\": \"James Cook\",\n",
    "        \"text\": \" James Cook is well known for his voyages of exploration for the British Navy in which he mapped out a significant amount of the world's uncharted waters. Cook's explorations took him around the world twice and led to countless descriptions of previously unknown plants and animals. Cook's explorations influenced many others and led to a number of scientists examining marine life more closely. Among those influenced was Charles Darwin who went on to make many contributions of his own. \",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"141\",\n",
    "        \"title\": \"History of marine biology\",\n",
    "        \"section\": \"Charles Darwin\",\n",
    "        \"text\": \" Charles Darwin, best known for his theory of evolution, made many significant contributions to the early study of marine biology. He spent much of his time from 1831 to 1836 on the voyage of HMS Beagle collecting and studying specimens from a variety of marine organisms. It was also on this expedition where Darwin began to study coral reefs and their formation. He came up with the theory that the overall growth of corals is a balance between the growth of corals upward and the sinking of the sea floor. He then came up with the idea that wherever coral atolls would be found, the central island where the coral had started to grow would be gradually subsiding\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"142\",\n",
    "        \"title\": \"History of marine biology\",\n",
    "        \"section\": \"Charles Wyville Thomson\",\n",
    "        \"text\": \" Another influential expedition was the voyage of HMS Challenger from 1872 to 1876, organized and later led by Charles Wyville Thomson. It was the first expedition purely devoted to marine science. The expedition collected and analyzed thousands of marine specimens, laying the foundation for present knowledge about life near the deep-sea floor. The findings from the expedition were a summary of the known natural, physical and chemical ocean science to that time.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a09878-fc4e-40a6-bdc7-e312402b7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.data_structures import KnowledgeNode, NodeType\n",
    "\n",
    "# create knowledge nodes\n",
    "nodes = []\n",
    "texts = []\n",
    "for c in text_chunks:\n",
    "    text = c.pop(\"text\")\n",
    "    title = c.pop(\"title\")\n",
    "    section = c.pop(\"section\")\n",
    "    context_text = f\"title: {title}\\nsection: {section}\\ntext: {text}\"\n",
    "    texts.append(context_text)\n",
    "\n",
    "# batch encode\n",
    "batch_embeddings = retriever.encode_context(texts)\n",
    "\n",
    "for jx, c in enumerate(text_chunks):\n",
    "    node = KnowledgeNode(\n",
    "        embedding=batch_embeddings[jx].tolist(),\n",
    "        node_type=NodeType.TEXT,\n",
    "        text_content=texts[jx],\n",
    "        metadata=c,\n",
    "    )\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae62642-0e6c-4d04-9ac1-315d169e86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nodes\n",
    "knowledge_store.load_nodes(nodes)\n",
    "print(\"Knowledge nodes loaded:\", knowledge_store.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5247e79-3c23-446c-9d74-28dfa0075eda",
   "metadata": {},
   "source": [
    "### Define an LLM Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25695672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# This will prompt you to login to huggingface\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.generators.huggingface.hf_multimodal_model import (\n",
    "    HFMultimodalModelGenerator,\n",
    ")\n",
    "\n",
    "generator = HFMultimodalModelGenerator(\n",
    "    model_name=\"google/gemma-3n-e2b-it\",  # can be any HF model name or path\n",
    "    load_model_at_init=True,  # defaults True, loads model on init\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b63594",
   "metadata": {},
   "source": [
    "If running on CPU and see OOM errors, switch device accordingly. (L4 is recommended if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45f80b",
   "metadata": {},
   "source": [
    "### Assemble the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d7eb23-3d3b-4859-b818-e78692ab4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag import RAGSystem, RAGConfig\n",
    "\n",
    "rag_config = RAGConfig(top_k=2)\n",
    "rag_system = RAGSystem(\n",
    "    knowledge_store=knowledge_store,  # knowledge store loaded from knowledge_store.py\n",
    "    generator=generator,\n",
    "    retriever=retriever,\n",
    "    rag_config=rag_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a5c38-476f-402e-b1cc-aaed0c5e7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a query\n",
    "response = rag_system.query(\"Who is James Cook?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a607e79a",
   "metadata": {},
   "source": [
    "## Perform Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96af83-b555-4e83-a8f7-93c6501214f8",
   "metadata": {},
   "source": [
    "### Create `Benchmarker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea97c9a-48e2-4db9-ac68-6685c133185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.evals import Benchmarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500094bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarker = Benchmarker(rag_system=rag_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46236860-5bfd-4113-b9dd-22f13f0ff87c",
   "metadata": {},
   "source": [
    "### Get the desired Benchmark (MMLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccbfa9-bf28-4e7d-a197-4a5a44cace45",
   "metadata": {},
   "source": [
    "For this notebook, we'll use a HuggingFace benchmark, namely the MMLU one. The recommended pattern for loading benchmarks from `fed_rag` is illustrated in the cells found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f382e2-e879-4123-99c9-c7e8ffc24c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fed_rag.evals.benchmarks as benchmarks\n",
    "\n",
    "# define the mmlu benchmark\n",
    "mmlu = benchmarks.HuggingFaceMMLU(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd6d39-96fc-4f84-b592-d9b03ec5efc5",
   "metadata": {},
   "source": [
    "In the above, we set `streaming` to `True` since the underlying dataset is quite large. By doing so, we can get a stream of `~fed_rag.data_structures.BenchmarkExample` that we can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0695db0-40b3-4365-b12b-f70a92cf1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_stream = mmlu.as_stream()\n",
    "next(example_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fffb850-24a8-4524-9670-d708b5932c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_stream.close()  # close the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfc49d-5a81-4175-bdf3-c6168f2368a4",
   "metadata": {},
   "source": [
    "### Define our Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e4069-818e-4238-a344-d559a441521d",
   "metadata": {},
   "source": [
    "In this notebook, we'll make use of the `ExactMatchEvaluationMetric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb4bfc8b-18a3-480e-b85c-95031f7883b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.evals.metrics import ExactMatchEvaluationMetric\n",
    "\n",
    "metric = ExactMatchEvaluationMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9e77d-279e-4997-b939-7b5ec4808373",
   "metadata": {},
   "source": [
    "All `BaseEvaluationMetric` are direcly callable (i.e., their special `__call__` methods are implemented). We can see the signature of this method by using the `help` builtin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa70e54-5c48-4072-8d17-755712603470",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(metric.__call__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5908e-4a3a-402d-af41-b2e33b7778ba",
   "metadata": {},
   "source": [
    "Exact match is case insensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ef24191-85e7-4ede-af4e-3abf096d9cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(prediction=\"A\", actual=\"A\")  # scores 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "739931be-0302-4afb-9a74-e9ae73c5ab74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(prediction=\"A\", actual=\"a\")  # also scores 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "204b685a-fd2f-47af-ac09-56ea7d0e67e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(prediction=\"A\", actual=\"b\")  # scores 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80c253",
   "metadata": {},
   "source": [
    "### Multiple-Choice Prompt: Default and Customization\n",
    "\n",
    "fed-rag sets a default prompt for MMLU so that LLMs like Gemma 3n answer with only a single letter (A, B, C, or D). This keeps the evaluation automatic and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu.prompt_template = \"\"\"\n",
    "{question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "\n",
    "You MUST answer ONLY with a single letter (A, B, C, or D). Do NOT provide any explanation, reasoning, or extra text. Answers with any additional text are considered incorrect.\n",
    "\n",
    "Example of a correct answer:\n",
    "B\n",
    "\n",
    "Example of an incorrect answer:\n",
    "B. Because...\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc0039",
   "metadata": {},
   "source": [
    "Customizing the prompt:\n",
    "If you want to experiment with different prompting strategies, just assign your own prompt to `mmlu.prompt_template` above to override the default prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b43ff3-a92b-4226-88d2-f97ec9ea27b6",
   "metadata": {},
   "source": [
    "### Run the benchmark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378400e-15c1-4680-a970-3e2e3cce7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = benchmarker.run(\n",
    "    benchmark=mmlu,\n",
    "    metric=metric,\n",
    "    is_streaming=True,\n",
    "    num_examples=3,  # for quick testing only run it on 3 examples\n",
    "    agg=\"avg\",  # can be 'avg', 'sum', 'max', 'min'\n",
    "    save_evaluations=True,  # needs fed-rag v0.0.23 or above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9350d-ff19-48d4-922f-22de51539ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121b1ce-05af-4354-b65f-27e6a1406b72",
   "metadata": {},
   "source": [
    "### Load evaluated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f655b1c-e659-40ef-bc59-5a92dcb3fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.evals.utils import load_evaluations\n",
    "\n",
    "evaluations = load_evaluations(result.evaluations_file)\n",
    "\n",
    "print(\"Score for first example:\", evaluations[0].score)\n",
    "print(\"Model output for second example:\", evaluations[1].rag_response.response)\n",
    "print(\"Prompt template used:\\n\", mmlu.prompt_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
