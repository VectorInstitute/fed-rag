{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#fedrag","title":"FedRAG","text":""},{"location":"#simplified-rag-fine-tuning-across-centralized-or-federated-architectures","title":"Simplified RAG fine-tuning across centralized or federated architectures Advanced RAG fine-tuning Work with your tools Lightweight abstractions","text":"<ul> <li> <p>Comprehensive support for state-of-the-art RAG fine-tuning methods that can be federated with ease.</p> <p> Getting started</p> </li> <li> <p></p> <p>Seamlessly integrates with popular frameworks including HuggingFace, and LlamaIndex \u2014 use the tools you already know.</p> <p> In-Depth Examples</p> </li> <li> <p></p> <p>Clean, intuitive abstractions that simplify RAG fine-tuning while maintaining full flexibility and control.</p> <p> API Reference</p> </li> </ul>"},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/bridges/","title":"Base Bridges Module","text":"<p>Base Bridge</p>"},{"location":"api_reference/bridges/#src.fed_rag.base.bridge.BaseBridgeMixin","title":"BaseBridgeMixin","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base Bridge Class.</p> Source code in <code>src/fed_rag/base/bridge.py</code> <pre><code>class BaseBridgeMixin(BaseModel):\n    \"\"\"Base Bridge Class.\"\"\"\n\n    # Version of the bridge implementaiton\n    _bridge_version: ClassVar[str]\n    _bridge_extra: ClassVar[Optional[str | None]]\n    _framework: ClassVar[str]\n    _compatible_versions: ClassVar[list[str]]\n    _method_name: ClassVar[str]\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init_subclass__(cls, **kwargs: Any):\n        \"\"\"Register bridge into ~RAGSystem bridge registry.\"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Register this bridge's metadata to the parent RAGSystem\n        for base in cls.__mro__:\n            if base.__name__ in [\n                \"_RAGSystem\",\n                \"_NoEncodeRAGSystem\",\n            ] and hasattr(base, \"_register_bridge\"):\n                metadata = cls.get_bridge_metadata()\n\n                # validate method exists\n                if not hasattr(cls, metadata[\"method_name\"]):\n                    raise MissingSpecifiedConversionMethod(\n                        f\"Bridge mixin for `{metadata['framework']}` is missing conversion method `{metadata['method_name']}`.\"\n                    )\n                base._register_bridge(metadata)\n                break\n\n    @classmethod\n    def get_bridge_metadata(cls) -&gt; BridgeMetadata:\n        metadata: BridgeMetadata = {\n            \"bridge_version\": cls._bridge_version,\n            \"framework\": cls._framework,\n            \"compatible_versions\": cls._compatible_versions,\n            \"method_name\": cls._method_name,\n        }\n        return metadata\n\n    @classmethod\n    def _validate_framework_installed(cls) -&gt; None:\n        if importlib.util.find_spec(cls._framework.replace(\"-\", \"_\")) is None:\n            missing_package_or_extra = (\n                f\"fed-rag[{cls._bridge_extra}]\"\n                if cls._bridge_extra\n                else cls._framework\n            )\n            msg = (\n                f\"`{cls._framework}` module is missing but needs to be installed. \"\n                f\"To fix please run `pip install {missing_package_or_extra}`.\"\n            )\n            raise MissingExtraError(msg)\n</code></pre>"},{"location":"api_reference/bridges/#src.fed_rag.base.bridge.BaseBridgeMixin.__init_subclass__","title":"__init_subclass__","text":"<pre><code>__init_subclass__(**kwargs)\n</code></pre> <p>Register bridge into ~RAGSystem bridge registry.</p> Source code in <code>src/fed_rag/base/bridge.py</code> <pre><code>def __init_subclass__(cls, **kwargs: Any):\n    \"\"\"Register bridge into ~RAGSystem bridge registry.\"\"\"\n    super().__init_subclass__(**kwargs)\n\n    # Register this bridge's metadata to the parent RAGSystem\n    for base in cls.__mro__:\n        if base.__name__ in [\n            \"_RAGSystem\",\n            \"_NoEncodeRAGSystem\",\n        ] and hasattr(base, \"_register_bridge\"):\n            metadata = cls.get_bridge_metadata()\n\n            # validate method exists\n            if not hasattr(cls, metadata[\"method_name\"]):\n                raise MissingSpecifiedConversionMethod(\n                    f\"Bridge mixin for `{metadata['framework']}` is missing conversion method `{metadata['method_name']}`.\"\n                )\n            base._register_bridge(metadata)\n            break\n</code></pre>"},{"location":"api_reference/bridges/llamaindex/","title":"LlamaIndex","text":""},{"location":"api_reference/bridges/llamaindex/#src.fed_rag._bridges.llamaindex.LlamaIndexBridgeMixin","title":"LlamaIndexBridgeMixin","text":"<p>               Bases: <code>BaseBridgeMixin</code></p> <p>LlamaIndex Bridge.</p> <p>This mixin adds LlamaIndex conversion capabilities to _RAGSystem. When mixed with an unbridged _RAGSystem, it allows direct conversion to LlamaIndex's BaseManagedIndex through the to_llamaindex() method.</p> Source code in <code>src/fed_rag/_bridges/llamaindex/bridge.py</code> <pre><code>class LlamaIndexBridgeMixin(BaseBridgeMixin):\n    \"\"\"LlamaIndex Bridge.\n\n    This mixin adds LlamaIndex conversion capabilities to _RAGSystem.\n    When mixed with an unbridged _RAGSystem, it allows direct conversion to\n    LlamaIndex's BaseManagedIndex through the to_llamaindex() method.\n    \"\"\"\n\n    _bridge_version = __version__\n    _bridge_extra = \"llama-index\"\n    _framework = \"llama-index\"\n    _compatible_versions = [\"0.12.35\"]\n    _method_name = \"to_llamaindex\"\n\n    def to_llamaindex(self: \"_RAGSystem\") -&gt; \"BaseManagedIndex\":\n        \"\"\"Converts the _RAGSystem to a ~llamaindex.core.BaseManagedIndex.\"\"\"\n        self._validate_framework_installed()\n\n        from fed_rag._bridges.llamaindex._managed_index import (\n            FedRAGManagedIndex,\n        )\n\n        return FedRAGManagedIndex(rag_system=self)\n</code></pre>"},{"location":"api_reference/bridges/llamaindex/#src.fed_rag._bridges.llamaindex.LlamaIndexBridgeMixin.to_llamaindex","title":"to_llamaindex","text":"<pre><code>to_llamaindex()\n</code></pre> <p>Converts the _RAGSystem to a ~llamaindex.core.BaseManagedIndex.</p> Source code in <code>src/fed_rag/_bridges/llamaindex/bridge.py</code> <pre><code>def to_llamaindex(self: \"_RAGSystem\") -&gt; \"BaseManagedIndex\":\n    \"\"\"Converts the _RAGSystem to a ~llamaindex.core.BaseManagedIndex.\"\"\"\n    self._validate_framework_installed()\n\n    from fed_rag._bridges.llamaindex._managed_index import (\n        FedRAGManagedIndex,\n    )\n\n    return FedRAGManagedIndex(rag_system=self)\n</code></pre>"},{"location":"api_reference/data_collators/","title":"Base Data Collator","text":"<p>Base Data Collator</p>"},{"location":"api_reference/data_collators/#src.fed_rag.base.data_collator.BaseDataCollator","title":"BaseDataCollator","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Data Collator.</p> Source code in <code>src/fed_rag/base/data_collator.py</code> <pre><code>class BaseDataCollator(BaseModel, ABC):\n    \"\"\"Base Data Collator.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    rag_system: RAGSystem\n\n    @abstractmethod\n    def __call__(self, features: list[dict[str, Any]], **kwargs: Any) -&gt; Any:\n        \"\"\"Collate examples into a batch.\"\"\"\n</code></pre>"},{"location":"api_reference/data_collators/#src.fed_rag.base.data_collator.BaseDataCollator.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(features, **kwargs)\n</code></pre> <p>Collate examples into a batch.</p> Source code in <code>src/fed_rag/base/data_collator.py</code> <pre><code>@abstractmethod\ndef __call__(self, features: list[dict[str, Any]], **kwargs: Any) -&gt; Any:\n    \"\"\"Collate examples into a batch.\"\"\"\n</code></pre>"},{"location":"api_reference/data_collators/huggingface/","title":"Huggingface","text":"<p>HuggingFace Data Collator For LM-Supervised Retriever Training</p>"},{"location":"api_reference/data_collators/huggingface/#src.fed_rag.data_collators.huggingface.lsr.DataCollatorForLSR","title":"DataCollatorForLSR","text":"<p>               Bases: <code>SentenceTransformerDataCollator</code>, <code>BaseDataCollator</code></p> <p>A HuggingFace DataCollator for LM-Supervised Retrieval.</p> Source code in <code>src/fed_rag/data_collators/huggingface/lsr.py</code> <pre><code>class DataCollatorForLSR(SentenceTransformerDataCollator, BaseDataCollator):\n    \"\"\"A HuggingFace DataCollator for LM-Supervised Retrieval.\"\"\"\n\n    prompt_template: str = Field(default=DEFAULT_PROMPT_TEMPLATE)\n    target_template: str = Field(default=DEFAULT_TARGET_TEMPLATE)\n    default_return_tensors: str = Field(default=\"pt\")\n\n    # Add these fields to make Pydantic aware of them\n    tokenize_fn: Callable = Field(\n        default_factory=lambda: (lambda *args, **kwargs: {})\n    )\n    valid_label_columns: list[str] = Field(\n        default_factory=lambda: [\"label\", \"score\"]\n    )\n    _warned_columns: set = PrivateAttr(\n        default_factory=set\n    )  # exclude=True to match dataclass repr=False\n\n    def __init__(\n        self,\n        rag_system: RAGSystem,\n        prompt_template: str | None = None,\n        target_template: str | None = None,\n        default_return_tensors: str = \"pt\",\n        **kwargs: Any,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        _validate_rag_system(rag_system)\n\n        prompt_template = prompt_template or DEFAULT_PROMPT_TEMPLATE\n        target_template = target_template or DEFAULT_TARGET_TEMPLATE\n\n        # init pydantic base model\n        BaseDataCollator.__init__(\n            self,\n            rag_system=rag_system,\n            prompt_template=prompt_template,\n            target_template=target_template,\n            default_return_tensors=default_return_tensors,\n            tokenize_fn=lambda *args, **kwargs: {},  # Pass this to Pydantic\n            **kwargs,\n        )\n\n    def __call__(\n        self, features: list[dict[str, Any]], return_tensors: str | None = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"Use the features of the dataset in order to get the retrieval and lm-scores.\n\n\n        Args:\n            features (list[Any]): Should contain a 'query' and 'reponse' field.\n            return_tensors (_type_, optional): supports right now only 'pt'\n\n        Returns:\n            dict[str, Any]: a dictionary of ~torch.Tensors with keys 'retrieval_scores'\n                and 'lm_scores'\n            Note that each ('query', 'response') pair generates one fine-tuning instance for LSR.\n        \"\"\"\n        return_tensors = (\n            return_tensors if return_tensors else self.default_return_tensors\n        )\n        if return_tensors != \"pt\":\n            raise FedRAGError(f\"Framework '{return_tensors}' not recognized!\")\n\n        # use rag system to get scores\n        batch_retriever_scores = []\n        batch_lm_scores = []\n        for example in features:\n            query = example.get(\"query\")\n            response = example.get(\"response\")\n\n            # retriever scores - this should participate in gradient computation\n            source_nodes = self.rag_system.retrieve(query)\n            retriever_scores = torch.tensor(\n                [n.score for n in source_nodes], requires_grad=True\n            )\n\n            # lm supervised scores - we don't want these to participate in gradient computation\n            lm_scores = []\n            with torch.no_grad():\n                for chunk in source_nodes:\n                    prompt = self.prompt_template.format(\n                        query=query,\n                        context=chunk.node.get_content()[\"text_content\"],\n                    )\n                    target = self.target_template.format(response=response)\n                    lm_score = self.rag_system.generator.compute_target_sequence_proba(\n                        prompt=prompt, target=target\n                    )\n                    lm_scores.append(lm_score)\n                lm_scores = torch.stack(lm_scores, dim=0)\n\n            # append to batch\n            batch_retriever_scores.append(retriever_scores)\n            batch_lm_scores.append(lm_scores)\n\n        # create torch.Tensors\n        retrieval_scores = torch.stack(batch_retriever_scores, dim=0)\n        lm_scores = torch.stack(batch_lm_scores, dim=0)\n\n        return {\"retrieval_scores\": retrieval_scores, \"lm_scores\": lm_scores}\n</code></pre>"},{"location":"api_reference/data_collators/huggingface/#src.fed_rag.data_collators.huggingface.lsr.DataCollatorForLSR.__call__","title":"__call__","text":"<pre><code>__call__(features, return_tensors=None)\n</code></pre> <p>Use the features of the dataset in order to get the retrieval and lm-scores.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[Any]</code> <p>Should contain a 'query' and 'reponse' field.</p> required <code>return_tensors</code> <code>_type_</code> <p>supports right now only 'pt'</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: a dictionary of ~torch.Tensors with keys 'retrieval_scores' and 'lm_scores'</p> <code>dict[str, Any]</code> <p>Note that each ('query', 'response') pair generates one fine-tuning instance for LSR.</p> Source code in <code>src/fed_rag/data_collators/huggingface/lsr.py</code> <pre><code>def __call__(\n    self, features: list[dict[str, Any]], return_tensors: str | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"Use the features of the dataset in order to get the retrieval and lm-scores.\n\n\n    Args:\n        features (list[Any]): Should contain a 'query' and 'reponse' field.\n        return_tensors (_type_, optional): supports right now only 'pt'\n\n    Returns:\n        dict[str, Any]: a dictionary of ~torch.Tensors with keys 'retrieval_scores'\n            and 'lm_scores'\n        Note that each ('query', 'response') pair generates one fine-tuning instance for LSR.\n    \"\"\"\n    return_tensors = (\n        return_tensors if return_tensors else self.default_return_tensors\n    )\n    if return_tensors != \"pt\":\n        raise FedRAGError(f\"Framework '{return_tensors}' not recognized!\")\n\n    # use rag system to get scores\n    batch_retriever_scores = []\n    batch_lm_scores = []\n    for example in features:\n        query = example.get(\"query\")\n        response = example.get(\"response\")\n\n        # retriever scores - this should participate in gradient computation\n        source_nodes = self.rag_system.retrieve(query)\n        retriever_scores = torch.tensor(\n            [n.score for n in source_nodes], requires_grad=True\n        )\n\n        # lm supervised scores - we don't want these to participate in gradient computation\n        lm_scores = []\n        with torch.no_grad():\n            for chunk in source_nodes:\n                prompt = self.prompt_template.format(\n                    query=query,\n                    context=chunk.node.get_content()[\"text_content\"],\n                )\n                target = self.target_template.format(response=response)\n                lm_score = self.rag_system.generator.compute_target_sequence_proba(\n                    prompt=prompt, target=target\n                )\n                lm_scores.append(lm_score)\n            lm_scores = torch.stack(lm_scores, dim=0)\n\n        # append to batch\n        batch_retriever_scores.append(retriever_scores)\n        batch_lm_scores.append(lm_scores)\n\n    # create torch.Tensors\n    retrieval_scores = torch.stack(batch_retriever_scores, dim=0)\n    lm_scores = torch.stack(batch_lm_scores, dim=0)\n\n    return {\"retrieval_scores\": retrieval_scores, \"lm_scores\": lm_scores}\n</code></pre>"},{"location":"api_reference/data_structures/bridge/","title":"Bridge","text":"<p>Bridge type definitions for fed-rag.</p>"},{"location":"api_reference/data_structures/bridge/#src.fed_rag.data_structures.bridge.BridgeMetadata","title":"BridgeMetadata","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type definition for bridge metadata.</p> Source code in <code>src/fed_rag/data_structures/bridge.py</code> <pre><code>class BridgeMetadata(TypedDict):\n    \"\"\"Type definition for bridge metadata.\"\"\"\n\n    bridge_version: str\n    framework: str\n    compatible_versions: list[str]\n    method_name: str\n</code></pre>"},{"location":"api_reference/data_structures/evals/","title":"Evals","text":"<p>Data structures for fed_rag.evals</p>"},{"location":"api_reference/data_structures/evals/#src.fed_rag.data_structures.evals.BenchmarkExample","title":"BenchmarkExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>Benchmark example data class.</p> Source code in <code>src/fed_rag/data_structures/evals.py</code> <pre><code>class BenchmarkExample(BaseModel):\n    \"\"\"Benchmark example data class.\"\"\"\n\n    query: str\n    response: str\n    context: str | None = None\n</code></pre>"},{"location":"api_reference/data_structures/evals/#src.fed_rag.data_structures.evals.BenchmarkResult","title":"BenchmarkResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Benchmark result data class.</p> Source code in <code>src/fed_rag/data_structures/evals.py</code> <pre><code>class BenchmarkResult(BaseModel):\n    \"\"\"Benchmark result data class.\"\"\"\n\n    score: float\n    metric_name: str\n    num_examples_used: int\n    num_total_examples: int\n    evaluations_file: str | None\n</code></pre>"},{"location":"api_reference/data_structures/evals/#src.fed_rag.data_structures.evals.BenchmarkEvaluatedExample","title":"BenchmarkEvaluatedExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>Evaluated benchmark example data class.</p> Source code in <code>src/fed_rag/data_structures/evals.py</code> <pre><code>class BenchmarkEvaluatedExample(BaseModel):\n    \"\"\"Evaluated benchmark example data class.\"\"\"\n\n    score: float\n    example: BenchmarkExample\n    rag_response: RAGResponse\n\n    def model_dump_json_without_embeddings(self) -&gt; str:\n        return self.model_dump_json(\n            exclude={\n                \"rag_response\": {\n                    \"source_nodes\": {\"__all__\": {\"node\": {\"embedding\"}}}\n                }\n            }\n        )\n</code></pre>"},{"location":"api_reference/data_structures/evals/#src.fed_rag.data_structures.evals.AggregationMode","title":"AggregationMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Mode for aggregating evaluation scores.</p> Source code in <code>src/fed_rag/data_structures/evals.py</code> <pre><code>class AggregationMode(str, Enum):\n    \"\"\"Mode for aggregating evaluation scores.\"\"\"\n\n    AVG = \"avg\"\n    SUM = \"sum\"\n    MAX = \"max\"\n    MIN = \"min\"\n</code></pre>"},{"location":"api_reference/data_structures/knowledge_node/","title":"Knowledge Node","text":"<p>Knowledge Node</p>"},{"location":"api_reference/data_structures/knowledge_node/#src.fed_rag.data_structures.knowledge_node.KnowledgeNode","title":"KnowledgeNode","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/data_structures/knowledge_node.py</code> <pre><code>class KnowledgeNode(BaseModel):\n    model_config = ConfigDict(\n        # ensures that validation is performed for defaulted None values\n        validate_default=True\n    )\n    node_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    embedding: list[float] | None = Field(\n        description=\"Encoded representation of node. If multimodal type, then this is shared embedding between image and text.\",\n        default=None,\n    )\n    node_type: NodeType = Field(description=\"Type of node.\")\n    text_content: str | None = Field(\n        description=\"Text content. Used for TEXT and potentially MULTIMODAL node types.\",\n        default=None,\n    )\n    image_content: bytes | None = Field(\n        description=\"Image content as binary data (decoded from base64)\",\n        default=None,\n    )\n    metadata: dict = Field(\n        description=\"Metadata for node.\", default_factory=dict\n    )\n\n    # validators\n    @field_validator(\"text_content\", mode=\"before\")\n    @classmethod\n    def validate_text_content(\n        cls, value: str | None, info: ValidationInfo\n    ) -&gt; str | None:\n        node_type = info.data.get(\"node_type\")\n        node_type = cast(NodeType, node_type)\n        if node_type == NodeType.TEXT and value is None:\n            raise ValueError(\"NodeType == 'text', but text_content is None.\")\n\n        if node_type == NodeType.MULTIMODAL and value is None:\n            raise ValueError(\n                \"NodeType == 'multimodal', but text_content is None.\"\n            )\n\n        return value\n\n    @field_validator(\"image_content\", mode=\"after\")\n    @classmethod\n    def validate_image_content(\n        cls, value: str | None, info: ValidationInfo\n    ) -&gt; str | None:\n        node_type = info.data.get(\"node_type\")\n        node_type = cast(NodeType, node_type)\n        if node_type == NodeType.IMAGE:\n            if value is None:\n                raise ValueError(\n                    \"NodeType == 'image', but image_content is None.\"\n                )\n\n        if node_type == NodeType.MULTIMODAL:\n            if value is None:\n                raise ValueError(\n                    \"NodeType == 'multimodal', but image_content is None.\"\n                )\n\n        return value\n\n    def get_content(self) -&gt; NodeContent:\n        \"\"\"Return dict of node content.\"\"\"\n        content: NodeContent = {\n            \"image_content\": self.image_content,\n            \"text_content\": self.text_content,\n        }\n        return content\n\n    @field_serializer(\"metadata\")\n    def serialize_metadata(\n        self, metadata: dict[Any, Any] | None\n    ) -&gt; str | None:\n        \"\"\"\n        Custom serializer for the metadata field.\n\n        Will serialize the metadata field into a json string.\n\n        Args:\n            metadata: Metadata dictionary to serialize.\n\n        Returns:\n            Serialized metadata as a json string.\n        \"\"\"\n        if metadata:\n            return json.dumps(metadata)\n        return None\n\n    @field_validator(\"metadata\", mode=\"before\")\n    @classmethod\n    def deserialize_metadata(\n        cls, metadata: dict[Any, Any] | str | None\n    ) -&gt; dict[Any, Any] | None:\n        \"\"\"\n        Custom validator for the metadata field.\n\n        Will deserialize the metadata from a json string if it's a string.\n\n        Args:\n            metadata: Metadata to validate. If it is a json string, it will be deserialized into a dictionary.\n\n        Returns:\n            Validated metadata.\n        \"\"\"\n        if isinstance(metadata, str):\n            deserialized_metadata = json.loads(metadata)\n            return cast(dict[Any, Any], deserialized_metadata)\n        if metadata is None:\n            return {}\n        return metadata\n\n    def model_dump_without_embeddings(self) -&gt; dict[str, Any]:\n        \"\"\"Seriliaze the node without the embedding.\"\"\"\n        return self.model_dump(exclude={\"embedding\"})\n</code></pre>"},{"location":"api_reference/data_structures/knowledge_node/#src.fed_rag.data_structures.knowledge_node.KnowledgeNode.get_content","title":"get_content","text":"<pre><code>get_content()\n</code></pre> <p>Return dict of node content.</p> Source code in <code>src/fed_rag/data_structures/knowledge_node.py</code> <pre><code>def get_content(self) -&gt; NodeContent:\n    \"\"\"Return dict of node content.\"\"\"\n    content: NodeContent = {\n        \"image_content\": self.image_content,\n        \"text_content\": self.text_content,\n    }\n    return content\n</code></pre>"},{"location":"api_reference/data_structures/knowledge_node/#src.fed_rag.data_structures.knowledge_node.KnowledgeNode.serialize_metadata","title":"serialize_metadata","text":"<pre><code>serialize_metadata(metadata)\n</code></pre> <p>Custom serializer for the metadata field.</p> <p>Will serialize the metadata field into a json string.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[Any, Any] | None</code> <p>Metadata dictionary to serialize.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Serialized metadata as a json string.</p> Source code in <code>src/fed_rag/data_structures/knowledge_node.py</code> <pre><code>@field_serializer(\"metadata\")\ndef serialize_metadata(\n    self, metadata: dict[Any, Any] | None\n) -&gt; str | None:\n    \"\"\"\n    Custom serializer for the metadata field.\n\n    Will serialize the metadata field into a json string.\n\n    Args:\n        metadata: Metadata dictionary to serialize.\n\n    Returns:\n        Serialized metadata as a json string.\n    \"\"\"\n    if metadata:\n        return json.dumps(metadata)\n    return None\n</code></pre>"},{"location":"api_reference/data_structures/knowledge_node/#src.fed_rag.data_structures.knowledge_node.KnowledgeNode.deserialize_metadata","title":"deserialize_metadata  <code>classmethod</code>","text":"<pre><code>deserialize_metadata(metadata)\n</code></pre> <p>Custom validator for the metadata field.</p> <p>Will deserialize the metadata from a json string if it's a string.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[Any, Any] | str | None</code> <p>Metadata to validate. If it is a json string, it will be deserialized into a dictionary.</p> required <p>Returns:</p> Type Description <code>dict[Any, Any] | None</code> <p>Validated metadata.</p> Source code in <code>src/fed_rag/data_structures/knowledge_node.py</code> <pre><code>@field_validator(\"metadata\", mode=\"before\")\n@classmethod\ndef deserialize_metadata(\n    cls, metadata: dict[Any, Any] | str | None\n) -&gt; dict[Any, Any] | None:\n    \"\"\"\n    Custom validator for the metadata field.\n\n    Will deserialize the metadata from a json string if it's a string.\n\n    Args:\n        metadata: Metadata to validate. If it is a json string, it will be deserialized into a dictionary.\n\n    Returns:\n        Validated metadata.\n    \"\"\"\n    if isinstance(metadata, str):\n        deserialized_metadata = json.loads(metadata)\n        return cast(dict[Any, Any], deserialized_metadata)\n    if metadata is None:\n        return {}\n    return metadata\n</code></pre>"},{"location":"api_reference/data_structures/knowledge_node/#src.fed_rag.data_structures.knowledge_node.KnowledgeNode.model_dump_without_embeddings","title":"model_dump_without_embeddings","text":"<pre><code>model_dump_without_embeddings()\n</code></pre> <p>Seriliaze the node without the embedding.</p> Source code in <code>src/fed_rag/data_structures/knowledge_node.py</code> <pre><code>def model_dump_without_embeddings(self) -&gt; dict[str, Any]:\n    \"\"\"Seriliaze the node without the embedding.\"\"\"\n    return self.model_dump(exclude={\"embedding\"})\n</code></pre>"},{"location":"api_reference/data_structures/rag/","title":"RAG","text":"<p>Auxiliary types for RAG System</p>"},{"location":"api_reference/data_structures/rag/#src.fed_rag.data_structures.rag.SourceNode","title":"SourceNode","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/data_structures/rag.py</code> <pre><code>class SourceNode(BaseModel):\n    score: float\n    node: KnowledgeNode\n\n    def __getattr__(self, __name: str) -&gt; Any:\n        \"\"\"Convenient wrapper on getattr of associated node.\"\"\"\n        return getattr(self.node, __name)\n</code></pre>"},{"location":"api_reference/data_structures/rag/#src.fed_rag.data_structures.rag.SourceNode.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(__name)\n</code></pre> <p>Convenient wrapper on getattr of associated node.</p> Source code in <code>src/fed_rag/data_structures/rag.py</code> <pre><code>def __getattr__(self, __name: str) -&gt; Any:\n    \"\"\"Convenient wrapper on getattr of associated node.\"\"\"\n    return getattr(self.node, __name)\n</code></pre>"},{"location":"api_reference/data_structures/results/","title":"Results","text":"<p>Data structures for results</p>"},{"location":"api_reference/data_structures/results/#src.fed_rag.data_structures.results.TrainResult","title":"TrainResult","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/data_structures/results.py</code> <pre><code>class TrainResult(BaseModel):\n    loss: float\n</code></pre>"},{"location":"api_reference/data_structures/results/#src.fed_rag.data_structures.results.TestResult","title":"TestResult","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/data_structures/results.py</code> <pre><code>class TestResult(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    loss: float\n    metrics: dict[str, Any] = Field(\n        description=\"Additional metrics computed on test set.\",\n        default_factory=dict,\n    )\n</code></pre>"},{"location":"api_reference/decorators/","title":"Trainer and Tester Decorators","text":"<p>Trainer Decorators</p> <p>Tester Decorators</p>"},{"location":"api_reference/decorators/#src.fed_rag.decorators.trainer.TrainerDecorators","title":"TrainerDecorators","text":"Source code in <code>src/fed_rag/decorators/trainer.py</code> <pre><code>class TrainerDecorators:\n    def pytorch(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.pytorch import inspect_trainer_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_trainer_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_trainer_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n\n    def huggingface(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.huggingface import inspect_trainer_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_trainer_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_trainer_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n</code></pre>"},{"location":"api_reference/decorators/#src.fed_rag.decorators.tester.TesterDecorators","title":"TesterDecorators","text":"Source code in <code>src/fed_rag/decorators/tester.py</code> <pre><code>class TesterDecorators:\n    def pytorch(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.pytorch import inspect_tester_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_tester_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_tester_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n\n    def huggingface(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.huggingface import inspect_tester_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_tester_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_tester_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n</code></pre>"},{"location":"api_reference/evals/","title":"Evals","text":"<p>Base Benchmark and Benchmarker</p> <p>Base EvaluationMetric</p>"},{"location":"api_reference/evals/#src.fed_rag.base.evals.benchmark.BaseBenchmark","title":"BaseBenchmark","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Benchmark.</p> Source code in <code>src/fed_rag/base/evals/benchmark.py</code> <pre><code>class BaseBenchmark(BaseModel, ABC):\n    \"\"\"Base Benchmark.\"\"\"\n\n    _examples: Sequence[BenchmarkExample] = PrivateAttr()\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    # give it a sequence interface for accessing examples more easily\n    def __getitem__(self, index: int) -&gt; BenchmarkExample:\n        return self._examples.__getitem__(index)\n\n    def __len__(self) -&gt; int:\n        return self._examples.__len__()\n\n    # shouldn't override Pydantic BaseModels' __iter__\n    def as_iterator(self) -&gt; Iterator[BenchmarkExample]:\n        return self._examples.__iter__()\n\n    @model_validator(mode=\"after\")\n    def set_examples(self) -&gt; \"BaseBenchmark\":\n        try:\n            self._examples = self._get_examples()\n        except BenchmarkParseError as e:\n            raise BenchmarkGetExamplesError(\n                f\"Failed to parse examples: {str(e)}\"\n            ) from e\n        except Exception as e:\n            raise (\n                BenchmarkGetExamplesError(f\"Failed to get examples: {str(e)}\")\n            ) from e\n        return self\n\n    # abstractmethods\n    @abstractmethod\n    def _get_examples(self, **kwargs: Any) -&gt; Sequence[BenchmarkExample]:\n        \"\"\"Method to get examples.\"\"\"\n\n    @abstractmethod\n    def as_stream(self) -&gt; Generator[BenchmarkExample, None, None]:\n        \"\"\"Produce a stream of `BenchmarkExamples`.\"\"\"\n\n    @property\n    @abstractmethod\n    def num_examples(self) -&gt; int:\n        \"\"\"Number of examples in the benchmark.\n\n        NOTE: if streaming, `_examples` is likely set to an empty list. Thus,\n        we leave this implementation for the subclasses.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/evals/#src.fed_rag.base.evals.benchmark.BaseBenchmark.num_examples","title":"num_examples  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>num_examples\n</code></pre> <p>Number of examples in the benchmark.</p> <p>NOTE: if streaming, <code>_examples</code> is likely set to an empty list. Thus, we leave this implementation for the subclasses.</p>"},{"location":"api_reference/evals/#src.fed_rag.base.evals.benchmark.BaseBenchmark.as_stream","title":"as_stream  <code>abstractmethod</code>","text":"<pre><code>as_stream()\n</code></pre> <p>Produce a stream of <code>BenchmarkExamples</code>.</p> Source code in <code>src/fed_rag/base/evals/benchmark.py</code> <pre><code>@abstractmethod\ndef as_stream(self) -&gt; Generator[BenchmarkExample, None, None]:\n    \"\"\"Produce a stream of `BenchmarkExamples`.\"\"\"\n</code></pre>"},{"location":"api_reference/evals/#src.fed_rag.base.evals.metric.BaseEvaluationMetric","title":"BaseEvaluationMetric","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Data Collator.</p> Source code in <code>src/fed_rag/base/evals/metric.py</code> <pre><code>class BaseEvaluationMetric(BaseModel, ABC):\n    \"\"\"Base Data Collator.\"\"\"\n\n    @abstractmethod\n    def __call__(\n        self, prediction: str, actual: str, *args: Any, **kwargs: Any\n    ) -&gt; float:\n        \"\"\"Evaluate an example prediction against the actual response.\"\"\"\n</code></pre>"},{"location":"api_reference/evals/#src.fed_rag.base.evals.metric.BaseEvaluationMetric.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(prediction, actual, *args, **kwargs)\n</code></pre> <p>Evaluate an example prediction against the actual response.</p> Source code in <code>src/fed_rag/base/evals/metric.py</code> <pre><code>@abstractmethod\ndef __call__(\n    self, prediction: str, actual: str, *args: Any, **kwargs: Any\n) -&gt; float:\n    \"\"\"Evaluate an example prediction against the actual response.\"\"\"\n</code></pre>"},{"location":"api_reference/evals/benchmarker/","title":"Benchmarker","text":"<p>Base Benchmark and Benchmarker</p>"},{"location":"api_reference/evals/benchmarker/#src.fed_rag.evals.benchmarker.Benchmarker","title":"Benchmarker","text":"<p>               Bases: <code>BaseModel</code></p> <p>Benchmarker</p> Source code in <code>src/fed_rag/evals/benchmarker.py</code> <pre><code>class Benchmarker(BaseModel):\n    \"\"\"Benchmarker\"\"\"\n\n    rag_system: RAGSystem\n\n    def _update_running_score(\n        self,\n        agg: AggregationMode,\n        running_score: float | None,\n        next_score: float,\n        num_examples_seen: int,\n    ) -&gt; float:\n        \"\"\"Update the running score.\n\n        Args:\n            agg (AggregationMode): aggregation mode.\n            running_score (float): the running score to be updated.\n            next_score (float): the score of the latest scored example.\n            num_examples_seen (int): the number of examples seen prior to the\n                latest scored example.\n\n        Returns:\n            float: the updated running score\n        \"\"\"\n        if not running_score:\n            return next_score\n\n        match agg:\n            case AggregationMode.AVG:\n                return (num_examples_seen * running_score + next_score) / (\n                    num_examples_seen + 1\n                )\n            case AggregationMode.SUM:\n                return running_score + next_score\n            case AggregationMode.MAX:\n                if running_score &lt; next_score:\n                    return next_score\n                else:\n                    return running_score\n            case AggregationMode.MIN:\n                if running_score &gt; next_score:\n                    return next_score\n                else:\n                    return running_score\n            case _:  # pragma: no cover\n                assert_never(agg)\n\n    @contextlib.contextmanager\n    def _get_examples_iterator(\n        self, benchmark: BaseBenchmark, is_streaming: bool\n    ) -&gt; Generator[BenchmarkExample, None, None]:\n        \"\"\"Wrapper over the iterator or stream.\n\n        To handle generator clean up safely.\n        \"\"\"\n        if is_streaming:\n            examples_iterator = benchmark.as_stream()\n        else:\n            examples_iterator = benchmark.as_iterator()\n\n        try:\n            yield examples_iterator\n        finally:\n            if hasattr(examples_iterator, \"close\"):\n                examples_iterator.close()\n\n    def run(\n        self,\n        benchmark: BaseBenchmark,\n        metric: BaseEvaluationMetric,\n        is_streaming: bool = False,\n        agg: AggregationMode | str = \"avg\",\n        batch_size: int = 1,\n        num_examples: int | None = None,\n        num_workers: int = 1,\n        output_dir: Path | str = DEFAULT_OUTPUT_DIR,\n        save_evaluations: bool = False,\n        **kwargs: Any,\n    ) -&gt; BenchmarkResult:\n        \"\"\"Execute the benchmark using the associated `RAGSystem`.\n\n        Args:\n            agg (AggregationMode | str): the aggregation mode to apply to all example scores.\n                Modes include `avg`, `sum`, `max`, or `min`.\n            benchmark (BaseBenchmark): the benchmark to run the `RAGSystem` against.\n            batch_size (int, optional): number of examples to process in a single batch.\n            metric (BaseEvaluationMetric): the metric to use for evaluation.\n            num_examples (int | None, optional): Number of examples to use from\n                the benchmark. If None, then the entire collection of examples of\n                the benchmark are ran. Defaults to None.\n            num_workers (int, optional): concurrent execution via threads.\n            output_dir (Path | None): the output directory for saving evaluations. Defaults to None.\n\n        Returns:\n            BenchmarkResult: the benchmark result\n\n        TODO: implement concurrent as well as batch execution. Need RAGSystem\n        to be able to handle batches as well.\n        \"\"\"\n        if isinstance(output_dir, str):\n            output_dir = Path(output_dir)\n\n        # create file for saving evaluations\n        if save_evaluations:\n            output_dir.mkdir(parents=True, exist_ok=True)\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            filename = (\n                output_dir\n                / f\"{benchmark.__class__.__name__}-{timestamp}.jsonl\"\n            )\n            f = open(filename, \"w\")\n        else:\n            filename = None\n            f = None\n\n        try:\n            with self._get_examples_iterator(\n                benchmark, is_streaming\n            ) as examples_iterator:\n                running_score = None\n                num_seen = 0\n                for example in examples_iterator:\n                    if num_seen == num_examples:\n                        break\n\n                    # prediction\n                    result = self.rag_system.query(example.query)\n\n                    # evaluation\n                    score = metric(\n                        prediction=result.response, actual=example.response\n                    )\n\n                    # evaluated benchmark example\n                    evaluated_example = BenchmarkEvaluatedExample(\n                        score=score,\n                        rag_response=result,\n                        example=example,\n                    )\n                    if f is not None:\n                        f.write(\n                            evaluated_example.model_dump_json_without_embeddings()\n                            + \"\\n\"\n                        )\n                        f.flush()\n\n                    # update running score\n                    running_score = self._update_running_score(\n                        agg=agg,\n                        running_score=running_score,\n                        next_score=score,\n                        num_examples_seen=num_seen,\n                    )\n\n                    num_seen += 1\n        finally:\n            if f:\n                f.close()\n\n        return BenchmarkResult(\n            score=running_score,\n            metric_name=metric.__class__.__name__,\n            num_examples_used=num_seen,\n            num_total_examples=benchmark.num_examples,\n            evaluations_file=filename.as_posix() if filename else None,\n        )\n</code></pre>"},{"location":"api_reference/evals/benchmarker/#src.fed_rag.evals.benchmarker.Benchmarker.run","title":"run","text":"<pre><code>run(\n    benchmark,\n    metric,\n    is_streaming=False,\n    agg=\"avg\",\n    batch_size=1,\n    num_examples=None,\n    num_workers=1,\n    output_dir=DEFAULT_OUTPUT_DIR,\n    save_evaluations=False,\n    **kwargs\n)\n</code></pre> <p>Execute the benchmark using the associated <code>RAGSystem</code>.</p> <p>Parameters:</p> Name Type Description Default <code>agg</code> <code>AggregationMode | str</code> <p>the aggregation mode to apply to all example scores. Modes include <code>avg</code>, <code>sum</code>, <code>max</code>, or <code>min</code>.</p> <code>'avg'</code> <code>benchmark</code> <code>BaseBenchmark</code> <p>the benchmark to run the <code>RAGSystem</code> against.</p> required <code>batch_size</code> <code>int</code> <p>number of examples to process in a single batch.</p> <code>1</code> <code>metric</code> <code>BaseEvaluationMetric</code> <p>the metric to use for evaluation.</p> required <code>num_examples</code> <code>int | None</code> <p>Number of examples to use from the benchmark. If None, then the entire collection of examples of the benchmark are ran. Defaults to None.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>concurrent execution via threads.</p> <code>1</code> <code>output_dir</code> <code>Path | None</code> <p>the output directory for saving evaluations. Defaults to None.</p> <code>DEFAULT_OUTPUT_DIR</code> <p>Returns:</p> Name Type Description <code>BenchmarkResult</code> <code>BenchmarkResult</code> <p>the benchmark result</p> <p>TODO: implement concurrent as well as batch execution. Need RAGSystem to be able to handle batches as well.</p> Source code in <code>src/fed_rag/evals/benchmarker.py</code> <pre><code>def run(\n    self,\n    benchmark: BaseBenchmark,\n    metric: BaseEvaluationMetric,\n    is_streaming: bool = False,\n    agg: AggregationMode | str = \"avg\",\n    batch_size: int = 1,\n    num_examples: int | None = None,\n    num_workers: int = 1,\n    output_dir: Path | str = DEFAULT_OUTPUT_DIR,\n    save_evaluations: bool = False,\n    **kwargs: Any,\n) -&gt; BenchmarkResult:\n    \"\"\"Execute the benchmark using the associated `RAGSystem`.\n\n    Args:\n        agg (AggregationMode | str): the aggregation mode to apply to all example scores.\n            Modes include `avg`, `sum`, `max`, or `min`.\n        benchmark (BaseBenchmark): the benchmark to run the `RAGSystem` against.\n        batch_size (int, optional): number of examples to process in a single batch.\n        metric (BaseEvaluationMetric): the metric to use for evaluation.\n        num_examples (int | None, optional): Number of examples to use from\n            the benchmark. If None, then the entire collection of examples of\n            the benchmark are ran. Defaults to None.\n        num_workers (int, optional): concurrent execution via threads.\n        output_dir (Path | None): the output directory for saving evaluations. Defaults to None.\n\n    Returns:\n        BenchmarkResult: the benchmark result\n\n    TODO: implement concurrent as well as batch execution. Need RAGSystem\n    to be able to handle batches as well.\n    \"\"\"\n    if isinstance(output_dir, str):\n        output_dir = Path(output_dir)\n\n    # create file for saving evaluations\n    if save_evaluations:\n        output_dir.mkdir(parents=True, exist_ok=True)\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = (\n            output_dir\n            / f\"{benchmark.__class__.__name__}-{timestamp}.jsonl\"\n        )\n        f = open(filename, \"w\")\n    else:\n        filename = None\n        f = None\n\n    try:\n        with self._get_examples_iterator(\n            benchmark, is_streaming\n        ) as examples_iterator:\n            running_score = None\n            num_seen = 0\n            for example in examples_iterator:\n                if num_seen == num_examples:\n                    break\n\n                # prediction\n                result = self.rag_system.query(example.query)\n\n                # evaluation\n                score = metric(\n                    prediction=result.response, actual=example.response\n                )\n\n                # evaluated benchmark example\n                evaluated_example = BenchmarkEvaluatedExample(\n                    score=score,\n                    rag_response=result,\n                    example=example,\n                )\n                if f is not None:\n                    f.write(\n                        evaluated_example.model_dump_json_without_embeddings()\n                        + \"\\n\"\n                    )\n                    f.flush()\n\n                # update running score\n                running_score = self._update_running_score(\n                    agg=agg,\n                    running_score=running_score,\n                    next_score=score,\n                    num_examples_seen=num_seen,\n                )\n\n                num_seen += 1\n    finally:\n        if f:\n            f.close()\n\n    return BenchmarkResult(\n        score=running_score,\n        metric_name=metric.__class__.__name__,\n        num_examples_used=num_seen,\n        num_total_examples=benchmark.num_examples,\n        evaluations_file=filename.as_posix() if filename else None,\n    )\n</code></pre>"},{"location":"api_reference/evals/benchmarks/huggingface/mmlu/","title":"MMLU","text":"<p>MMLU benchmark</p>"},{"location":"api_reference/evals/benchmarks/huggingface/mmlu/#src.fed_rag.evals.benchmarks.huggingface.mmlu.HuggingFaceMMLU","title":"HuggingFaceMMLU","text":"<p>               Bases: <code>HuggingFaceBenchmarkMixin</code>, <code>BaseBenchmark</code></p> <p>HuggingFace MMLU Benchmark.</p> Example schema <p>{     \"question\": \"What is the embryological origin of the hyoid bone?\",     \"choices\": [         \"The first pharyngeal arch\",         \"The first and second pharyngeal arches\",         \"The second pharyngeal arch\",         \"The second and third pharyngeal arches\",     ],     \"answer\": \"D\", }</p> Source code in <code>src/fed_rag/evals/benchmarks/huggingface/mmlu.py</code> <pre><code>class HuggingFaceMMLU(HuggingFaceBenchmarkMixin, BaseBenchmark):\n    \"\"\"HuggingFace MMLU Benchmark.\n\n    Example schema:\n        {\n            \"question\": \"What is the embryological origin of the hyoid bone?\",\n            \"choices\": [\n                \"The first pharyngeal arch\",\n                \"The first and second pharyngeal arches\",\n                \"The second pharyngeal arch\",\n                \"The second and third pharyngeal arches\",\n            ],\n            \"answer\": \"D\",\n        }\n    \"\"\"\n\n    dataset_name = \"cais/mmlu\"\n    configuration_name: str = \"all\"\n    response_key: ClassVar[dict[int, str]] = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n\n    def _get_query_from_example(self, example: dict[str, Any]) -&gt; str:\n        choices = example[\"choices\"]\n        formatted_choices = f\"A: {choices[0]}\\nB: {choices[1]}\\nC: {choices[2]}\\nD: {choices[3]}\"\n        return f\"{example['question']}\\n\\n{formatted_choices}\"\n\n    def _get_response_from_example(self, example: dict[str, Any]) -&gt; str:\n        return self.response_key[example[\"answer\"]]\n\n    def _get_context_from_example(self, example: dict[str, Any]) -&gt; str | None:\n        return None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def _validate_extra_installed(cls, data: Any) -&gt; Any:\n        \"\"\"Validate that huggingface-evals dependencies are installed.\"\"\"\n        check_huggingface_evals_installed(cls.__name__)\n        return data\n</code></pre>"},{"location":"api_reference/evals/benchmarks/huggingface/pubmedqa/","title":"PubMedQA","text":"<p>PubMedQA benchmark</p>"},{"location":"api_reference/evals/benchmarks/huggingface/pubmedqa/#src.fed_rag.evals.benchmarks.huggingface.pubmedqa.HuggingFacePubMedQA","title":"HuggingFacePubMedQA","text":"<p>               Bases: <code>HuggingFaceBenchmarkMixin</code>, <code>BaseBenchmark</code></p> <p>HuggingFace PubMedQA Benchmark.</p> <p>PubMedQA is a biomedical question answering dataset where each question can be answered with \"yes\", \"no\", or \"maybe\" based on the given context.</p> Example schema <p>{     \"pubid\": \"25429730\",     \"question\": \"Are group 2 innate lymphoid cells (ILC2s) increased in chronic rhinosinusitis with nasal polyps or eosinophilia?\",     \"context\": {         \"contexts\": [             \"Chronic rhinosinusitis is a heterogeneous disease with uncertain pathogenesis.\",             \"The study aimed to identify ILC2s in sinus mucosa in patients with CRS.\",             \"35 patients including 13 with eosinophilic CRS were recruited.\",             \"ILC2 frequencies were associated with the presence of nasal polyps and increased blood eosinophilia.\"         ],         \"labels\": [\"label1\", \"label2\", \"label3\", \"label4\"],         \"meshes\": [\"Chronic Disease\", \"Nasal Polyps\", \"Immunity, Innate\"]     },     \"long_answer\": \"Based on our analysis, increased ILC2s are associated with CRS with nasal polyps.\",     \"final_decision\": \"yes\"  # or \"no\" or \"maybe\" }</p> Source code in <code>src/fed_rag/evals/benchmarks/huggingface/pubmedqa.py</code> <pre><code>class HuggingFacePubMedQA(HuggingFaceBenchmarkMixin, BaseBenchmark):\n    \"\"\"HuggingFace PubMedQA Benchmark.\n\n    PubMedQA is a biomedical question answering dataset where each question\n    can be answered with \"yes\", \"no\", or \"maybe\" based on the given context.\n\n    Example schema:\n        {\n            \"pubid\": \"25429730\",\n            \"question\": \"Are group 2 innate lymphoid cells (ILC2s) increased in chronic rhinosinusitis with nasal polyps or eosinophilia?\",\n            \"context\": {\n                \"contexts\": [\n                    \"Chronic rhinosinusitis is a heterogeneous disease with uncertain pathogenesis.\",\n                    \"The study aimed to identify ILC2s in sinus mucosa in patients with CRS.\",\n                    \"35 patients including 13 with eosinophilic CRS were recruited.\",\n                    \"ILC2 frequencies were associated with the presence of nasal polyps and increased blood eosinophilia.\"\n                ],\n                \"labels\": [\"label1\", \"label2\", \"label3\", \"label4\"],\n                \"meshes\": [\"Chronic Disease\", \"Nasal Polyps\", \"Immunity, Innate\"]\n            },\n            \"long_answer\": \"Based on our analysis, increased ILC2s are associated with CRS with nasal polyps.\",\n            \"final_decision\": \"yes\"  # or \"no\" or \"maybe\"\n        }\"\"\"\n\n    dataset_name = \"qiaojin/PubMedQA\"\n    configuration_name: str = \"pqa_labeled\"\n\n    def _get_query_from_example(self, example: dict[str, Any]) -&gt; str:\n        return str(example[\"question\"])\n\n    def _get_response_from_example(self, example: dict[str, Any]) -&gt; str:\n        return str(example[\"final_decision\"])\n\n    def _get_context_from_example(self, example: dict[str, Any]) -&gt; str:\n        context = example.get(\"context\", {})\n        if isinstance(context, dict):\n            contexts_list = context.get(\"contexts\")\n            if isinstance(contexts_list, list):\n                return \" \".join(contexts_list)\n            # Fallback: join all values if \"contexts\" is missing\n            return \" \".join(\n                \" \".join(v) if isinstance(v, list) else str(v)\n                for v in context.values()\n            )\n        elif isinstance(context, str):\n            return context\n        else:\n            raise BenchmarkParseError(\n                f\"Unexpected context type: {type(context)} in example: {example}\"\n            )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def _validate_extra_installed(cls, data: Any) -&gt; Any:\n        \"\"\"Validate that huggingface-evals dependencies are installed.\"\"\"\n        check_huggingface_evals_installed(cls.__name__)\n        return data\n</code></pre>"},{"location":"api_reference/evals/metrics/exact_match/","title":"Exact Match","text":"<p>Exact Match Metric</p>"},{"location":"api_reference/evals/metrics/exact_match/#src.fed_rag.evals.metrics.exact_match.ExactMatchEvaluationMetric","title":"ExactMatchEvaluationMetric","text":"<p>               Bases: <code>BaseEvaluationMetric</code></p> <p>Exact match evaluation metric class.</p> Source code in <code>src/fed_rag/evals/metrics/exact_match.py</code> <pre><code>class ExactMatchEvaluationMetric(BaseEvaluationMetric):\n    \"\"\"Exact match evaluation metric class.\"\"\"\n\n    def __call__(\n        self, prediction: str, actual: str, *args: Any, **kwargs: Any\n    ) -&gt; float:\n        return float(prediction.lower() == actual.lower())\n</code></pre>"},{"location":"api_reference/exceptions/","title":"FedRAG Exceptions","text":"<p>Base Error Class for FedRAG.</p>"},{"location":"api_reference/exceptions/#src.fed_rag.exceptions.core.FedRAGError","title":"FedRAGError","text":"<p>               Bases: <code>Exception</code></p> <p>Base error for all fed-rag exceptions.</p> Source code in <code>src/fed_rag/exceptions/core.py</code> <pre><code>class FedRAGError(Exception):\n    \"\"\"Base error for all fed-rag exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/#src.fed_rag.exceptions.core.FedRAGWarning","title":"FedRAGWarning","text":"<p>               Bases: <code>Warning</code></p> <p>Base warning for all fed-rag warnings.</p> Source code in <code>src/fed_rag/exceptions/core.py</code> <pre><code>class FedRAGWarning(Warning):\n    \"\"\"Base warning for all fed-rag warnings.\"\"\"\n</code></pre>"},{"location":"api_reference/exceptions/bridge/","title":"Bridge","text":"<p>Exceptions for Bridges.</p>"},{"location":"api_reference/exceptions/bridge/#src.fed_rag.exceptions.bridge.BridgeError","title":"BridgeError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base bridge error for all bridge-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/bridge.py</code> <pre><code>class BridgeError(FedRAGError):\n    \"\"\"Base bridge error for all bridge-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/evals/","title":"Evals","text":"<p>Exceptions for Evals.</p>"},{"location":"api_reference/exceptions/evals/#src.fed_rag.exceptions.evals.EvalsError","title":"EvalsError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base evals error for all evals-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/evals.py</code> <pre><code>class EvalsError(FedRAGError):\n    \"\"\"Base evals error for all evals-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/evals/#src.fed_rag.exceptions.evals.EvalsWarning","title":"EvalsWarning","text":"<p>               Bases: <code>FedRAGWarning</code></p> <p>Base inspector warning for all evals-related warnings.</p> Source code in <code>src/fed_rag/exceptions/evals.py</code> <pre><code>class EvalsWarning(FedRAGWarning):\n    \"\"\"Base inspector warning for all evals-related warnings.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/evals/#src.fed_rag.exceptions.evals.EvaluationsFileNotFoundError","title":"EvaluationsFileNotFoundError","text":"<p>               Bases: <code>EvalsError</code>, <code>FileNotFoundError</code></p> <p>Benchmark evaluations file not found error.</p> Source code in <code>src/fed_rag/exceptions/evals.py</code> <pre><code>class EvaluationsFileNotFoundError(EvalsError, FileNotFoundError):\n    \"\"\"Benchmark evaluations file not found error.\"\"\"\n</code></pre>"},{"location":"api_reference/exceptions/fl_tasks/","title":"FL Tasks","text":"<p>Exceptions for FL Tasks.</p>"},{"location":"api_reference/exceptions/fl_tasks/#src.fed_rag.exceptions.fl_tasks.FLTaskError","title":"FLTaskError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base fl task error for all fl-task-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/fl_tasks.py</code> <pre><code>class FLTaskError(FedRAGError):\n    \"\"\"Base fl task error for all fl-task-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/fl_tasks/#src.fed_rag.exceptions.fl_tasks.MissingRequiredNetParam","title":"MissingRequiredNetParam","text":"<p>               Bases: <code>FLTaskError</code></p> <p>Raised when invoking fl_task.server without passing the specified model/net param.</p> Source code in <code>src/fed_rag/exceptions/fl_tasks.py</code> <pre><code>class MissingRequiredNetParam(FLTaskError):\n    \"\"\"Raised when invoking fl_task.server without passing the specified model/net param.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/fl_tasks/#src.fed_rag.exceptions.fl_tasks.NetTypeMismatch","title":"NetTypeMismatch","text":"<p>               Bases: <code>FLTaskError</code></p> <p>Raised when a <code>trainer</code> and <code>tester</code> spec have differing <code>net_parameter_class_name</code>.</p> <p>This indicates that the these methods have different types for the <code>net_parameter</code>.</p> Source code in <code>src/fed_rag/exceptions/fl_tasks.py</code> <pre><code>class NetTypeMismatch(FLTaskError):\n    \"\"\"Raised when a `trainer` and `tester` spec have differing `net_parameter_class_name`.\n\n    This indicates that the these methods have different types for the `net_parameter`.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/inspectors/","title":"Inspectors","text":"<p>Exceptions for inspectors.</p>"},{"location":"api_reference/exceptions/inspectors/#src.fed_rag.exceptions.inspectors.InspectorError","title":"InspectorError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base inspector error for all inspector-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/inspectors.py</code> <pre><code>class InspectorError(FedRAGError):\n    \"\"\"Base inspector error for all inspector-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/inspectors/#src.fed_rag.exceptions.inspectors.InspectorWarning","title":"InspectorWarning","text":"<p>               Bases: <code>FedRAGWarning</code></p> <p>Base inspector warning for all inspector-related warnings.</p> Source code in <code>src/fed_rag/exceptions/inspectors.py</code> <pre><code>class InspectorWarning(FedRAGWarning):\n    \"\"\"Base inspector warning for all inspector-related warnings.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/knowledge_stores/","title":"Knowledge Stores","text":"<p>Exceptions for Knowledge Stores.</p>"},{"location":"api_reference/exceptions/knowledge_stores/#src.fed_rag.exceptions.knowledge_stores.KnowledgeStoreError","title":"KnowledgeStoreError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base knowledge store error for all knowledge-store-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/knowledge_stores.py</code> <pre><code>class KnowledgeStoreError(FedRAGError):\n    \"\"\"Base knowledge store error for all knowledge-store-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/knowledge_stores/#src.fed_rag.exceptions.knowledge_stores.KnowledgeStoreWarning","title":"KnowledgeStoreWarning","text":"<p>               Bases: <code>FedRAGWarning</code></p> <p>Base knowledge store error for all knowledge-store-related warnings.</p> Source code in <code>src/fed_rag/exceptions/knowledge_stores.py</code> <pre><code>class KnowledgeStoreWarning(FedRAGWarning):\n    \"\"\"Base knowledge store error for all knowledge-store-related warnings.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/loss/","title":"Loss","text":"<p>Exceptions for loss.</p>"},{"location":"api_reference/exceptions/loss/#src.fed_rag.exceptions.loss.LossError","title":"LossError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base loss errors for all loss-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/loss.py</code> <pre><code>class LossError(FedRAGError):\n    \"\"\"Base loss errors for all loss-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/rag_trainer/","title":"RAG Trainer","text":""},{"location":"api_reference/exceptions/rag_trainer/#src.fed_rag.exceptions.trainer_manager.RAGTrainerManagerError","title":"RAGTrainerManagerError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base errors for all rag trainer manager relevant exceptions.</p> Source code in <code>src/fed_rag/exceptions/trainer_manager.py</code> <pre><code>class RAGTrainerManagerError(FedRAGError):\n    \"\"\"Base errors for all rag trainer manager relevant exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/tokenizer/","title":"Tokenizer","text":"<p>Exceptions for Tokenizer.</p>"},{"location":"api_reference/exceptions/tokenizer/#src.fed_rag.exceptions.tokenizer.TokenizerError","title":"TokenizerError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base evals error for all tokenizer-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/tokenizer.py</code> <pre><code>class TokenizerError(FedRAGError):\n    \"\"\"Base evals error for all tokenizer-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/tokenizer/#src.fed_rag.exceptions.tokenizer.TokenizerWarning","title":"TokenizerWarning","text":"<p>               Bases: <code>FedRAGWarning</code></p> <p>Base inspector warning for all tokenizer-related warnings.</p> Source code in <code>src/fed_rag/exceptions/tokenizer.py</code> <pre><code>class TokenizerWarning(FedRAGWarning):\n    \"\"\"Base inspector warning for all tokenizer-related warnings.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/finetuning_datasets/","title":"Finetuning Datasets","text":"<p>Data utils</p>"},{"location":"api_reference/finetuning_datasets/#src.fed_rag.utils.data._functions.ReturnType","title":"ReturnType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/fed_rag/utils/data/_functions.py</code> <pre><code>class ReturnType(str, Enum):\n    PYTORCH = \"pt\"\n    HUGGINGFACE = \"hf\"\n    TEXT = \"txt\"\n</code></pre>"},{"location":"api_reference/finetuning_datasets/#src.fed_rag.utils.data._functions.build_finetune_dataset","title":"build_finetune_dataset","text":"<pre><code>build_finetune_dataset(\n    rag_system,\n    examples,\n    eos_token_id,\n    finetune_example_template=DEFAULT_FINETUNE_EXAMPLE_TEMPLATE,\n    query_key=\"query\",\n    answer_key=\"answer\",\n    return_dataset=ReturnType.PYTORCH,\n)\n</code></pre> <p>Generates the finetuning dataset using the supplied rag_system and examples.</p> Source code in <code>src/fed_rag/utils/data/_functions.py</code> <pre><code>def build_finetune_dataset(\n    rag_system: RAGSystem,\n    examples: Sequence[dict],\n    eos_token_id: int,\n    finetune_example_template: str = DEFAULT_FINETUNE_EXAMPLE_TEMPLATE,\n    query_key: str = \"query\",\n    answer_key: str = \"answer\",\n    return_dataset: ReturnType = ReturnType.PYTORCH,\n) -&gt; Any:\n    \"\"\"Generates the finetuning dataset using the supplied rag_system and examples.\"\"\"\n\n    if (\n        isinstance(return_dataset, str)\n        and return_dataset not in ReturnType._value2member_map_.keys()\n    ):\n        raise ValueError(\n            \"Invalid `return_type` specified.\"\n        )  # TODO: give a proper exception to this\n\n    inputs_list = []\n    targets_list = []\n    attention_mask_list = []\n    finetuning_instances = []\n    for example in examples:\n        # retrieve\n        source_nodes = rag_system.retrieve(query=example[query_key])\n        total_sum_scores = sum(s.score for s in source_nodes)\n\n        # parallel in-context retrieval-augmentation creates\n        # top_k separated finetuning instances\n        for source in source_nodes:\n            finetune_instance_text = finetune_example_template.format(\n                query=example[query_key],\n                answer=example[answer_key],\n                context=source.node.get_content()[\"text_content\"],\n            )\n            finetuning_instances.append(finetune_instance_text)\n            _weight = source.score / total_sum_scores\n\n            # tokenize to get input_ids and target_ids\n            tokenizer = rag_system.generator.tokenizer\n            encode_result = tokenizer.encode(finetune_instance_text)\n            input_ids = encode_result[\"input_ids\"]\n            attention_mask = encode_result[\"attention_mask\"]\n            target_ids = input_ids[1:] + [eos_token_id]\n\n            inputs_list.append(input_ids)\n            targets_list.append(target_ids)\n            attention_mask_list.append(attention_mask)\n\n    if return_dataset == ReturnType.TEXT:\n        return finetuning_instances\n    elif return_dataset == ReturnType.PYTORCH:\n        return PyTorchRAGFinetuningDataset(\n            input_ids=[torch.Tensor(el) for el in inputs_list],\n            target_ids=[torch.Tensor(el) for el in targets_list],\n        )\n    elif return_dataset == ReturnType.HUGGINGFACE:\n        # needs `fed-rag[huggingface]` extra to be installed\n        # this import will fail if not installed\n        from fed_rag.utils.data.finetuning_datasets.huggingface import (\n            HuggingFaceRAGFinetuningDataset,\n        )\n\n        return HuggingFaceRAGFinetuningDataset.from_inputs(\n            input_ids=inputs_list,\n            target_ids=targets_list,\n            attention_mask=attention_mask_list,\n        )\n    else:\n        assert_never(return_dataset)  # pragma: no cover\n</code></pre>"},{"location":"api_reference/finetuning_datasets/huggingface/","title":"Huggingface","text":"<p>HuggingFace RAG Finetuning Dataset</p>"},{"location":"api_reference/finetuning_datasets/huggingface/#src.fed_rag.utils.data.finetuning_datasets.huggingface.HuggingFaceRAGFinetuningDataset","title":"HuggingFaceRAGFinetuningDataset","text":"<p>               Bases: <code>Dataset</code></p> <p>Thin wrapper over ~datasets.Dataset.</p> Source code in <code>src/fed_rag/utils/data/finetuning_datasets/huggingface.py</code> <pre><code>class HuggingFaceRAGFinetuningDataset(Dataset):\n    \"\"\"Thin wrapper over ~datasets.Dataset.\"\"\"\n\n    @classmethod\n    def from_inputs(\n        cls,\n        input_ids: list[list[int]],\n        target_ids: list[list[int]],\n        attention_mask: list[list[int]],\n    ) -&gt; Self:\n        return cls.from_dict(  # type: ignore[no-any-return]\n            {\n                \"input_ids\": input_ids,\n                \"target_ids\": target_ids,\n                \"attention_mask\": attention_mask,\n            }\n        )\n</code></pre>"},{"location":"api_reference/finetuning_datasets/pytorch/","title":"Pytorch","text":"<p>PyTorch RAG Finetuning Dataset</p>"},{"location":"api_reference/finetuning_datasets/pytorch/#src.fed_rag.utils.data.finetuning_datasets.pytorch.PyTorchRAGFinetuningDataset","title":"PyTorchRAGFinetuningDataset","text":"<p>               Bases: <code>Dataset</code></p> <p>PyTorch RAG Fine-Tuning Dataset Class.</p> <p>Parameters:</p> Name Type Description Default <code>Dataset</code> <code>_type_</code> <p>description</p> required Source code in <code>src/fed_rag/utils/data/finetuning_datasets/pytorch.py</code> <pre><code>class PyTorchRAGFinetuningDataset(Dataset):\n    \"\"\"PyTorch RAG Fine-Tuning Dataset Class.\n\n    Args:\n        Dataset (_type_): _description_\n    \"\"\"\n\n    def __init__(\n        self, input_ids: list[torch.Tensor], target_ids: list[torch.Tensor]\n    ):\n        self.input_ids = input_ids\n        self.target_ids = target_ids\n\n    def __len__(self) -&gt; int:\n        return len(self.input_ids)\n\n    def __getitem__(self, idx: int) -&gt; Any:\n        return self.input_ids[idx], self.target_ids[idx]\n</code></pre>"},{"location":"api_reference/fl_tasks/","title":"Base FL Task Classes","text":"<p>Base FL Task</p>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask","title":"BaseFLTask","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>class BaseFLTask(BaseModel, ABC):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def training_loop(self) -&gt; Callable:\n        ...\n\n    @classmethod\n    @abstractmethod\n    def from_configs(\n        cls, trainer_cfg: BaseFLTaskConfig, tester_cfg: Any\n    ) -&gt; Self:\n        ...\n\n    @classmethod\n    @abstractmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        try:\n            trainer_cfg = getattr(trainer, \"__fl_task_trainer_config\")\n        except AttributeError:\n            msg = (\n                \"`__fl_task_trainer_config` has not been set on training loop. Make \"\n                \"sure to decorate your training loop with the appropriate \"\n                \"decorator.\"\n            )\n            raise MissingFLTaskConfig(msg)\n\n        try:\n            tester_cfg = getattr(tester, \"__fl_task_tester_config\")\n        except AttributeError:\n            msg = (\n                \"`__fl_task_tester_config` has not been set on tester callable. Make \"\n                \"sure to decorate your tester with the appropriate decorator.\"\n            )\n            raise MissingFLTaskConfig(msg)\n        return cls.from_configs(trainer_cfg, tester_cfg)\n\n    @abstractmethod\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        \"\"\"Simulate the FL task.\n\n        Either use flwr's simulation tools, or create our own here.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def server(self, **kwargs: Any) -&gt; Server:\n        \"\"\"Create a flwr.Server object.\"\"\"\n        ...\n\n    @abstractmethod\n    def client(self, **kwargs: Any) -&gt; Client:\n        \"\"\"Create a flwr.Client object.\"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.simulate","title":"simulate  <code>abstractmethod</code>","text":"<pre><code>simulate(num_clients, **kwargs)\n</code></pre> <p>Simulate the FL task.</p> <p>Either use flwr's simulation tools, or create our own here.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n    \"\"\"Simulate the FL task.\n\n    Either use flwr's simulation tools, or create our own here.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.server","title":"server  <code>abstractmethod</code>","text":"<pre><code>server(**kwargs)\n</code></pre> <p>Create a flwr.Server object.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef server(self, **kwargs: Any) -&gt; Server:\n    \"\"\"Create a flwr.Server object.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.client","title":"client  <code>abstractmethod</code>","text":"<pre><code>client(**kwargs)\n</code></pre> <p>Create a flwr.Client object.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef client(self, **kwargs: Any) -&gt; Client:\n    \"\"\"Create a flwr.Client object.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTaskConfig","title":"BaseFLTaskConfig","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>class BaseFLTaskConfig(BaseModel):\n    pass\n</code></pre>"},{"location":"api_reference/fl_tasks/huggingface/","title":"Huggingface","text":"<p>HuggingFace FL Task.</p> <p>NOTE: Using this module requires the <code>huggingface</code> extra to be installed.</p>"},{"location":"api_reference/fl_tasks/huggingface/#src.fed_rag.fl_tasks.huggingface.HuggingFaceFlowerClient","title":"HuggingFaceFlowerClient","text":"<p>               Bases: <code>NumPyClient</code></p> Source code in <code>src/fed_rag/fl_tasks/huggingface.py</code> <pre><code>class HuggingFaceFlowerClient(NumPyClient):\n    def __init__(\n        self,\n        task_bundle: BaseFLTaskBundle,\n    ) -&gt; None:\n        super().__init__()\n        self.task_bundle = task_bundle\n\n    def __getattr__(self, name: str) -&gt; Any:\n        if name in self.task_bundle.model_fields:\n            return getattr(self.task_bundle, name)\n        else:\n            return super().__getattr__(name)\n\n    def get_weights(self) -&gt; NDArrays:\n        return _get_weights(self.net)\n\n    def set_weights(self, parameters: NDArrays) -&gt; None:\n        if isinstance(self.net, PeftModel):\n            # get state dict\n            state_dict = get_peft_model_state_dict(self.net)\n            state_dict = cast(Dict[str, Any], state_dict)\n\n            # update state dict with supplied parameters\n            params_dict = zip(state_dict.keys(), parameters)\n            state_dict = OrderedDict(\n                {k: torch.tensor(v) for k, v in params_dict}\n            )\n            set_peft_model_state_dict(self.net, state_dict)\n        else:  # SentenceTransformer | PreTrainedModel\n            # get state dict\n            state_dict = self.net.state_dict()\n\n            # update state dict with supplied parameters\n            params_dict = zip(state_dict.keys(), parameters)\n            state_dict = OrderedDict(\n                {k: torch.tensor(v) for k, v in params_dict}\n            )\n            self.net.load_state_dict(state_dict, strict=True)\n\n    def fit(\n        self, parameters: NDArrays, config: dict[str, Scalar]\n    ) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n        self.set_weights(parameters)\n\n        result: TrainResult = self.trainer(\n            self.net,\n            self.train_dataset,\n            self.val_dataset,\n            **self.task_bundle.extra_train_kwargs,\n        )\n        return (\n            self.get_weights(),\n            len(self.train_dataset),\n            {\"loss\": result.loss},\n        )\n\n    def evaluate(\n        self, parameters: NDArrays, config: dict[str, Scalar]\n    ) -&gt; tuple[float, int, dict[str, Scalar]]:\n        self.set_weights(parameters)\n        result: TestResult = self.tester(\n            self.net, self.val_dataset, **self.extra_test_kwargs\n        )\n        return result.loss, len(self.val_dataset), result.metrics\n</code></pre>"},{"location":"api_reference/fl_tasks/huggingface/#src.fed_rag.fl_tasks.huggingface.HuggingFaceFLTask","title":"HuggingFaceFLTask","text":"<p>               Bases: <code>BaseFLTask</code></p> Source code in <code>src/fed_rag/fl_tasks/huggingface.py</code> <pre><code>class HuggingFaceFLTask(BaseFLTask):\n    _client: NumPyClient = PrivateAttr()\n    _trainer: Callable = PrivateAttr()\n    _trainer_spec: TrainerSignatureSpec = PrivateAttr()\n    _tester: Callable = PrivateAttr()\n    _tester_spec: TesterSignatureSpec = PrivateAttr()\n\n    def __init__(\n        self,\n        trainer: Callable,\n        trainer_spec: TrainerSignatureSpec,\n        tester: Callable,\n        tester_spec: TesterSignatureSpec,\n        **kwargs: Any,\n    ) -&gt; None:\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        if (\n            trainer_spec.net_parameter_class_name\n            != tester_spec.net_parameter_class_name\n        ):\n            msg = (\n                \"`trainer`'s model class is not the same as that for `tester`.\"\n            )\n            raise NetTypeMismatch(msg)\n\n        if trainer_spec.net_parameter != tester_spec.net_parameter:\n            msg = (\n                \"`trainer`'s model parameter name is not the same as that for `tester`. \"\n                \"Will use the name supplied in `trainer`.\"\n            )\n            warnings.warn(msg, UnequalNetParamWarning)\n\n        super().__init__(**kwargs)\n        self._trainer = trainer\n        self._trainer_spec = trainer_spec\n        self._tester = tester\n        self._tester_spec = tester_spec\n\n    @property\n    def training_loop(self) -&gt; Callable:\n        return self._trainer\n\n    @classmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        # extract trainer spec\n        try:\n            trainer_spec: TrainerSignatureSpec = getattr(\n                trainer, \"__fl_task_trainer_config\"\n            )\n        except AttributeError:\n            msg = \"Cannot extract `TrainerSignatureSpec` from supplied `trainer`.\"\n            raise MissingTrainerSpec(msg)\n\n        # extract tester spec\n        try:\n            tester_spec: TesterSignatureSpec = getattr(\n                tester, \"__fl_task_tester_config\"\n            )\n        except AttributeError:\n            msg = (\n                \"Cannot extract `TesterSignatureSpec` from supplied `tester`.\"\n            )\n            raise MissingTesterSpec(msg)\n\n        return cls(\n            trainer=trainer,\n            trainer_spec=trainer_spec,\n            tester=tester,\n            tester_spec=tester_spec,\n        )\n\n    def server(\n        self,\n        strategy: Strategy | None = None,\n        client_manager: ClientManager | None = None,\n        **kwargs: Any,\n    ) -&gt; HuggingFaceFlowerServer | None:\n        if strategy is None:\n            if self._trainer_spec.net_parameter not in kwargs:\n                msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n                raise MissingRequiredNetParam(msg)\n\n            model = kwargs.pop(self._trainer_spec.net_parameter)\n\n            ndarrays = _get_weights(model)\n            parameters = ndarrays_to_parameters(ndarrays)\n            strategy = FedAvg(\n                fraction_evaluate=1.0,\n                initial_parameters=parameters,\n            )\n\n        if client_manager is None:\n            client_manager = SimpleClientManager()\n\n        return HuggingFaceFlowerServer(\n            client_manager=client_manager, strategy=strategy\n        )\n\n    def client(self, **kwargs: Any) -&gt; Client | None:\n        # validate kwargs\n        if self._trainer_spec.net_parameter not in kwargs:\n            msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n            raise MissingRequiredNetParam(msg)\n        # build bundle\n        net = kwargs.pop(self._trainer_spec.net_parameter)\n        train_dataset = kwargs.pop(self._trainer_spec.train_data_param)\n        val_dataset = kwargs.pop(self._trainer_spec.val_data_param)\n\n        bundle = BaseFLTaskBundle(\n            net=net,\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            extra_train_kwargs=kwargs,\n            extra_test_kwargs={},  # TODO make this functional or get rid of it\n            trainer=self._trainer,\n            tester=self._tester,\n        )\n        return HuggingFaceFlowerClient(task_bundle=bundle)\n\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        raise NotImplementedError\n\n    @classmethod\n    def from_configs(cls, trainer_cfg: Any, tester_cfg: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/fl_tasks/pytorch/","title":"Pytorch","text":"<p>PyTorch FL Task</p>"},{"location":"api_reference/fl_tasks/pytorch/#src.fed_rag.fl_tasks.pytorch.PyTorchFLTask","title":"PyTorchFLTask","text":"<p>               Bases: <code>BaseFLTask</code></p> Source code in <code>src/fed_rag/fl_tasks/pytorch.py</code> <pre><code>class PyTorchFLTask(BaseFLTask):\n    _client: NumPyClient = PrivateAttr()\n    _trainer: Callable = PrivateAttr()\n    _trainer_spec: TrainerSignatureSpec = PrivateAttr()\n    _tester: Callable = PrivateAttr()\n    _tester_spec: TesterSignatureSpec = PrivateAttr()\n\n    def __init__(\n        self,\n        trainer: Callable,\n        trainer_spec: TrainerSignatureSpec,\n        tester: Callable,\n        tester_spec: TesterSignatureSpec,\n        **kwargs: Any,\n    ) -&gt; None:\n        if trainer_spec.net_parameter != tester_spec.net_parameter:\n            msg = (\n                \"`trainer`'s model parameter name is not the same as that for `tester`. \"\n                \"Will use the name supplied in `trainer`.\"\n            )\n            warnings.warn(msg, UnequalNetParamWarning)\n\n        super().__init__(**kwargs)\n        self._trainer = trainer\n        self._trainer_spec = trainer_spec\n        self._tester = tester\n        self._tester_spec = tester_spec\n\n    @property\n    def training_loop(self) -&gt; Callable:\n        return self._trainer\n\n    @classmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        # extract trainer spec\n        try:\n            trainer_spec: TrainerSignatureSpec = getattr(\n                trainer, \"__fl_task_trainer_config\"\n            )\n        except AttributeError:\n            msg = \"Cannot extract `TrainerSignatureSpec` from supplied `trainer`.\"\n            raise MissingTrainerSpec(msg)\n\n        # extract tester spec\n        try:\n            tester_spec: TesterSignatureSpec = getattr(\n                tester, \"__fl_task_tester_config\"\n            )\n        except AttributeError:\n            msg = (\n                \"Cannot extract `TesterSignatureSpec` from supplied `tester`.\"\n            )\n            raise MissingTesterSpec(msg)\n\n        return cls(\n            trainer=trainer,\n            trainer_spec=trainer_spec,\n            tester=tester,\n            tester_spec=tester_spec,\n        )\n\n    @classmethod\n    def from_configs(cls, trainer_cfg: Any, tester_cfg: Any) -&gt; Any:\n        return super().from_configs(trainer_cfg, tester_cfg)\n\n    def server(\n        self,\n        strategy: Strategy | None = None,\n        client_manager: ClientManager | None = None,\n        **kwargs: Any,\n    ) -&gt; PyTorchFlowerServer | None:\n        if strategy is None:\n            if self._trainer_spec.net_parameter not in kwargs:\n                msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n                raise MissingRequiredNetParam(msg)\n\n            model = kwargs.pop(self._trainer_spec.net_parameter)\n\n            ndarrays = _get_weights(model)\n            parameters = ndarrays_to_parameters(ndarrays)\n            strategy = FedAvg(\n                fraction_evaluate=1.0,\n                initial_parameters=parameters,\n            )\n\n        if client_manager is None:\n            client_manager = SimpleClientManager()\n\n        return PyTorchFlowerServer(\n            client_manager=client_manager, strategy=strategy\n        )\n\n    def client(self, **kwargs: Any) -&gt; Client | None:\n        # validate kwargs\n        if self._trainer_spec.net_parameter not in kwargs:\n            msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n            raise MissingRequiredNetParam(msg)\n        # build bundle\n        net = kwargs.pop(self._trainer_spec.net_parameter)\n        trainloader = kwargs.pop(self._trainer_spec.train_data_param)\n        valloader = kwargs.pop(self._trainer_spec.val_data_param)\n\n        bundle = BaseFLTaskBundle(\n            net=net,\n            trainloader=trainloader,\n            valloader=valloader,\n            extra_train_kwargs=kwargs,\n            extra_test_kwargs={},  # TODO make this functional or get rid of it\n            trainer=self._trainer,\n            tester=self._tester,\n        )\n        return PyTorchFlowerClient(task_bundle=bundle)\n\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/generators/","title":"Generators","text":"<p>Base Generator</p>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator","title":"BaseGenerator","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Generator Class.</p> Source code in <code>src/fed_rag/base/generator.py</code> <pre><code>class BaseGenerator(BaseModel, ABC):\n    \"\"\"Base Generator Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def generate(self, query: str, context: str, **kwargs: dict) -&gt; str:\n        \"\"\"Generate an output from a given query and context.\"\"\"\n\n    @property\n    @abstractmethod\n    def model(self) -&gt; torch.nn.Module:\n        \"\"\"Model associated with this generator.\"\"\"\n\n    @property\n    @abstractmethod\n    def tokenizer(self) -&gt; BaseTokenizer:\n        \"\"\"Tokenizer associated with this generator.\"\"\"\n\n    @abstractmethod\n    def compute_target_sequence_proba(\n        self, prompt: str, target: str\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute P(target | prompt).\n\n        NOTE: this is used in LM Supervised Retriever fine-tuning.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def prompt_template(self) -&gt; str:\n        \"\"\"Prompt template for formating query and context.\"\"\"\n\n    @prompt_template.setter\n    @abstractmethod\n    def prompt_template(self, value: str) -&gt; None:\n        \"\"\"Prompt template setter.\"\"\"\n</code></pre>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.model","title":"model  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>model\n</code></pre> <p>Model associated with this generator.</p>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.tokenizer","title":"tokenizer  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>tokenizer\n</code></pre> <p>Tokenizer associated with this generator.</p>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.prompt_template","title":"prompt_template  <code>abstractmethod</code> <code>property</code> <code>writable</code>","text":"<pre><code>prompt_template\n</code></pre> <p>Prompt template for formating query and context.</p>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(query, context, **kwargs)\n</code></pre> <p>Generate an output from a given query and context.</p> Source code in <code>src/fed_rag/base/generator.py</code> <pre><code>@abstractmethod\ndef generate(self, query: str, context: str, **kwargs: dict) -&gt; str:\n    \"\"\"Generate an output from a given query and context.\"\"\"\n</code></pre>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.compute_target_sequence_proba","title":"compute_target_sequence_proba  <code>abstractmethod</code>","text":"<pre><code>compute_target_sequence_proba(prompt, target)\n</code></pre> <p>Compute P(target | prompt).</p> <p>NOTE: this is used in LM Supervised Retriever fine-tuning.</p> Source code in <code>src/fed_rag/base/generator.py</code> <pre><code>@abstractmethod\ndef compute_target_sequence_proba(\n    self, prompt: str, target: str\n) -&gt; torch.Tensor:\n    \"\"\"Compute P(target | prompt).\n\n    NOTE: this is used in LM Supervised Retriever fine-tuning.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/generators/huggingface/","title":"Huggingface","text":"<p>HuggingFace PeftModel Generator</p> <p>HuggingFace PretrainedModel Generator</p>"},{"location":"api_reference/generators/huggingface/#src.fed_rag.generators.huggingface.hf_peft_model.HFPeftModelGenerator","title":"HFPeftModelGenerator","text":"<p>               Bases: <code>HuggingFaceGeneratorMixin</code>, <code>BaseGenerator</code></p> <p>HFPeftModelGenerator Class.</p> <p>NOTE: this class supports loading PeftModel's from HF Hub or from local. TODO: support loading custom models via a <code>~peft.Config</code> and <code>~peft.get_peft_model</code></p> Source code in <code>src/fed_rag/generators/huggingface/hf_peft_model.py</code> <pre><code>class HFPeftModelGenerator(HuggingFaceGeneratorMixin, BaseGenerator):\n    \"\"\"HFPeftModelGenerator Class.\n\n    NOTE: this class supports loading PeftModel's from HF Hub or from local.\n    TODO: support loading custom models via a `~peft.Config` and `~peft.get_peft_model`\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str = Field(\n        description=\"Name of Peft model. Used for loading model from HF hub or local.\"\n    )\n    base_model_name: str = Field(\n        description=\"Name of the frozen HuggingFace base model. Used for loading the model from HF hub or local.\"\n    )\n    generation_config: \"GenerationConfig\" = Field(\n        description=\"The generation config used for generating with the PreTrainedModel.\"\n    )\n    load_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading peft model from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    load_base_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading base model from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    _prompt_template: str = PrivateAttr(default=DEFAULT_PROMPT_TEMPLATE)\n    _model: Optional[\"PeftModel\"] = PrivateAttr(default=None)\n    _tokenizer: HFPretrainedTokenizer | None = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str,\n        base_model_name: str,\n        generation_config: Optional[\"GenerationConfig\"] = None,\n        prompt_template: str | None = None,\n        load_model_kwargs: dict | None = None,\n        load_base_model_kwargs: dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        # if reaches here, then passed checks for huggingface extra installation\n        from transformers.generation.utils import GenerationConfig\n\n        generation_config = generation_config or GenerationConfig()\n        super().__init__(\n            model_name=model_name,\n            base_model_name=base_model_name,\n            generation_config=generation_config,\n            prompt_template=prompt_template,\n            load_model_kwargs=load_model_kwargs or {},\n            load_base_model_kwargs=load_base_model_kwargs or {},\n        )\n        self._tokenizer = HFPretrainedTokenizer(\n            model_name=base_model_name, load_model_at_init=load_model_at_init\n        )\n        self._prompt_template = prompt_template or DEFAULT_PROMPT_TEMPLATE\n        if load_model_at_init:\n            self._model = self._load_model_from_hf()\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_dependencies(cls, data: Any) -&gt; Any:\n        \"\"\"Validate that huggingface dependencies are installed.\"\"\"\n        check_huggingface_installed(cls.__name__)\n        return data\n\n    def _load_model_from_hf(self, **kwargs: Any) -&gt; \"PeftModel\":\n        from peft import PeftModel, prepare_model_for_kbit_training\n        from transformers import AutoModelForCausalLM\n\n        self.load_model_kwargs.update(kwargs)  # update load_model_kwargs\n        base_model = AutoModelForCausalLM.from_pretrained(\n            self.base_model_name, **self.load_base_model_kwargs\n        )\n\n        if \"quantization_config\" in self.load_base_model_kwargs:\n            # preprocess model for kbit fine-tuning\n            # https://huggingface.co/docs/peft/developer_guides/quantization\n            base_model = prepare_model_for_kbit_training(base_model)\n\n        return PeftModel.from_pretrained(\n            base_model, self.model_name, **self.load_model_kwargs\n        )\n\n    @property\n    def model(self) -&gt; \"PeftModel\":\n        if self._model is None:\n            # load HF PeftModel\n            self._model = self._load_model_from_hf()\n        return self._model\n\n    @model.setter\n    def model(self, value: \"PeftModel\") -&gt; None:\n        self._model = value\n\n    @property\n    def tokenizer(self) -&gt; HFPretrainedTokenizer:\n        return self._tokenizer\n\n    @tokenizer.setter\n    def tokenizer(self, value: HFPretrainedTokenizer) -&gt; None:\n        self._tokenizer = value\n\n    @property\n    def prompt_template(self) -&gt; str:\n        return self._prompt_template\n\n    @prompt_template.setter\n    def prompt_template(self, value: str) -&gt; None:\n        self._prompt_template = value\n</code></pre>"},{"location":"api_reference/generators/huggingface/#src.fed_rag.generators.huggingface.hf_peft_model.HFPeftModelGenerator.check_dependencies","title":"check_dependencies  <code>classmethod</code>","text":"<pre><code>check_dependencies(data)\n</code></pre> <p>Validate that huggingface dependencies are installed.</p> Source code in <code>src/fed_rag/generators/huggingface/hf_peft_model.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef check_dependencies(cls, data: Any) -&gt; Any:\n    \"\"\"Validate that huggingface dependencies are installed.\"\"\"\n    check_huggingface_installed(cls.__name__)\n    return data\n</code></pre>"},{"location":"api_reference/generators/huggingface/#src.fed_rag.generators.huggingface.hf_pretrained_model.HFPretrainedModelGenerator","title":"HFPretrainedModelGenerator","text":"<p>               Bases: <code>HuggingFaceGeneratorMixin</code>, <code>BaseGenerator</code></p> Source code in <code>src/fed_rag/generators/huggingface/hf_pretrained_model.py</code> <pre><code>class HFPretrainedModelGenerator(HuggingFaceGeneratorMixin, BaseGenerator):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str = Field(\n        description=\"Name of HuggingFace model. Used for loading the model from HF hub or local.\"\n    )\n    generation_config: \"GenerationConfig\" = Field(\n        description=\"The generation config used for generating with the PreTrainedModel.\"\n    )\n    load_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading models from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    _prompt_template: str = PrivateAttr(default=DEFAULT_PROMPT_TEMPLATE)\n    _model: Optional[\"PreTrainedModel\"] = PrivateAttr(default=None)\n    _tokenizer: HFPretrainedTokenizer | None = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str,\n        generation_config: Optional[\"GenerationConfig\"] = None,\n        prompt_template: str | None = None,\n        load_model_kwargs: dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        # if reaches here, then passed checks for extra\n        from transformers.generation.utils import GenerationConfig\n\n        generation_config = generation_config or GenerationConfig()\n        super().__init__(\n            model_name=model_name,\n            generation_config=generation_config,\n            load_model_kwargs=load_model_kwargs or {},\n        )\n        self._tokenizer = HFPretrainedTokenizer(\n            model_name=model_name, load_model_at_init=load_model_at_init\n        )\n        self._prompt_template = prompt_template or DEFAULT_PROMPT_TEMPLATE\n        if load_model_at_init:\n            self._model = self._load_model_from_hf()\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_dependencies(cls, data: Any) -&gt; Any:\n        \"\"\"Validate that huggingface dependencies are installed.\"\"\"\n        check_huggingface_installed(cls.__name__)\n        return data\n\n    def _load_model_from_hf(self, **kwargs: Any) -&gt; \"PreTrainedModel\":\n        from transformers import AutoModelForCausalLM\n\n        self.load_model_kwargs.update(kwargs)\n        return AutoModelForCausalLM.from_pretrained(\n            self.model_name, **self.load_model_kwargs\n        )\n\n    @property\n    def model(self) -&gt; \"PreTrainedModel\":\n        if self._model is None:\n            # load HF Pretrained Model\n            self._model = self._load_model_from_hf()\n        return self._model\n\n    @model.setter\n    def model(self, value: \"PreTrainedModel\") -&gt; None:\n        self._model = value\n\n    @property\n    def tokenizer(self) -&gt; HFPretrainedTokenizer:\n        return self._tokenizer\n\n    @tokenizer.setter\n    def tokenizer(self, value: HFPretrainedTokenizer) -&gt; None:\n        self._tokenizer = value\n\n    @property\n    def prompt_template(self) -&gt; str:\n        return self._prompt_template\n\n    @prompt_template.setter\n    def prompt_template(self, value: str) -&gt; None:\n        self._prompt_template = value\n</code></pre>"},{"location":"api_reference/generators/huggingface/#src.fed_rag.generators.huggingface.hf_pretrained_model.HFPretrainedModelGenerator.check_dependencies","title":"check_dependencies  <code>classmethod</code>","text":"<pre><code>check_dependencies(data)\n</code></pre> <p>Validate that huggingface dependencies are installed.</p> Source code in <code>src/fed_rag/generators/huggingface/hf_pretrained_model.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef check_dependencies(cls, data: Any) -&gt; Any:\n    \"\"\"Validate that huggingface dependencies are installed.\"\"\"\n    check_huggingface_installed(cls.__name__)\n    return data\n</code></pre>"},{"location":"api_reference/generators/unsloth/","title":"Unsloth","text":"<p>Unsloth FastModel Generator</p>"},{"location":"api_reference/generators/unsloth/#src.fed_rag.generators.unsloth.unsloth_fast_model.UnslothFastModelGenerator","title":"UnslothFastModelGenerator","text":"<p>               Bases: <code>UnslothGeneratorMixin</code>, <code>BaseGenerator</code></p> <p>Unsloth FastModel Integration.</p> <p>System Requirements (see https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements)     - Operating System: Works on Linux and Windows.     - Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0     (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc)     Check your GPU! GTX 1070, 1080 works, but is slow.     - Your device must have xformers, torch, BitsandBytes and triton support.     - Unsloth only works if you have a NVIDIA GPU. Make sure you also have disk space to train &amp; save your model</p> Source code in <code>src/fed_rag/generators/unsloth/unsloth_fast_model.py</code> <pre><code>class UnslothFastModelGenerator(UnslothGeneratorMixin, BaseGenerator):\n    \"\"\"Unsloth FastModel Integration.\n\n    System Requirements (see https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements)\n        - Operating System: Works on Linux and Windows.\n        - Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0\n        (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc)\n        Check your GPU! GTX 1070, 1080 works, but is slow.\n        - Your device must have xformers, torch, BitsandBytes and triton support.\n        - Unsloth only works if you have a NVIDIA GPU. Make sure you also have disk space to train &amp; save your model\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str = Field(\n        description=\"Name of Unsloth model. Used for loading the model from HF hub or local.\"\n    )\n    generation_config: \"GenerationConfig\" = Field(\n        description=\"The generation config used for generating with the PreTrainedModel.\"\n    )\n    load_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading ~unsloth.FastModel.from_pretrained(). Defaults to None.\",\n        default_factory=dict,\n    )\n    _prompt_template: str = PrivateAttr(default=DEFAULT_PROMPT_TEMPLATE)\n    _model: Optional[Union[\"PreTrainedModel\", \"PeftModel\"]] = PrivateAttr(\n        default=None\n    )\n    _tokenizer: UnslothPretrainedTokenizer | None = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str,\n        generation_config: Optional[\"GenerationConfig\"] = None,\n        prompt_template: str | None = None,\n        load_model_kwargs: dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        # if reaches here, then passed checks for extra\n        from transformers.generation.utils import GenerationConfig\n\n        generation_config = (\n            generation_config if generation_config else GenerationConfig()\n        )\n        super().__init__(\n            model_name=model_name,\n            generation_config=generation_config,\n            load_model_kwargs=load_model_kwargs if load_model_kwargs else {},\n        )\n        self._prompt_template = (\n            prompt_template if prompt_template else DEFAULT_PROMPT_TEMPLATE\n        )\n        if load_model_at_init:\n            self._model, tokenizer = self._load_model_and_tokenizer()\n            self._tokenizer = UnslothPretrainedTokenizer(\n                model_name=self.model_name, tokenizer=tokenizer\n            )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_dependencies(cls, data: Any) -&gt; Any:\n        \"\"\"Validate that qdrant dependencies are installed.\"\"\"\n        check_unsloth_installed(cls.__name__)\n        return data\n\n    def _load_model_and_tokenizer(\n        self, **kwargs: Any\n    ) -&gt; tuple[Union[\"PreTrainedModel\", \"PeftModel\"], \"PreTrainedTokenizer\"]:\n        from unsloth import FastLanguageModel\n\n        load_kwargs = self.load_model_kwargs\n        load_kwargs.update(kwargs)\n        self.load_model_kwargs = load_kwargs\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            self.model_name, **load_kwargs\n        )\n        return model, tokenizer\n\n    @property\n    def model(self) -&gt; Union[\"PreTrainedModel\", \"PeftModel\"]:\n        if self._model is None:\n            # load HF Pretrained Model\n            model, tokenizer = self._load_model_and_tokenizer()\n            self._model = model\n            if self._tokenizer is None:\n                self._tokenizer = UnslothPretrainedTokenizer(\n                    model_name=self.model_name, tokenizer=tokenizer\n                )\n        return self._model\n\n    @model.setter\n    def model(self, value: Union[\"PreTrainedModel\", \"PeftModel\"]) -&gt; None:\n        self._model = value\n\n    @property\n    def tokenizer(self) -&gt; UnslothPretrainedTokenizer:\n        return self._tokenizer\n\n    @tokenizer.setter\n    def tokenizer(self, value: UnslothPretrainedTokenizer) -&gt; None:\n        self._tokenizer = value\n\n    @property\n    def prompt_template(self) -&gt; str:\n        return self._prompt_template\n\n    @prompt_template.setter\n    def prompt_template(self, value: str) -&gt; None:\n        self._prompt_template = value\n\n    def _get_peft_model(self, **kwargs: Any) -&gt; \"PeftModel\":\n        \"\"\"A light wrapper over ~FastModel.get_peft_model().\"\"\"\n        from unsloth import FastLanguageModel\n\n        model = FastLanguageModel.get_peft_model(self.model, **kwargs)\n\n        # Fix any potential dtype mismatch with any adapters and base model\n        base_dtype = next(model.parameters()).dtype\n\n        for _name, param in model.named_parameters():\n            if param.requires_grad and param.dtype != base_dtype:\n                param.data = param.data.to(base_dtype)\n\n        return model\n\n    def to_peft(self, **kwargs: Any) -&gt; Self:\n        \"\"\"Sets the current model to PeftModel\n\n        NOTE: Pass params to underlying get_peft_model using **kwargs.\n\n        This returns Self to support fluent style:\n            `generator = UnslothFastModelGenerator(...).to_peft(...)`\n        \"\"\"\n        from peft import PeftModel\n\n        if isinstance(self.model, PeftModel):\n            raise GeneratorError(\n                \"Cannot use `to_peft` when underlying model is already a `~peft.PeftModel`.\"\n            )\n\n        # set model to new peft model\n        self.model = self._get_peft_model(**kwargs)\n        return self\n</code></pre>"},{"location":"api_reference/generators/unsloth/#src.fed_rag.generators.unsloth.unsloth_fast_model.UnslothFastModelGenerator.check_dependencies","title":"check_dependencies  <code>classmethod</code>","text":"<pre><code>check_dependencies(data)\n</code></pre> <p>Validate that qdrant dependencies are installed.</p> Source code in <code>src/fed_rag/generators/unsloth/unsloth_fast_model.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef check_dependencies(cls, data: Any) -&gt; Any:\n    \"\"\"Validate that qdrant dependencies are installed.\"\"\"\n    check_unsloth_installed(cls.__name__)\n    return data\n</code></pre>"},{"location":"api_reference/generators/unsloth/#src.fed_rag.generators.unsloth.unsloth_fast_model.UnslothFastModelGenerator.to_peft","title":"to_peft","text":"<pre><code>to_peft(**kwargs)\n</code></pre> <p>Sets the current model to PeftModel</p> <p>NOTE: Pass params to underlying get_peft_model using **kwargs.</p> This returns Self to support fluent style <p><code>generator = UnslothFastModelGenerator(...).to_peft(...)</code></p> Source code in <code>src/fed_rag/generators/unsloth/unsloth_fast_model.py</code> <pre><code>def to_peft(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Sets the current model to PeftModel\n\n    NOTE: Pass params to underlying get_peft_model using **kwargs.\n\n    This returns Self to support fluent style:\n        `generator = UnslothFastModelGenerator(...).to_peft(...)`\n    \"\"\"\n    from peft import PeftModel\n\n    if isinstance(self.model, PeftModel):\n        raise GeneratorError(\n            \"Cannot use `to_peft` when underlying model is already a `~peft.PeftModel`.\"\n        )\n\n    # set model to new peft model\n    self.model = self._get_peft_model(**kwargs)\n    return self\n</code></pre>"},{"location":"api_reference/inspectors/","title":"Inspectors","text":"<p>Common abstractions for inspectors</p> <p>Data structures for results</p>"},{"location":"api_reference/inspectors/#src.fed_rag.inspectors.common.TesterSignatureSpec","title":"TesterSignatureSpec","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/inspectors/common.py</code> <pre><code>class TesterSignatureSpec(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    net_parameter: str\n    test_data_param: str\n    extra_test_kwargs: list[str] = []\n    net_parameter_class_name: str\n</code></pre>"},{"location":"api_reference/inspectors/#src.fed_rag.inspectors.common.TesterSignatureSpec","title":"TesterSignatureSpec","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/inspectors/common.py</code> <pre><code>class TesterSignatureSpec(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    net_parameter: str\n    test_data_param: str\n    extra_test_kwargs: list[str] = []\n    net_parameter_class_name: str\n</code></pre>"},{"location":"api_reference/inspectors/#src.fed_rag.data_structures.results.TrainResult","title":"TrainResult","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/data_structures/results.py</code> <pre><code>class TrainResult(BaseModel):\n    loss: float\n</code></pre>"},{"location":"api_reference/inspectors/#src.fed_rag.data_structures.results.TestResult","title":"TestResult","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/data_structures/results.py</code> <pre><code>class TestResult(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    loss: float\n    metrics: dict[str, Any] = Field(\n        description=\"Additional metrics computed on test set.\",\n        default_factory=dict,\n    )\n</code></pre>"},{"location":"api_reference/inspectors/huggingface/","title":"Huggingface","text":""},{"location":"api_reference/inspectors/huggingface/#src.fed_rag.inspectors.huggingface.inspect_trainer_signature","title":"inspect_trainer_signature","text":"<pre><code>inspect_trainer_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/huggingface/trainer.py</code> <pre><code>def inspect_trainer_signature(fn: Callable) -&gt; TrainerSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TrainResult):\n        msg = \"Trainer should return a fed_rag.data_structures.TrainResult or a subclass of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_train_kwargs = []\n    net_param = None\n    train_data_param = None\n    val_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := get_type_name(t):\n            if (\n                type_name\n                in [\n                    \"SentenceTransformer\",\n                    \"PreTrainedModel\",\n                    \"PeftModel\",\n                    \"HFModelType\",\n                ]  # TODO: should accept union types involving these two\n                and net_param is None\n            ):\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"Dataset\" and train_data_param is None:\n                train_data_param = name\n                continue\n\n            if type_name == \"Dataset\" and val_data_param is None:\n                val_data_param = name\n                continue\n\n        extra_train_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For HuggingFace this param must have type `PreTrainedModel` or `SentenceTransformers`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if train_data_param is None:\n        msg = (\n            \"Inspection failed to find two data params for train and val datasets.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingMultipleDataParams(msg)\n\n    if val_data_param is None:\n        msg = (\n            \"Inspection found one data param but failed to find another. \"\n            \"Two data params are required for train and val datasets.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TrainerSignatureSpec(\n        net_parameter=net_param,\n        train_data_param=train_data_param,\n        val_data_param=val_data_param,\n        extra_train_kwargs=extra_train_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/huggingface/#src.fed_rag.inspectors.huggingface.inspect_tester_signature","title":"inspect_tester_signature","text":"<pre><code>inspect_tester_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/huggingface/tester.py</code> <pre><code>def inspect_tester_signature(fn: Callable) -&gt; TesterSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TestResult):\n        msg = \"Tester should return a fed_rag.data_structures.TestResult or a subclass of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_tester_kwargs = []\n    net_param = None\n    test_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := get_type_name(t):\n            if (\n                type_name\n                in [\n                    \"PreTrainedModel\",\n                    \"SentenceTransformer\",\n                    \"PeftModel\",\n                    \"HFModelType\",\n                ]\n                and net_param is None\n            ):\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"Dataset\" and test_data_param is None:\n                test_data_param = name\n                continue\n\n        extra_tester_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For HuggingFace this param must have type `PreTrainedModel` or `SentenceTransformers`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if test_data_param is None:\n        msg = (\n            \"Inspection failed to find a data param for a test dataset.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TesterSignatureSpec(\n        net_parameter=net_param,\n        test_data_param=test_data_param,\n        extra_test_kwargs=extra_tester_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/pytorch/","title":"Pytorch","text":""},{"location":"api_reference/inspectors/pytorch/#src.fed_rag.inspectors.pytorch.inspect_trainer_signature","title":"inspect_trainer_signature","text":"<pre><code>inspect_trainer_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/pytorch/trainer.py</code> <pre><code>def inspect_trainer_signature(fn: Callable) -&gt; TrainerSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TrainResult):\n        msg = \"Trainer should return a fed_rag.data_structures.TrainResult or a subclsas of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_train_kwargs = []\n    net_param = None\n    train_data_param = None\n    val_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if type_name == \"Module\" and net_param is None:\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"DataLoader\" and train_data_param is None:\n                train_data_param = name\n                continue\n\n            if type_name == \"DataLoader\" and val_data_param is None:\n                val_data_param = name\n                continue\n\n        extra_train_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For PyTorch this param must have type `nn.Module`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if train_data_param is None:\n        msg = (\n            \"Inspection failed to find two data params for train and val datasets.\"\n            \"For PyTorch these params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingMultipleDataParams(msg)\n\n    if val_data_param is None:\n        msg = (\n            \"Inspection found one data param but failed to find another. \"\n            \"Two data params are required for train and val datasets.\"\n            \"For PyTorch these params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TrainerSignatureSpec(\n        net_parameter=net_param,\n        train_data_param=train_data_param,\n        val_data_param=val_data_param,\n        extra_train_kwargs=extra_train_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/pytorch/#src.fed_rag.inspectors.pytorch.inspect_tester_signature","title":"inspect_tester_signature","text":"<pre><code>inspect_tester_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/pytorch/tester.py</code> <pre><code>def inspect_tester_signature(fn: Callable) -&gt; TesterSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TestResult):\n        msg = \"Tester should return a fed_rag.data_structures.TestResult or a subclsas of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_tester_kwargs = []\n    net_param = None\n    test_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if type_name == \"Module\" and net_param is None:\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"DataLoader\" and test_data_param is None:\n                test_data_param = name\n                continue\n\n        extra_tester_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For PyTorch this param must have type `nn.Module`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if test_data_param is None:\n        msg = (\n            \"Inspection failed to find a data param for a test dataset.\"\n            \"For PyTorch this params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TesterSignatureSpec(\n        net_parameter=net_param,\n        test_data_param=test_data_param,\n        extra_test_kwargs=extra_tester_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/knowledge_nodes/","title":"Index","text":"<p>Knowledge Node</p>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.data_structures.knowledge_node.KnowledgeNode","title":"KnowledgeNode","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/data_structures/knowledge_node.py</code> <pre><code>class KnowledgeNode(BaseModel):\n    model_config = ConfigDict(\n        # ensures that validation is performed for defaulted None values\n        validate_default=True\n    )\n    node_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    embedding: list[float] | None = Field(\n        description=\"Encoded representation of node. If multimodal type, then this is shared embedding between image and text.\",\n        default=None,\n    )\n    node_type: NodeType = Field(description=\"Type of node.\")\n    text_content: str | None = Field(\n        description=\"Text content. Used for TEXT and potentially MULTIMODAL node types.\",\n        default=None,\n    )\n    image_content: bytes | None = Field(\n        description=\"Image content as binary data (decoded from base64)\",\n        default=None,\n    )\n    metadata: dict = Field(\n        description=\"Metadata for node.\", default_factory=dict\n    )\n\n    # validators\n    @field_validator(\"text_content\", mode=\"before\")\n    @classmethod\n    def validate_text_content(\n        cls, value: str | None, info: ValidationInfo\n    ) -&gt; str | None:\n        node_type = info.data.get(\"node_type\")\n        node_type = cast(NodeType, node_type)\n        if node_type == NodeType.TEXT and value is None:\n            raise ValueError(\"NodeType == 'text', but text_content is None.\")\n\n        if node_type == NodeType.MULTIMODAL and value is None:\n            raise ValueError(\n                \"NodeType == 'multimodal', but text_content is None.\"\n            )\n\n        return value\n\n    @field_validator(\"image_content\", mode=\"after\")\n    @classmethod\n    def validate_image_content(\n        cls, value: str | None, info: ValidationInfo\n    ) -&gt; str | None:\n        node_type = info.data.get(\"node_type\")\n        node_type = cast(NodeType, node_type)\n        if node_type == NodeType.IMAGE:\n            if value is None:\n                raise ValueError(\n                    \"NodeType == 'image', but image_content is None.\"\n                )\n\n        if node_type == NodeType.MULTIMODAL:\n            if value is None:\n                raise ValueError(\n                    \"NodeType == 'multimodal', but image_content is None.\"\n                )\n\n        return value\n\n    def get_content(self) -&gt; NodeContent:\n        \"\"\"Return dict of node content.\"\"\"\n        content: NodeContent = {\n            \"image_content\": self.image_content,\n            \"text_content\": self.text_content,\n        }\n        return content\n\n    @field_serializer(\"metadata\")\n    def serialize_metadata(\n        self, metadata: dict[Any, Any] | None\n    ) -&gt; str | None:\n        \"\"\"\n        Custom serializer for the metadata field.\n\n        Will serialize the metadata field into a json string.\n\n        Args:\n            metadata: Metadata dictionary to serialize.\n\n        Returns:\n            Serialized metadata as a json string.\n        \"\"\"\n        if metadata:\n            return json.dumps(metadata)\n        return None\n\n    @field_validator(\"metadata\", mode=\"before\")\n    @classmethod\n    def deserialize_metadata(\n        cls, metadata: dict[Any, Any] | str | None\n    ) -&gt; dict[Any, Any] | None:\n        \"\"\"\n        Custom validator for the metadata field.\n\n        Will deserialize the metadata from a json string if it's a string.\n\n        Args:\n            metadata: Metadata to validate. If it is a json string, it will be deserialized into a dictionary.\n\n        Returns:\n            Validated metadata.\n        \"\"\"\n        if isinstance(metadata, str):\n            deserialized_metadata = json.loads(metadata)\n            return cast(dict[Any, Any], deserialized_metadata)\n        if metadata is None:\n            return {}\n        return metadata\n\n    def model_dump_without_embeddings(self) -&gt; dict[str, Any]:\n        \"\"\"Seriliaze the node without the embedding.\"\"\"\n        return self.model_dump(exclude={\"embedding\"})\n</code></pre>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.data_structures.knowledge_node.KnowledgeNode.get_content","title":"get_content","text":"<pre><code>get_content()\n</code></pre> <p>Return dict of node content.</p> Source code in <code>src/fed_rag/data_structures/knowledge_node.py</code> <pre><code>def get_content(self) -&gt; NodeContent:\n    \"\"\"Return dict of node content.\"\"\"\n    content: NodeContent = {\n        \"image_content\": self.image_content,\n        \"text_content\": self.text_content,\n    }\n    return content\n</code></pre>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.data_structures.knowledge_node.KnowledgeNode.serialize_metadata","title":"serialize_metadata","text":"<pre><code>serialize_metadata(metadata)\n</code></pre> <p>Custom serializer for the metadata field.</p> <p>Will serialize the metadata field into a json string.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[Any, Any] | None</code> <p>Metadata dictionary to serialize.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Serialized metadata as a json string.</p> Source code in <code>src/fed_rag/data_structures/knowledge_node.py</code> <pre><code>@field_serializer(\"metadata\")\ndef serialize_metadata(\n    self, metadata: dict[Any, Any] | None\n) -&gt; str | None:\n    \"\"\"\n    Custom serializer for the metadata field.\n\n    Will serialize the metadata field into a json string.\n\n    Args:\n        metadata: Metadata dictionary to serialize.\n\n    Returns:\n        Serialized metadata as a json string.\n    \"\"\"\n    if metadata:\n        return json.dumps(metadata)\n    return None\n</code></pre>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.data_structures.knowledge_node.KnowledgeNode.deserialize_metadata","title":"deserialize_metadata  <code>classmethod</code>","text":"<pre><code>deserialize_metadata(metadata)\n</code></pre> <p>Custom validator for the metadata field.</p> <p>Will deserialize the metadata from a json string if it's a string.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[Any, Any] | str | None</code> <p>Metadata to validate. If it is a json string, it will be deserialized into a dictionary.</p> required <p>Returns:</p> Type Description <code>dict[Any, Any] | None</code> <p>Validated metadata.</p> Source code in <code>src/fed_rag/data_structures/knowledge_node.py</code> <pre><code>@field_validator(\"metadata\", mode=\"before\")\n@classmethod\ndef deserialize_metadata(\n    cls, metadata: dict[Any, Any] | str | None\n) -&gt; dict[Any, Any] | None:\n    \"\"\"\n    Custom validator for the metadata field.\n\n    Will deserialize the metadata from a json string if it's a string.\n\n    Args:\n        metadata: Metadata to validate. If it is a json string, it will be deserialized into a dictionary.\n\n    Returns:\n        Validated metadata.\n    \"\"\"\n    if isinstance(metadata, str):\n        deserialized_metadata = json.loads(metadata)\n        return cast(dict[Any, Any], deserialized_metadata)\n    if metadata is None:\n        return {}\n    return metadata\n</code></pre>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.data_structures.knowledge_node.KnowledgeNode.model_dump_without_embeddings","title":"model_dump_without_embeddings","text":"<pre><code>model_dump_without_embeddings()\n</code></pre> <p>Seriliaze the node without the embedding.</p> Source code in <code>src/fed_rag/data_structures/knowledge_node.py</code> <pre><code>def model_dump_without_embeddings(self) -&gt; dict[str, Any]:\n    \"\"\"Seriliaze the node without the embedding.\"\"\"\n    return self.model_dump(exclude={\"embedding\"})\n</code></pre>"},{"location":"api_reference/knowledge_stores/","title":"Base KnowledgeStore","text":"<p>Base Knowledge Store</p>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore","title":"BaseKnowledgeStore","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Knowledge Store Class.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>class BaseKnowledgeStore(BaseModel, ABC):\n    \"\"\"Base Knowledge Store Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    name: str = Field(\n        description=\"Name of Knowledge Store used for caching and loading.\",\n        default=DEFAULT_KNOWLEDGE_STORE_NAME,\n    )\n\n    @abstractmethod\n    def load_node(self, node: \"KnowledgeNode\") -&gt; None:\n        \"\"\"Load a \"KnowledgeNode\" into the KnowledgeStore.\"\"\"\n\n    @abstractmethod\n    def load_nodes(self, nodes: list[\"KnowledgeNode\"]) -&gt; None:\n        \"\"\"Load multiple \"KnowledgeNode\"s in batch.\"\"\"\n\n    @abstractmethod\n    def retrieve(\n        self, query_emb: list[float], top_k: int\n    ) -&gt; list[tuple[float, \"KnowledgeNode\"]]:\n        \"\"\"Retrieve top-k nodes from KnowledgeStore against a provided user query.\n\n        Returns:\n            A list of tuples where the first element represents the similarity score\n            of the node to the query, and the second element is the node itself.\n        \"\"\"\n\n    @abstractmethod\n    def delete_node(self, node_id: str) -&gt; bool:\n        \"\"\"Remove a node from the KnowledgeStore by ID, returning success status.\"\"\"\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Clear all nodes from the KnowledgeStore.\"\"\"\n\n    @property\n    @abstractmethod\n    def count(self) -&gt; int:\n        \"\"\"Return the number of nodes in the store.\"\"\"\n\n    @abstractmethod\n    def persist(self) -&gt; None:\n        \"\"\"Save the KnowledgeStore nodes to a permanent storage.\"\"\"\n\n    @abstractmethod\n    def load(self) -&gt; None:\n        \"\"\"Load the KnowledgeStore nodes from a permanent storage using `name`.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.count","title":"count  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>count\n</code></pre> <p>Return the number of nodes in the store.</p>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load_node","title":"load_node  <code>abstractmethod</code>","text":"<pre><code>load_node(node)\n</code></pre> <p>Load a \"KnowledgeNode\" into the KnowledgeStore.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef load_node(self, node: \"KnowledgeNode\") -&gt; None:\n    \"\"\"Load a \"KnowledgeNode\" into the KnowledgeStore.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load_nodes","title":"load_nodes  <code>abstractmethod</code>","text":"<pre><code>load_nodes(nodes)\n</code></pre> <p>Load multiple \"KnowledgeNode\"s in batch.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef load_nodes(self, nodes: list[\"KnowledgeNode\"]) -&gt; None:\n    \"\"\"Load multiple \"KnowledgeNode\"s in batch.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.retrieve","title":"retrieve  <code>abstractmethod</code>","text":"<pre><code>retrieve(query_emb, top_k)\n</code></pre> <p>Retrieve top-k nodes from KnowledgeStore against a provided user query.</p> <p>Returns:</p> Type Description <code>list[tuple[float, KnowledgeNode]]</code> <p>A list of tuples where the first element represents the similarity score</p> <code>list[tuple[float, KnowledgeNode]]</code> <p>of the node to the query, and the second element is the node itself.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef retrieve(\n    self, query_emb: list[float], top_k: int\n) -&gt; list[tuple[float, \"KnowledgeNode\"]]:\n    \"\"\"Retrieve top-k nodes from KnowledgeStore against a provided user query.\n\n    Returns:\n        A list of tuples where the first element represents the similarity score\n        of the node to the query, and the second element is the node itself.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.delete_node","title":"delete_node  <code>abstractmethod</code>","text":"<pre><code>delete_node(node_id)\n</code></pre> <p>Remove a node from the KnowledgeStore by ID, returning success status.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef delete_node(self, node_id: str) -&gt; bool:\n    \"\"\"Remove a node from the KnowledgeStore by ID, returning success status.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.clear","title":"clear  <code>abstractmethod</code>","text":"<pre><code>clear()\n</code></pre> <p>Clear all nodes from the KnowledgeStore.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Clear all nodes from the KnowledgeStore.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.persist","title":"persist  <code>abstractmethod</code>","text":"<pre><code>persist()\n</code></pre> <p>Save the KnowledgeStore nodes to a permanent storage.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef persist(self) -&gt; None:\n    \"\"\"Save the KnowledgeStore nodes to a permanent storage.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load()\n</code></pre> <p>Load the KnowledgeStore nodes from a permanent storage using <code>name</code>.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef load(self) -&gt; None:\n    \"\"\"Load the KnowledgeStore nodes from a permanent storage using `name`.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/in_memory/","title":"InMemory","text":"<p>In Memory Knowledge Store</p>"},{"location":"api_reference/knowledge_stores/in_memory/#src.fed_rag.knowledge_stores.in_memory.InMemoryKnowledgeStore","title":"InMemoryKnowledgeStore","text":"<p>               Bases: <code>BaseKnowledgeStore</code></p> <p>InMemoryKnowledgeStore Class.</p> Source code in <code>src/fed_rag/knowledge_stores/in_memory.py</code> <pre><code>class InMemoryKnowledgeStore(BaseKnowledgeStore):\n    \"\"\"InMemoryKnowledgeStore Class.\"\"\"\n\n    cache_dir: str = Field(default=DEFAULT_CACHE_DIR)\n    _data: dict[str, KnowledgeNode] = PrivateAttr(default_factory=dict)\n\n    @classmethod\n    def from_nodes(cls, nodes: list[KnowledgeNode], **kwargs: Any) -&gt; Self:\n        instance = cls(**kwargs)\n        instance.load_nodes(nodes)\n        return instance\n\n    def load_node(self, node: KnowledgeNode) -&gt; None:\n        if node.node_id not in self._data:\n            self._data[node.node_id] = node\n\n    def load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n        for node in nodes:\n            self.load_node(node)\n\n    def retrieve(\n        self, query_emb: list[float], top_k: int\n    ) -&gt; list[tuple[float, KnowledgeNode]]:\n        all_nodes = list(self._data.values())\n        node_ids_and_scores = _get_top_k_nodes(\n            nodes=all_nodes, query_emb=query_emb, top_k=top_k\n        )\n        return [(el[1], self._data[el[0]]) for el in node_ids_and_scores]\n\n    def delete_node(self, node_id: str) -&gt; bool:\n        if node_id in self._data:\n            del self._data[node_id]\n            return True\n        else:\n            return False\n\n    def clear(self) -&gt; None:\n        self._data = {}\n\n    @property\n    def count(self) -&gt; int:\n        return len(self._data)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(self, handler: Any) -&gt; Dict[str, Any]:\n        data = handler(self)\n        data = cast(Dict[str, Any], data)\n        # include _data in serialization\n        if self._data:\n            data[\"_data\"] = self._data\n        return data  # type: ignore[no-any-return]\n\n    def persist(self) -&gt; None:\n        serialized_model = self.model_dump()\n        data_values = list(serialized_model[\"_data\"].values())\n\n        parquet_table = pa.Table.from_pylist(data_values)\n\n        filename = Path(self.cache_dir) / f\"{self.name}.parquet\"\n        Path(filename).parent.mkdir(parents=True, exist_ok=True)\n        pq.write_table(parquet_table, filename)\n\n    def load(self) -&gt; None:\n        filename = Path(self.cache_dir) / f\"{self.name}.parquet\"\n        if not filename.exists():\n            msg = f\"Knowledge store '{self.name}' not found at expected location: {filename}\"\n            raise KnowledgeStoreNotFoundError(msg)\n\n        parquet_data = pq.read_table(filename).to_pylist()\n        nodes = [KnowledgeNode(**data) for data in parquet_data]\n        self.load_nodes(nodes)\n</code></pre>"},{"location":"api_reference/knowledge_stores/in_memory/#src.fed_rag.knowledge_stores.in_memory.ManagedInMemoryKnowledgeStore","title":"ManagedInMemoryKnowledgeStore","text":"<p>               Bases: <code>ManagedMixin</code>, <code>InMemoryKnowledgeStore</code></p> Source code in <code>src/fed_rag/knowledge_stores/in_memory.py</code> <pre><code>class ManagedInMemoryKnowledgeStore(ManagedMixin, InMemoryKnowledgeStore):\n    def persist(self) -&gt; None:\n        serialized_model = self.model_dump()\n        data_values = list(serialized_model[\"_data\"].values())\n\n        parquet_table = pa.Table.from_pylist(data_values)\n\n        filename = Path(self.cache_dir) / self.name / f\"{self.ks_id}.parquet\"\n        Path(filename).parent.mkdir(parents=True, exist_ok=True)\n        pq.write_table(parquet_table, filename)\n\n    @classmethod\n    def from_name_and_id(\n        cls, name: str, ks_id: str, cache_dir: str | None = None\n    ) -&gt; Self:\n        cache_dir = cache_dir if cache_dir else DEFAULT_CACHE_DIR\n        filename = Path(cache_dir) / name / f\"{ks_id}.parquet\"\n        if not filename.exists():\n            msg = f\"Knowledge store '{name}/{ks_id}' not found at expected location: {filename}\"\n            raise KnowledgeStoreNotFoundError(msg)\n\n        parquet_data = pq.read_table(filename).to_pylist()\n        nodes = [KnowledgeNode(**data) for data in parquet_data]\n        knowledge_store = ManagedInMemoryKnowledgeStore.from_nodes(\n            nodes, name=name, cache_dir=cache_dir\n        )\n        # set id\n        knowledge_store.ks_id = ks_id\n        return knowledge_store\n</code></pre>"},{"location":"api_reference/knowledge_stores/mixins/","title":"Mixins","text":""},{"location":"api_reference/knowledge_stores/mixins/#src.fed_rag.knowledge_stores.mixins.ManagedMixin","title":"ManagedMixin","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> Source code in <code>src/fed_rag/knowledge_stores/mixins.py</code> <pre><code>class ManagedMixin(BaseModel, ABC):\n    ks_id: str = Field(default_factory=generate_ks_id)\n\n    @classmethod\n    @abstractmethod\n    def from_name_and_id(cls, ks_id: str) -&gt; Self:\n        \"\"\"Load a managed Knowledge Store by id.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/mixins/#src.fed_rag.knowledge_stores.mixins.ManagedMixin.from_name_and_id","title":"from_name_and_id  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>from_name_and_id(ks_id)\n</code></pre> <p>Load a managed Knowledge Store by id.</p> Source code in <code>src/fed_rag/knowledge_stores/mixins.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_name_and_id(cls, ks_id: str) -&gt; Self:\n    \"\"\"Load a managed Knowledge Store by id.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/","title":"Qdrant","text":""},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore","title":"QdrantKnowledgeStore","text":"<p>               Bases: <code>BaseKnowledgeStore</code></p> <p>Qdrant Knowledge Store Class</p> <p>NOTE: This is a minimal implementation in order to just get started using Qdrant.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>class QdrantKnowledgeStore(BaseKnowledgeStore):\n    \"\"\"Qdrant Knowledge Store Class\n\n    NOTE: This is a minimal implementation in order to just get started using Qdrant.\n    \"\"\"\n\n    host: str = Field(default=\"localhost\")\n    port: int = Field(default=6333)\n    grpc_port: int = Field(default=6334)\n    https: bool = Field(default=False)\n    api_key: SecretStr | None = Field(default=None)\n    collection_name: str = Field(description=\"Name of Qdrant collection\")\n    collection_distance: Literal[\n        \"Cosine\", \"Euclid\", \"Dot\", \"Manhattan\"\n    ] = Field(\n        description=\"Distance definition for collection\", default=\"Cosine\"\n    )\n    client_kwargs: dict[str, Any] = Field(default_factory=dict)\n    timeout: int | None = Field(default=None)\n    in_memory: bool = Field(\n        default=False,\n        description=\"Specifies whether the client should refer to an in-memory service.\",\n    )\n    load_nodes_kwargs: dict[str, Any] = Field(default_factory=dict)\n    _in_memory_client: Optional[\"QdrantClient\"] = PrivateAttr(default=None)\n\n    @contextmanager\n    def get_client(\n        self,\n    ) -&gt; Generator[\"QdrantClient\", None, None]:\n        if self.in_memory:\n            if self._in_memory_client is None:\n                self._in_memory_client = _get_qdrant_client(\n                    in_memory=self.in_memory,\n                    host=self.host,\n                    port=self.port,\n                    grpc_port=self.grpc_port,\n                    https=self.https,\n                    timeout=self.timeout,\n                    api_key=self.api_key.get_secret_value()\n                    if self.api_key\n                    else None,\n                    **self.client_kwargs,\n                )\n\n            yield self._in_memory_client  # yield persistent in-memory client\n        else:\n            # create a new client connection and yield this\n            client = _get_qdrant_client(\n                host=self.host,\n                port=self.port,\n                grpc_port=self.grpc_port,\n                https=self.https,\n                timeout=self.timeout,\n                api_key=self.api_key.get_secret_value()\n                if self.api_key\n                else None,\n                **self.client_kwargs,\n            )\n\n            try:\n                yield client\n            finally:\n                try:\n                    client.close()\n                except Exception as e:\n                    warnings.warn(\n                        f\"Unable to close client: {str(e)}\",\n                        KnowledgeStoreWarning,\n                    )\n\n    def _collection_exists(self) -&gt; bool:\n        \"\"\"Check if a collection exists.\"\"\"\n        with self.get_client() as client:\n            return client.collection_exists(self.collection_name)  # type: ignore[no-any-return]\n\n    def _create_collection(\n        self, collection_name: str, vector_size: int, distance: str\n    ) -&gt; None:\n        from qdrant_client.models import Distance, VectorParams\n\n        try:\n            # Try to convert to enum\n            distance = Distance(distance)\n        except ValueError:\n            # Catch the ValueError from enum conversion and raise your custom error\n            raise InvalidDistanceError(\n                f\"Unsupported distance: {distance}. \"\n                f\"Mode must be one of: {', '.join([m.value for m in Distance])}\"\n            )\n\n        with self.get_client() as client:\n            try:\n                client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config=VectorParams(\n                        size=vector_size, distance=distance\n                    ),\n                )\n            except Exception as e:\n                raise KnowledgeStoreError(\n                    f\"Failed to create collection: {str(e)}\"\n                ) from e\n\n    def _ensure_collection_exists(self) -&gt; None:\n        if not self._collection_exists():\n            raise KnowledgeStoreNotFoundError(\n                f\"Collection '{self.collection_name}' does not exist.\"\n            )\n\n    def _check_if_collection_exists_otherwise_create_one(\n        self, vector_size: int\n    ) -&gt; None:\n        if not self._collection_exists():\n            try:\n                self._create_collection(\n                    collection_name=self.collection_name,\n                    vector_size=vector_size,\n                    distance=self.collection_distance,\n                )\n            except Exception as e:\n                raise KnowledgeStoreError(\n                    f\"Failed to create new collection: '{self.collection_name}'\"\n                ) from e\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_dependencies(cls, data: Any) -&gt; Any:\n        \"\"\"Validate that qdrant dependencies are installed.\"\"\"\n        check_qdrant_installed()\n        return data\n\n    def load_node(self, node: KnowledgeNode) -&gt; None:\n        self._check_if_collection_exists_otherwise_create_one(\n            vector_size=len(node.embedding)\n        )\n\n        point = _convert_knowledge_node_to_qdrant_point(node)\n        with self.get_client() as client:\n            try:\n                client.upsert(\n                    collection_name=self.collection_name, points=[point]\n                )\n            except Exception as e:\n                raise LoadNodeError(\n                    f\"Failed to load node {node.node_id} into collection '{self.collection_name}': {str(e)}\"\n                ) from e\n\n    def load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n        if not nodes:\n            return\n\n        self._check_if_collection_exists_otherwise_create_one(\n            vector_size=len(nodes[0].embedding)\n        )\n\n        points = [_convert_knowledge_node_to_qdrant_point(n) for n in nodes]\n        with self.get_client() as client:\n            try:\n                client.upload_points(\n                    collection_name=self.collection_name,\n                    points=points,\n                    **self.load_nodes_kwargs,\n                )\n            except Exception as e:\n                raise LoadNodeError(\n                    f\"Loading nodes into collection '{self.collection_name}' failed: {str(e)}\"\n                ) from e\n\n    def retrieve(\n        self, query_emb: list[float], top_k: int\n    ) -&gt; list[tuple[float, KnowledgeNode]]:\n        \"\"\"Retrieve top-k nodes from the vector store.\"\"\"\n        from qdrant_client.conversions.common_types import QueryResponse\n\n        self._ensure_collection_exists()\n\n        with self.get_client() as client:\n            try:\n                hits: QueryResponse = client.query_points(\n                    collection_name=self.collection_name,\n                    query=query_emb,\n                    limit=top_k,\n                )\n            except Exception as e:\n                raise KnowledgeStoreError(\n                    f\"Failed to retrieve from collection '{self.collection_name}': {str(e)}\"\n                ) from e\n\n        return [\n            _convert_scored_point_to_knowledge_node_and_score_tuple(pt)\n            for pt in hits.points\n        ]\n\n    def delete_node(self, node_id: str) -&gt; bool:\n        \"\"\"Delete a node based on its node_id.\"\"\"\n        from qdrant_client.http.models import (\n            FieldCondition,\n            Filter,\n            MatchValue,\n            UpdateResult,\n            UpdateStatus,\n        )\n\n        self._ensure_collection_exists()\n\n        with self.get_client() as client:\n            try:\n                res: UpdateResult = client.delete(\n                    collection_name=self.collection_name,\n                    points_selector=Filter(\n                        must=[\n                            FieldCondition(\n                                key=\"node_id\", match=MatchValue(value=node_id)\n                            )\n                        ]\n                    ),\n                )\n            except Exception:\n                raise KnowledgeStoreError(\n                    f\"Failed to delete node: '{node_id}' from collection '{self.collection_name}'\"\n                )\n\n        return bool(res.status == UpdateStatus.COMPLETED)\n\n    def clear(self) -&gt; None:\n        self._ensure_collection_exists()\n\n        # delete the collection\n        with self.get_client() as client:\n            try:\n                client.delete_collection(collection_name=self.collection_name)\n            except Exception as e:\n                raise KnowledgeStoreError(\n                    f\"Failed to delete collection '{self.collection_name}': {str(e)}\"\n                ) from e\n\n    @property\n    def count(self) -&gt; int:\n        from qdrant_client.http.models import CountResult\n\n        self._ensure_collection_exists()\n\n        with self.get_client() as client:\n            try:\n                res: CountResult = client.count(\n                    collection_name=self.collection_name\n                )\n                return int(res.count)\n            except Exception as e:\n                raise KnowledgeStoreError(\n                    f\"Failed to get vector count for collection '{self.collection_name}': {str(e)}\"\n                ) from e\n\n    def persist(self) -&gt; None:\n        \"\"\"Persist a knowledge store to disk.\"\"\"\n        raise NotImplementedError(\n            \"`persist()` is not available in QdrantKnowledgeStore.\"\n        )\n\n    def load(self) -&gt; None:\n        \"\"\"Load a previously persisted knowledge store.\"\"\"\n        raise NotImplementedError(\n            \"`load()` is not available in QdrantKnowledgeStore. \"\n            \"Data is automatically persisted and loaded from the Qdrant server.\"\n        )\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore.check_dependencies","title":"check_dependencies  <code>classmethod</code>","text":"<pre><code>check_dependencies(data)\n</code></pre> <p>Validate that qdrant dependencies are installed.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef check_dependencies(cls, data: Any) -&gt; Any:\n    \"\"\"Validate that qdrant dependencies are installed.\"\"\"\n    check_qdrant_installed()\n    return data\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore.retrieve","title":"retrieve","text":"<pre><code>retrieve(query_emb, top_k)\n</code></pre> <p>Retrieve top-k nodes from the vector store.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>def retrieve(\n    self, query_emb: list[float], top_k: int\n) -&gt; list[tuple[float, KnowledgeNode]]:\n    \"\"\"Retrieve top-k nodes from the vector store.\"\"\"\n    from qdrant_client.conversions.common_types import QueryResponse\n\n    self._ensure_collection_exists()\n\n    with self.get_client() as client:\n        try:\n            hits: QueryResponse = client.query_points(\n                collection_name=self.collection_name,\n                query=query_emb,\n                limit=top_k,\n            )\n        except Exception as e:\n            raise KnowledgeStoreError(\n                f\"Failed to retrieve from collection '{self.collection_name}': {str(e)}\"\n            ) from e\n\n    return [\n        _convert_scored_point_to_knowledge_node_and_score_tuple(pt)\n        for pt in hits.points\n    ]\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore.delete_node","title":"delete_node","text":"<pre><code>delete_node(node_id)\n</code></pre> <p>Delete a node based on its node_id.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>def delete_node(self, node_id: str) -&gt; bool:\n    \"\"\"Delete a node based on its node_id.\"\"\"\n    from qdrant_client.http.models import (\n        FieldCondition,\n        Filter,\n        MatchValue,\n        UpdateResult,\n        UpdateStatus,\n    )\n\n    self._ensure_collection_exists()\n\n    with self.get_client() as client:\n        try:\n            res: UpdateResult = client.delete(\n                collection_name=self.collection_name,\n                points_selector=Filter(\n                    must=[\n                        FieldCondition(\n                            key=\"node_id\", match=MatchValue(value=node_id)\n                        )\n                    ]\n                ),\n            )\n        except Exception:\n            raise KnowledgeStoreError(\n                f\"Failed to delete node: '{node_id}' from collection '{self.collection_name}'\"\n            )\n\n    return bool(res.status == UpdateStatus.COMPLETED)\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore.persist","title":"persist","text":"<pre><code>persist()\n</code></pre> <p>Persist a knowledge store to disk.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>def persist(self) -&gt; None:\n    \"\"\"Persist a knowledge store to disk.\"\"\"\n    raise NotImplementedError(\n        \"`persist()` is not available in QdrantKnowledgeStore.\"\n    )\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load a previously persisted knowledge store.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load a previously persisted knowledge store.\"\"\"\n    raise NotImplementedError(\n        \"`load()` is not available in QdrantKnowledgeStore. \"\n        \"Data is automatically persisted and loaded from the Qdrant server.\"\n    )\n</code></pre>"},{"location":"api_reference/loss/pytorch/","title":"Pytorch","text":"<p>LM-Supervised Retriever Loss.</p>"},{"location":"api_reference/loss/pytorch/#src.fed_rag.loss.pytorch.lsr.ReductionMode","title":"ReductionMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Reduction mode enum.</p> Source code in <code>src/fed_rag/loss/pytorch/lsr.py</code> <pre><code>class ReductionMode(str, Enum):\n    \"\"\"Reduction mode enum.\"\"\"\n\n    MEAN = \"mean\"\n    SUM = \"sum\"\n\n    @classmethod\n    def members_list(cls) -&gt; list[str]:\n        return [member for member in cls]\n</code></pre>"},{"location":"api_reference/loss/pytorch/#src.fed_rag.loss.pytorch.lsr.LSRLoss","title":"LSRLoss","text":"<p>               Bases: <code>Module</code></p> <p>PyTorch implementation of the LM-Supervised Retriever Loss.</p> <p>Given input context x and ground truth continuation y, computes KL divergence between retrieval likelihood P_R(d|x) and language model likelihood Q_LM(d|x,y), where d is the retrieved document.</p> Shi, Weijia, et al. \"Replug: Retrieval-augmented black-box language models.\" <p>arXiv preprint arXiv:2301.12652 (2023).</p> <p>Arxiv: https://arxiv.org/pdf/2301.12652</p> Source code in <code>src/fed_rag/loss/pytorch/lsr.py</code> <pre><code>class LSRLoss(nn.Module):\n    \"\"\"PyTorch implementation of the LM-Supervised Retriever Loss.\n\n    Given input context x and ground truth continuation y, computes KL divergence\n    between retrieval likelihood P_R(d|x) and language model likelihood Q_LM(d|x,y),\n    where d is the retrieved document.\n\n    Source: Shi, Weijia, et al. \"Replug: Retrieval-augmented black-box language models.\"\n        arXiv preprint arXiv:2301.12652 (2023).\n    Arxiv: https://arxiv.org/pdf/2301.12652\n    \"\"\"\n\n    def __init__(self, reduction: ReductionMode = ReductionMode.MEAN):\n        # This line is critical - it initializes all the Module machinery\n        super(LSRLoss, self).__init__()\n\n        if reduction not in ReductionMode.members_list():\n            msg = (\n                f\"Invalid reduction {reduction}. \"\n                f\"Valid reductions are: {', '.join(ReductionMode.members_list())}\"\n            )\n            raise InvalidReductionParam(msg)\n\n        self.reduction = reduction\n\n    def forward(\n        self, retrieval_scores: torch.Tensor, lm_scores: torch.Tensor\n    ) -&gt; torch.Tensor:\n        retrieval_log_probs = F.log_softmax(retrieval_scores, dim=1)\n        lm_probs = F.softmax(lm_scores, dim=1)\n        kl_div = F.kl_div(retrieval_log_probs, lm_probs, reduction=\"none\").sum(\n            dim=-1\n        )\n\n        match self.reduction:\n            case ReductionMode.MEAN:\n                return kl_div.mean()\n            case ReductionMode.SUM:\n                return kl_div.sum()\n            case _:  # pragma: no cover\n                assert_never(self.reduction)  # pragma: no cover\n</code></pre>"},{"location":"api_reference/rag_system/","title":"RAG System and Auxiliary Types","text":"<p>RAG System Module</p> <p>Auxiliary types for RAG System</p>"},{"location":"api_reference/rag_system/#src.fed_rag.core.rag_system.RAGSystem","title":"RAGSystem","text":"<p>               Bases: <code>LlamaIndexBridgeMixin</code>, <code>_RAGSystem</code></p> <p>RAG System with all available bridge functionality.</p> <p>The RAGSystem is the main entry point for creating and managing retrieval-augmented generation systems.</p> Source code in <code>src/fed_rag/core/rag_system.py</code> <pre><code>class RAGSystem(LlamaIndexBridgeMixin, _RAGSystem):\n    \"\"\"RAG System with all available bridge functionality.\n\n    The RAGSystem is the main entry point for creating and managing\n    retrieval-augmented generation systems.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.data_structures.rag.RAGConfig","title":"RAGConfig","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/data_structures/rag.py</code> <pre><code>class RAGConfig(BaseModel):\n    top_k: int\n    context_separator: str = \"\\n\"\n</code></pre>"},{"location":"api_reference/retrievers/","title":"Retrievers","text":"<p>Base Retriever</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever","title":"BaseRetriever","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Retriever Class.</p> Source code in <code>src/fed_rag/base/retriever.py</code> <pre><code>class BaseRetriever(BaseModel, ABC):\n    \"\"\"Base Retriever Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def encode_query(\n        self, query: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode query.\"\"\"\n\n    @abstractmethod\n    def encode_context(\n        self, context: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode context.\"\"\"\n\n    @property\n    @abstractmethod\n    def encoder(self) -&gt; torch.nn.Module | None:\n        \"\"\"PyTorch model associated with the encoder associated with retriever.\"\"\"\n\n    @property\n    @abstractmethod\n    def query_encoder(self) -&gt; torch.nn.Module | None:\n        \"\"\"PyTorch model associated with the query encoder associated with retriever.\"\"\"\n\n    @property\n    @abstractmethod\n    def context_encoder(self) -&gt; torch.nn.Module | None:\n        \"\"\"PyTorch model associated with the context encoder associated with retriever.\"\"\"\n</code></pre>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.encoder","title":"encoder  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>encoder\n</code></pre> <p>PyTorch model associated with the encoder associated with retriever.</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.query_encoder","title":"query_encoder  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>query_encoder\n</code></pre> <p>PyTorch model associated with the query encoder associated with retriever.</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.context_encoder","title":"context_encoder  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>context_encoder\n</code></pre> <p>PyTorch model associated with the context encoder associated with retriever.</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.encode_query","title":"encode_query  <code>abstractmethod</code>","text":"<pre><code>encode_query(query, **kwargs)\n</code></pre> <p>Encode query.</p> Source code in <code>src/fed_rag/base/retriever.py</code> <pre><code>@abstractmethod\ndef encode_query(\n    self, query: str | list[str], **kwargs: Any\n) -&gt; torch.Tensor:\n    \"\"\"Encode query.\"\"\"\n</code></pre>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.encode_context","title":"encode_context  <code>abstractmethod</code>","text":"<pre><code>encode_context(context, **kwargs)\n</code></pre> <p>Encode context.</p> Source code in <code>src/fed_rag/base/retriever.py</code> <pre><code>@abstractmethod\ndef encode_context(\n    self, context: str | list[str], **kwargs: Any\n) -&gt; torch.Tensor:\n    \"\"\"Encode context.\"\"\"\n</code></pre>"},{"location":"api_reference/retrievers/huggingface/","title":"Huggingface","text":"<p>HuggingFace SentenceTransformer Retriever</p>"},{"location":"api_reference/retrievers/huggingface/#src.fed_rag.retrievers.huggingface.hf_sentence_transformer.HFSentenceTransformerRetriever","title":"HFSentenceTransformerRetriever","text":"<p>               Bases: <code>BaseRetriever</code></p> Source code in <code>src/fed_rag/retrievers/huggingface/hf_sentence_transformer.py</code> <pre><code>class HFSentenceTransformerRetriever(BaseRetriever):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str | None = Field(\n        description=\"Name of HuggingFace SentenceTransformer model.\",\n        default=None,\n    )\n    query_model_name: str | None = Field(\n        description=\"Name of HuggingFace SentenceTransformer model used for encoding queries.\",\n        default=None,\n    )\n    context_model_name: str | None = Field(\n        description=\"Name of HuggingFace SentenceTransformer model used for encoding context.\",\n        default=None,\n    )\n    load_model_kwargs: LoadKwargs = Field(\n        description=\"Optional kwargs dict for loading models from HF. Defaults to None.\",\n        default_factory=LoadKwargs,\n    )\n    _encoder: Optional[\"SentenceTransformer\"] = PrivateAttr(default=None)\n    _query_encoder: Optional[\"SentenceTransformer\"] = PrivateAttr(default=None)\n    _context_encoder: Optional[\"SentenceTransformer\"] = PrivateAttr(\n        default=None\n    )\n\n    def __init__(\n        self,\n        model_name: str | None = None,\n        query_model_name: str | None = None,\n        context_model_name: str | None = None,\n        load_model_kwargs: LoadKwargs | dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        if isinstance(load_model_kwargs, dict):\n            # use same dict for all\n            load_model_kwargs = LoadKwargs(\n                encoder=load_model_kwargs,\n                query_encoder=load_model_kwargs,\n                context_encoder=load_model_kwargs,\n            )\n\n        load_model_kwargs = (\n            load_model_kwargs if load_model_kwargs else LoadKwargs()\n        )\n\n        super().__init__(\n            model_name=model_name,\n            query_model_name=query_model_name,\n            context_model_name=context_model_name,\n            load_model_kwargs=load_model_kwargs,\n        )\n        if load_model_at_init:\n            if model_name:\n                self._encoder = self._load_model_from_hf(load_type=\"encoder\")\n            else:\n                self._query_encoder = self._load_model_from_hf(\n                    load_type=\"query_encoder\"\n                )\n                self._context_encoder = self._load_model_from_hf(\n                    load_type=\"context_encoder\"\n                )\n\n    def _load_model_from_hf(\n        self,\n        load_type: Literal[\"encoder\", \"query_encoder\", \"context_encoder\"],\n        **kwargs: Any,\n    ) -&gt; \"SentenceTransformer\":\n        if load_type == \"encoder\":\n            load_kwargs = self.load_model_kwargs.encoder\n            load_kwargs.update(kwargs)\n            return SentenceTransformer(self.model_name, **load_kwargs)\n        elif load_type == \"context_encoder\":\n            load_kwargs = self.load_model_kwargs.context_encoder\n            load_kwargs.update(kwargs)\n            return SentenceTransformer(self.context_model_name, **load_kwargs)\n        elif load_type == \"query_encoder\":\n            load_kwargs = self.load_model_kwargs.query_encoder\n            load_kwargs.update(kwargs)\n            return SentenceTransformer(self.query_model_name, **load_kwargs)\n        else:\n            raise InvalidLoadType(\"Invalid `load_type` supplied.\")\n\n    def encode_context(\n        self, context: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        # validation guarantees one of these is not None\n        encoder = self.encoder if self.encoder else self.context_encoder\n        encoder = cast(SentenceTransformer, encoder)\n\n        return encoder.encode(context)\n\n    def encode_query(\n        self, query: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        # validation guarantees one of these is not None\n        encoder = self.encoder if self.encoder else self.query_encoder\n        encoder = cast(SentenceTransformer, encoder)\n\n        return encoder.encode(query)\n\n    @property\n    def encoder(self) -&gt; Optional[\"SentenceTransformer\"]:\n        if self.model_name and self._encoder is None:\n            self._encoder = self._load_model_from_hf(load_type=\"encoder\")\n        return self._encoder\n\n    @property\n    def query_encoder(self) -&gt; Optional[\"SentenceTransformer\"]:\n        if self.query_model_name and self._query_encoder is None:\n            self._query_encoder = self._load_model_from_hf(\n                load_type=\"query_encoder\"\n            )\n        return self._query_encoder\n\n    @property\n    def context_encoder(self) -&gt; Optional[\"SentenceTransformer\"]:\n        if self.context_model_name and self._context_encoder is None:\n            self._context_encoder = self._load_model_from_hf(\n                load_type=\"context_encoder\"\n            )\n        return self._context_encoder\n</code></pre>"},{"location":"api_reference/tokenizers/","title":"Tokenizers","text":"<p>Base Tokenizer</p>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer","title":"BaseTokenizer","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Tokenizer Class.</p> Source code in <code>src/fed_rag/base/tokenizer.py</code> <pre><code>class BaseTokenizer(BaseModel, ABC):\n    \"\"\"Base Tokenizer Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def encode(self, input: str, **kwargs: dict) -&gt; EncodeResult:\n        \"\"\"Encode the input string into list of integers.\"\"\"\n\n    @abstractmethod\n    def decode(self, input_ids: str, **kwargs: dict) -&gt; str:\n        \"\"\"Decode the input token ids into a string.\"\"\"\n\n    @property\n    @abstractmethod\n    def unwrapped(self) -&gt; Any:\n        \"\"\"Return the underlying tokenizer if there is one.\"\"\"\n</code></pre>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer.unwrapped","title":"unwrapped  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>unwrapped\n</code></pre> <p>Return the underlying tokenizer if there is one.</p>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer.encode","title":"encode  <code>abstractmethod</code>","text":"<pre><code>encode(input, **kwargs)\n</code></pre> <p>Encode the input string into list of integers.</p> Source code in <code>src/fed_rag/base/tokenizer.py</code> <pre><code>@abstractmethod\ndef encode(self, input: str, **kwargs: dict) -&gt; EncodeResult:\n    \"\"\"Encode the input string into list of integers.\"\"\"\n</code></pre>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer.decode","title":"decode  <code>abstractmethod</code>","text":"<pre><code>decode(input_ids, **kwargs)\n</code></pre> <p>Decode the input token ids into a string.</p> Source code in <code>src/fed_rag/base/tokenizer.py</code> <pre><code>@abstractmethod\ndef decode(self, input_ids: str, **kwargs: dict) -&gt; str:\n    \"\"\"Decode the input token ids into a string.\"\"\"\n</code></pre>"},{"location":"api_reference/tokenizers/huggingface/","title":"Huggingface","text":"<p>HuggingFace PretrainedTokenizer</p>"},{"location":"api_reference/tokenizers/huggingface/#src.fed_rag.tokenizers.hf_pretrained_tokenizer.HFPretrainedTokenizer","title":"HFPretrainedTokenizer","text":"<p>               Bases: <code>BaseTokenizer</code></p> Source code in <code>src/fed_rag/tokenizers/hf_pretrained_tokenizer.py</code> <pre><code>class HFPretrainedTokenizer(BaseTokenizer):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str = Field(\n        description=\"Name of HuggingFace model. Used for loading the model from HF hub or local.\"\n    )\n    load_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading models from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    _tokenizer: Optional[\"PreTrainedTokenizer\"] = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str,\n        load_model_kwargs: dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n        super().__init__(\n            model_name=model_name,\n            load_model_kwargs=load_model_kwargs if load_model_kwargs else {},\n        )\n        if load_model_at_init:\n            self._tokenizer = self._load_model_from_hf()\n\n    def _load_model_from_hf(self, **kwargs: Any) -&gt; \"PreTrainedTokenizer\":\n        load_kwargs = self.load_model_kwargs\n        load_kwargs.update(kwargs)\n        self.load_model_kwargs = load_kwargs\n        return AutoTokenizer.from_pretrained(self.model_name, **load_kwargs)\n\n    @property\n    def unwrapped(self) -&gt; \"PreTrainedTokenizer\":\n        if self._tokenizer is None:\n            # load HF Pretrained Tokenizer\n            tokenizer = self._load_model_from_hf()\n            self._tokenizer = tokenizer\n        return self._tokenizer\n\n    @unwrapped.setter\n    def unwrapped(self, value: \"PreTrainedTokenizer\") -&gt; None:\n        self._tokenizer = value\n\n    def encode(self, input: str, **kwargs: Any) -&gt; EncodeResult:\n        tokenizer_result = self.unwrapped(text=input, **kwargs)\n        input_ids = tokenizer_result.get(\"input_ids\")\n        attention_mask = tokenizer_result.get(\"attention_mask\", None)\n\n        if not input_ids:\n            raise TokenizerError(\"Tokenizer returned empty input_ids\")\n\n        # maybe flatten\n        if isinstance(input_ids[0], list):\n            if len(input_ids) == 1:\n                input_ids = input_ids[0]\n                if attention_mask is not None:\n                    attention_mask = attention_mask[0]\n            else:\n                raise TokenizerError(\n                    \"Unexpected shape of `input_ids` from `tokenizer.__call__`.\"\n                )\n\n        retval: EncodeResult = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n        }\n        return retval\n\n    def decode(self, input_ids: list[int], **kwargs: Any) -&gt; str:\n        return self.unwrapped.decode(token_ids=input_ids, **kwargs)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_reference/trainer_managers/","title":"Base RAG Trainer Manager","text":"<p>Base RAG Trainer Manager</p>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager","title":"BaseRAGTrainerManager","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base RAG Trainer Class.</p> <p>The manager becomes solely responsible for orchestration, not for maintaining state (i.e., the RAGSystem).</p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>class BaseRAGTrainerManager(BaseModel, ABC):\n    \"\"\"Base RAG Trainer Class.\n\n    The manager becomes solely responsible for orchestration, not for maintaining state\n    (i.e., the RAGSystem).\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    mode: RAGTrainMode\n    retriever_trainer: BaseRetrieverTrainer | None = None\n    generator_trainer: BaseGeneratorTrainer | None = None\n\n    @field_validator(\"mode\", mode=\"before\")\n    @classmethod\n    def validate_mode(cls, v: str) -&gt; str:\n        try:\n            # Try to convert to enum\n            mode = RAGTrainMode(v)\n            return mode\n        except ValueError:\n            # Catch the ValueError from enum conversion and raise your custom error\n            raise UnsupportedTrainerMode(\n                f\"Unsupported RAG train mode: {v}. \"\n                f\"Mode must be one of: {', '.join([m.value for m in RAGTrainMode])}\"\n            )\n\n    # Validate trainer presence\n    @model_validator(mode=\"after\")\n    def validate_trainers(self) -&gt; \"BaseRAGTrainerManager\":\n        \"\"\"Validate trainer requirements.\"\"\"\n        # Validate trainer presence based on mode\n        if (\n            self.mode == RAGTrainMode.RETRIEVER\n            and self.retriever_trainer is None\n        ):\n            raise UnspecifiedRetrieverTrainer(\n                \"Retriever trainer must be set when in retriever mode\"\n            )\n        if (\n            self.mode == RAGTrainMode.GENERATOR\n            and self.generator_trainer is None\n        ):\n            raise UnspecifiedGeneratorTrainer(\n                \"Generator trainer must be set when in generator mode\"\n            )\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_trainers_consistency(self) -&gt; \"BaseRAGTrainerManager\":\n        \"\"\"Validate that trainers use consistent RAG systems if both are present.\"\"\"\n        if (\n            self.retriever_trainer is not None\n            and self.generator_trainer is not None\n        ):\n            # Check if both trainers have the same RAG system reference\n            if id(self.retriever_trainer.rag_system) != id(\n                self.generator_trainer.rag_system\n            ):\n                raise InconsistentRAGSystems(\n                    \"Inconsistent RAG systems detected between retriever and generator trainers. \"\n                    \"Both trainers must use the same RAG system instance for consistent training.\"\n                )\n\n        return self\n\n    @abstractmethod\n    def _prepare_retriever_for_training(\n        self, freeze_context_encoder: bool = True, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Prepare retriever model for training.\"\"\"\n\n    @abstractmethod\n    def _prepare_generator_for_training(self, **kwargs: Any) -&gt; None:\n        \"\"\"Prepare generator model for training.\"\"\"\n\n    @abstractmethod\n    def _train_retriever(self, **kwargs: Any) -&gt; Any:\n        \"\"\"Train loop for retriever.\"\"\"\n\n    @abstractmethod\n    def _train_generator(self, **kwargs: Any) -&gt; Any:\n        \"\"\"Train loop for generator.\"\"\"\n\n    @abstractmethod\n    def train(self, **kwargs: Any) -&gt; Any:\n        \"\"\"Train loop for rag system.\"\"\"\n\n    @abstractmethod\n    def get_federated_task(self) -&gt; BaseFLTask:\n        \"\"\"Get the federated task.\"\"\"\n\n    @property\n    def model(self) -&gt; Any:\n        \"\"\"Return the model to be trained.\"\"\"\n        match self.mode:\n            case RAGTrainMode.RETRIEVER:\n                trainer = cast(BaseRetrieverTrainer, self.retriever_trainer)\n            case RAGTrainMode.GENERATOR:\n                trainer = cast(BaseGeneratorTrainer, self.generator_trainer)\n            case _:  # pragma: no cover\n                assert_never(self.mode)  # pragma: no cover\n        return trainer.model\n</code></pre>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager.model","title":"model  <code>property</code>","text":"<pre><code>model\n</code></pre> <p>Return the model to be trained.</p>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager.validate_trainers","title":"validate_trainers","text":"<pre><code>validate_trainers()\n</code></pre> <p>Validate trainer requirements.</p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_trainers(self) -&gt; \"BaseRAGTrainerManager\":\n    \"\"\"Validate trainer requirements.\"\"\"\n    # Validate trainer presence based on mode\n    if (\n        self.mode == RAGTrainMode.RETRIEVER\n        and self.retriever_trainer is None\n    ):\n        raise UnspecifiedRetrieverTrainer(\n            \"Retriever trainer must be set when in retriever mode\"\n        )\n    if (\n        self.mode == RAGTrainMode.GENERATOR\n        and self.generator_trainer is None\n    ):\n        raise UnspecifiedGeneratorTrainer(\n            \"Generator trainer must be set when in generator mode\"\n        )\n\n    return self\n</code></pre>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager.validate_trainers_consistency","title":"validate_trainers_consistency","text":"<pre><code>validate_trainers_consistency()\n</code></pre> <p>Validate that trainers use consistent RAG systems if both are present.</p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_trainers_consistency(self) -&gt; \"BaseRAGTrainerManager\":\n    \"\"\"Validate that trainers use consistent RAG systems if both are present.\"\"\"\n    if (\n        self.retriever_trainer is not None\n        and self.generator_trainer is not None\n    ):\n        # Check if both trainers have the same RAG system reference\n        if id(self.retriever_trainer.rag_system) != id(\n            self.generator_trainer.rag_system\n        ):\n            raise InconsistentRAGSystems(\n                \"Inconsistent RAG systems detected between retriever and generator trainers. \"\n                \"Both trainers must use the same RAG system instance for consistent training.\"\n            )\n\n    return self\n</code></pre>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager.train","title":"train  <code>abstractmethod</code>","text":"<pre><code>train(**kwargs)\n</code></pre> <p>Train loop for rag system.</p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>@abstractmethod\ndef train(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Train loop for rag system.\"\"\"\n</code></pre>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager.get_federated_task","title":"get_federated_task  <code>abstractmethod</code>","text":"<pre><code>get_federated_task()\n</code></pre> <p>Get the federated task.</p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>@abstractmethod\ndef get_federated_task(self) -&gt; BaseFLTask:\n    \"\"\"Get the federated task.\"\"\"\n</code></pre>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.RAGTrainMode","title":"RAGTrainMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>class RAGTrainMode(str, Enum):\n    RETRIEVER = \"retriever\"\n    GENERATOR = \"generator\"\n</code></pre>"},{"location":"api_reference/trainer_managers/huggingface/","title":"Huggingface","text":"<p>HuggingFace RAG Trainer</p>"},{"location":"api_reference/trainer_managers/huggingface/#src.fed_rag.trainer_managers.huggingface.HuggingFaceRAGTrainerManager","title":"HuggingFaceRAGTrainerManager","text":"<p>               Bases: <code>BaseRAGTrainerManager</code></p> <p>HuggingFace RAG Trainer Manager</p> Source code in <code>src/fed_rag/trainer_managers/huggingface.py</code> <pre><code>class HuggingFaceRAGTrainerManager(BaseRAGTrainerManager):\n    \"\"\"HuggingFace RAG Trainer Manager\"\"\"\n\n    def __init__(\n        self,\n        mode: RAGTrainMode,\n        retriever_trainer: BaseTrainer | None = None,\n        generator_trainer: BaseTrainer | None = None,\n        **kwargs: Any,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n        super().__init__(\n            mode=mode,\n            retriever_trainer=retriever_trainer,\n            generator_trainer=generator_trainer,\n            **kwargs,\n        )\n\n    def _prepare_generator_for_training(self, **kwargs: Any) -&gt; None:\n        self.generator_trainer.model.train()\n\n        # freeze generator\n        if self.retriever_trainer:\n            self.retriever_trainer.model.eval()\n\n    def _prepare_retriever_for_training(\n        self, freeze_context_encoder: bool = True, **kwargs: Any\n    ) -&gt; None:\n        self.retriever_trainer.model.train()\n\n        # freeze generator\n        if self.generator_trainer:\n            self.generator_trainer.model.eval()\n\n    def _train_retriever(self, **kwargs: Any) -&gt; TrainResult:\n        if self.retriever_trainer:\n            self._prepare_retriever_for_training()\n            return self.retriever_trainer.train(**kwargs)\n        else:\n            raise UnspecifiedRetrieverTrainer(\n                \"Attempted to perform retriever trainer with an unspecified trainer.\"\n            )\n\n    def _train_generator(self, **kwargs: Any) -&gt; TrainResult:\n        if self.generator_trainer:\n            self._prepare_generator_for_training()\n            return self.generator_trainer.train(**kwargs)\n        else:\n            raise UnspecifiedGeneratorTrainer(\n                \"Attempted to perform generator trainer with an unspecified trainer.\"\n            )\n\n    def train(self, **kwargs: Any) -&gt; TrainResult:\n        if self.mode == \"retriever\":\n            return self._train_retriever(**kwargs)\n        elif self.mode == \"generator\":\n            return self._train_generator(**kwargs)\n        else:\n            assert_never(self.mode)  # pragma: no cover\n\n    def _get_federated_trainer(self) -&gt; tuple[Callable, \"HFModelType\"]:\n        if self.mode == \"retriever\":\n            if self.retriever_trainer is None:\n                raise UnspecifiedRetrieverTrainer(\n                    \"Cannot federate an unspecified retriever trainer.\"\n                )\n            retriever_train_fn = self.retriever_trainer.train\n            retriever_module = self.retriever_trainer.model\n            retriever_module = cast(\"SentenceTransformer\", retriever_module)\n\n            # Create a standalone function for federation\n            def train_wrapper(\n                model: \"HFModelType\",\n                train_dataset: \"Dataset\",\n                val_dataset: \"Dataset\",\n            ) -&gt; TrainResult:\n                _ = retriever_train_fn()\n                return TrainResult(loss=0)\n\n            return (\n                federate.trainer.huggingface(train_wrapper),\n                retriever_module,\n            )\n\n        elif self.mode == \"generator\":\n            if self.generator_trainer is None:\n                raise UnspecifiedGeneratorTrainer(\n                    \"Cannot federate an unspecified generator trainer.\"\n                )\n            generator_train_fn = self.generator_trainer.train\n            generator_module = self.generator_trainer.model\n\n            # Create a standalone function for federation\n            def train_wrapper(\n                model: \"HFModelType\",  # TODO: handle union types in inspector\n                train_dataset: \"Dataset\",\n                val_dataset: \"Dataset\",\n            ) -&gt; TrainResult:\n                _ = generator_train_fn()\n                # TODO get loss from out\n                return TrainResult(loss=0)\n\n            return (\n                federate.trainer.huggingface(train_wrapper),\n                generator_module,\n            )\n        else:\n            assert_never(self.mode)  # pragma: no cover\n\n    def get_federated_task(self) -&gt; \"HuggingFaceFLTask\":\n        from fed_rag.fl_tasks.huggingface import HuggingFaceFLTask\n\n        federated_trainer, _module = self._get_federated_trainer()\n\n        # TODO: add logic for getting evaluator/tester and then federate it as well\n        # federated_tester = self.get_federated_tester(tester_decorator)\n        # For now, using a simple placeholder test function\n        def test_fn(\n            model: \"HFModelType\", eval_dataset: \"Dataset\"\n        ) -&gt; TestResult:\n            # Implement simple testing or return a placeholder\n            return TestResult(loss=0.42, metrics={})  # pragma: no cover\n\n        federated_tester = federate.tester.huggingface(test_fn)\n\n        return HuggingFaceFLTask.from_trainer_and_tester(\n            trainer=federated_trainer,\n            tester=federated_tester,\n        )\n</code></pre>"},{"location":"api_reference/trainer_managers/pytorch/","title":"Pytorch","text":"<p>PyTorch RAG Trainer</p>"},{"location":"api_reference/trainer_managers/pytorch/#src.fed_rag.trainer_managers.pytorch.PyTorchRAGTrainerManager","title":"PyTorchRAGTrainerManager","text":"<p>               Bases: <code>BaseRAGTrainerManager</code></p> <p>PyTorch native RAG Trainer Manager</p> Source code in <code>src/fed_rag/trainer_managers/pytorch.py</code> <pre><code>class PyTorchRAGTrainerManager(BaseRAGTrainerManager):\n    \"\"\"PyTorch native RAG Trainer Manager\"\"\"\n\n    def _prepare_generator_for_training(self, **kwargs: Any) -&gt; None:\n        self.generator_trainer.model.train()\n\n        # freeze retriever\n        if self.retriever_trainer:\n            self.retriever_trainer.model.eval()\n\n    def _prepare_retriever_for_training(\n        self, freeze_context_encoder: bool = True, **kwargs: Any\n    ) -&gt; None:\n        self.retriever_trainer.model.train()\n\n        # freeze generator\n        if self.generator_trainer:\n            self.generator_trainer.model.eval()\n\n    def _train_retriever(self, **kwargs: Any) -&gt; None:\n        if self.retriever_trainer:\n            self._prepare_retriever_for_training()\n            self.retriever_trainer.train()\n        else:\n            raise UnspecifiedRetrieverTrainer(\n                \"Attempted to perform retriever trainer with an unspecified trainer.\"\n            )\n\n    def _train_generator(self, **kwargs: Any) -&gt; None:\n        if self.generator_trainer:\n            self._prepare_generator_for_training()\n            self.generator_trainer.train()\n        else:\n            raise UnspecifiedGeneratorTrainer(\n                \"Attempted to perform generator trainer with an unspecified trainer.\"\n            )\n\n    def train(self, **kwargs: Any) -&gt; None:\n        if self.mode == \"retriever\":\n            self._train_retriever()\n        elif self.mode == \"generator\":\n            self._train_generator()\n        else:\n            assert_never(self.mode)  # pragma: no cover\n\n    def _get_federated_trainer(self) -&gt; tuple[Callable, nn.Module]:\n        if self.mode == \"retriever\":\n            if self.retriever_trainer is None:\n                raise UnspecifiedRetrieverTrainer(\n                    \"Cannot federate an unspecified retriever trainer.\"\n                )\n            retriever_train_fn = self.retriever_trainer.train\n            retriever_module = self.retriever_trainer.model\n\n            # Create a standalone function for federation\n            def train_wrapper(\n                _mdl: nn.Module,\n                _train_dataloader: DataLoader,\n                _val_dataloader: DataLoader,\n            ) -&gt; TrainResult:\n                _ = retriever_train_fn()\n                return TrainResult(loss=0)\n\n            return federate.trainer.pytorch(train_wrapper), retriever_module\n\n        elif self.mode == \"generator\":\n            if self.generator_trainer is None:\n                raise UnspecifiedGeneratorTrainer(\n                    \"Cannot federate an unspecified generator trainer.\"\n                )\n            generator_train_fn = self.generator_trainer.train\n            generator_module = self.generator_trainer.model\n\n            # Create a standalone function for federation\n            def train_wrapper(\n                _mdl: nn.Module,\n                _train_dataloader: DataLoader,\n                _val_dataloader: DataLoader,\n            ) -&gt; TrainResult:\n                _ = generator_train_fn()\n                # TODO get loss from out\n                return TrainResult(loss=0)\n\n            return federate.trainer.pytorch(train_wrapper), generator_module\n        else:\n            assert_never(self.mode)  # pragma: no cover\n\n    def get_federated_task(self) -&gt; PyTorchFLTask:\n        federated_trainer, _module = self._get_federated_trainer()\n\n        # TODO: add logic for getting evaluator/tester and then federate it as well\n        # federated_tester = self.get_federated_tester(tester_decorator)\n        # For now, using a simple placeholder test function\n        def test_fn(_mdl: nn.Module, _dataloader: DataLoader) -&gt; TestResult:\n            # Implement simple testing or return a placeholder\n            return TestResult(loss=0.42, metrics={})  # pragma: no cover\n\n        federated_tester = federate.tester.pytorch(test_fn)\n\n        return PyTorchFLTask.from_trainer_and_tester(\n            trainer=federated_trainer,\n            tester=federated_tester,\n        )\n</code></pre>"},{"location":"api_reference/trainers/","title":"Base Trainer","text":"<p>Base Trainer</p>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseTrainer","title":"BaseTrainer","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Trainer Class.</p> Source code in <code>src/fed_rag/base/trainer.py</code> <pre><code>class BaseTrainer(BaseModel, ABC):\n    \"\"\"Base Trainer Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    rag_system: RAGSystem\n    train_dataset: Any\n    _model = PrivateAttr()\n\n    @abstractmethod\n    def train(self) -&gt; TrainResult:\n        \"\"\"Train loop.\"\"\"\n\n    @abstractmethod\n    def evaluate(self) -&gt; TestResult:\n        \"\"\"Evaluation\"\"\"\n\n    @abstractmethod\n    def _get_model_from_rag_system(self) -&gt; Any:\n        \"\"\"Get the model from the RAG system.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def set_model(self) -&gt; \"BaseTrainer\":\n        self._model = self._get_model_from_rag_system()\n        return self\n\n    @property\n    def model(self) -&gt; Any:\n        \"\"\"Return the model to be trained.\"\"\"\n        return self._model\n\n    @model.setter\n    def model(self, v: Any) -&gt; None:\n        \"\"\"Set the model to be trained.\"\"\"\n        self._model = v\n</code></pre>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseTrainer.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model\n</code></pre> <p>Return the model to be trained.</p>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseTrainer.train","title":"train  <code>abstractmethod</code>","text":"<pre><code>train()\n</code></pre> <p>Train loop.</p> Source code in <code>src/fed_rag/base/trainer.py</code> <pre><code>@abstractmethod\ndef train(self) -&gt; TrainResult:\n    \"\"\"Train loop.\"\"\"\n</code></pre>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseTrainer.evaluate","title":"evaluate  <code>abstractmethod</code>","text":"<pre><code>evaluate()\n</code></pre> <p>Evaluation</p> Source code in <code>src/fed_rag/base/trainer.py</code> <pre><code>@abstractmethod\ndef evaluate(self) -&gt; TestResult:\n    \"\"\"Evaluation\"\"\"\n</code></pre>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseRetrieverTrainer","title":"BaseRetrieverTrainer","text":"<p>               Bases: <code>BaseTrainer</code>, <code>ABC</code></p> <p>Base Retriever Trainer Class.</p> Source code in <code>src/fed_rag/base/trainer.py</code> <pre><code>class BaseRetrieverTrainer(BaseTrainer, ABC):\n    \"\"\"Base Retriever Trainer Class.\"\"\"\n\n    def _get_model_from_rag_system(self) -&gt; Any:\n        if self.rag_system.retriever.encoder:\n            return self.rag_system.retriever.encoder\n        else:\n            return (\n                self.rag_system.retriever.query_encoder\n            )  # only update query encoder\n</code></pre>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseGeneratorTrainer","title":"BaseGeneratorTrainer","text":"<p>               Bases: <code>BaseTrainer</code>, <code>ABC</code></p> <p>Base Retriever Trainer Class.</p> Source code in <code>src/fed_rag/base/trainer.py</code> <pre><code>class BaseGeneratorTrainer(BaseTrainer, ABC):\n    \"\"\"Base Retriever Trainer Class.\"\"\"\n\n    def _get_model_from_rag_system(self) -&gt; Any:\n        return self.rag_system.generator.model\n</code></pre>"},{"location":"api_reference/trainers/huggingface/","title":"Huggingface","text":"<p>HuggingFace Trainer Mixin</p> <p>HuggingFace LM-Supervised Retriever Trainer</p> <p>HuggingFace Retrieval-Augmented Generator Trainer</p>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.mixin.HuggingFaceTrainerProtocol","title":"HuggingFaceTrainerProtocol","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/fed_rag/trainers/huggingface/mixin.py</code> <pre><code>@runtime_checkable\nclass HuggingFaceTrainerProtocol(Protocol):\n    train_dataset: \"Dataset\"\n    training_arguments: Optional[\"TrainingArguments\"]\n\n    def model(\n        self,\n    ) -&gt; Union[\"SentenceTransformer\", \"PreTrainedModel\", \"PeftModel\"]:\n        pass  # pragma: no cover\n</code></pre>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.mixin.HuggingFaceTrainerMixin","title":"HuggingFaceTrainerMixin","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>HuggingFace Trainer Mixin.</p> Source code in <code>src/fed_rag/trainers/huggingface/mixin.py</code> <pre><code>class HuggingFaceTrainerMixin(BaseModel, ABC):\n    \"\"\"HuggingFace Trainer Mixin.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n    train_dataset: \"Dataset\"\n    training_arguments: Optional[\"TrainingArguments\"] = None\n\n    def __init__(\n        self,\n        train_dataset: \"Dataset\",\n        training_arguments: Optional[\"TrainingArguments\"] = None,\n        **kwargs: Any,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n        super().__init__(\n            train_dataset=train_dataset,\n            training_arguments=training_arguments,\n            **kwargs,\n        )\n\n    @property\n    @abstractmethod\n    def hf_trainer_obj(self) -&gt; \"Trainer\":\n        \"\"\"A ~transformers.Trainer object.\"\"\"\n</code></pre>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.mixin.HuggingFaceTrainerMixin.hf_trainer_obj","title":"hf_trainer_obj  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>hf_trainer_obj\n</code></pre> <p>A ~transformers.Trainer object.</p>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.lsr.HuggingFaceTrainerForLSR","title":"HuggingFaceTrainerForLSR","text":"<p>               Bases: <code>HuggingFaceTrainerMixin</code>, <code>BaseRetrieverTrainer</code></p> <p>HuggingFace LM-Supervised Retriever Trainer.</p> Source code in <code>src/fed_rag/trainers/huggingface/lsr.py</code> <pre><code>class HuggingFaceTrainerForLSR(HuggingFaceTrainerMixin, BaseRetrieverTrainer):\n    \"\"\"HuggingFace LM-Supervised Retriever Trainer.\"\"\"\n\n    _hf_trainer: Optional[\"SentenceTransformerTrainer\"] = PrivateAttr(\n        default=None\n    )\n\n    def __init__(\n        self,\n        rag_system: RAGSystem,\n        train_dataset: \"Dataset\",\n        training_arguments: Optional[\"TrainingArguments\"] = None,\n        **kwargs: Any,\n    ):\n        super().__init__(\n            train_dataset=train_dataset,\n            rag_system=rag_system,\n            training_arguments=training_arguments,\n            **kwargs,\n        )\n\n    @model_validator(mode=\"after\")\n    def set_private_attributes(self) -&gt; \"HuggingFaceTrainerForLSR\":\n        # if made it to here, then this import is available\n        from sentence_transformers import SentenceTransformer\n\n        # validate rag system\n        _validate_rag_system(self.rag_system)\n\n        # validate model\n        if not isinstance(self.model, SentenceTransformer):\n            raise TrainerError(\n                \"For `HuggingFaceTrainerForLSR`, attribute `model` must be of type \"\n                \"`~sentence_transformers.SentenceTransformer`.\"\n            )\n\n        self._hf_trainer = LSRSentenceTransformerTrainer(\n            model=self.model,\n            args=self.training_arguments,\n            data_collator=DataCollatorForLSR(rag_system=self.rag_system),\n            train_dataset=self.train_dataset,\n        )\n\n        return self\n\n    def train(self, **kwargs: Any) -&gt; TrainResult:\n        output: TrainOutput = self.hf_trainer_obj.train(**kwargs)\n        return TrainResult(loss=output.training_loss)\n\n    def evaluate(self) -&gt; TestResult:\n        # TODO: implement this\n        raise NotImplementedError\n\n    @property\n    def hf_trainer_obj(self) -&gt; \"SentenceTransformerTrainer\":\n        return self._hf_trainer\n</code></pre>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.lsr.LSRSentenceTransformerTrainer","title":"LSRSentenceTransformerTrainer","text":"<p>               Bases: <code>SentenceTransformerTrainer</code></p> Source code in <code>src/fed_rag/trainers/huggingface/lsr.py</code> <pre><code>class LSRSentenceTransformerTrainer(SentenceTransformerTrainer):\n    def __init__(\n        self,\n        *args: Any,\n        data_collator: DataCollatorForLSR,\n        loss: Optional[LSRLoss] = None,\n        **kwargs: Any,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        # set loss\n        if loss is None:\n            loss = LSRLoss()\n        else:\n            if not isinstance(loss, LSRLoss):\n                raise InvalidLossError(\n                    \"`LSRSentenceTransformerTrainer` must use ~fed_rag.loss.LSRLoss`.\"\n                )\n\n        if not isinstance(data_collator, DataCollatorForLSR):\n            raise InvalidDataCollatorError(\n                \"`LSRSentenceTransformerTrainer` must use ~fed_rag.data_collators.DataCollatorForLSR`.\"\n            )\n\n        super().__init__(\n            *args, loss=loss, data_collator=data_collator, **kwargs\n        )\n\n    def collect_scores(\n        self, inputs: dict[str, torch.Tensor | Any]\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        if \"retrieval_scores\" not in inputs:\n            raise MissingInputTensor(\n                \"Collated `inputs` are missing key `retrieval_scores`\"\n            )\n\n        if \"lm_scores\" not in inputs:\n            raise MissingInputTensor(\n                \"Collated `inputs` are missing key `lm_scores`\"\n            )\n\n        retrieval_scores = inputs.get(\"retrieval_scores\")\n        lm_scores = inputs.get(\"lm_scores\")\n\n        return retrieval_scores, lm_scores\n\n    def compute_loss(\n        self,\n        model: \"SentenceTransformer\",\n        inputs: dict[str, torch.Tensor | Any],\n        return_outputs: bool = False,\n        num_items_in_batch: Any | None = None,\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, dict[str, Any]]:\n        \"\"\"Compute LSR loss.\n\n        NOTE: the forward pass of the model is taken care of in the DataCollatorForLSR.\n\n        Args:\n            model (SentenceTransformer): _description_\n            inputs (dict[str, torch.Tensor  |  Any]): _description_\n            return_outputs (bool, optional): _description_. Defaults to False.\n            num_items_in_batch (Any | None, optional): _description_. Defaults to None.\n\n        Raises:\n            NotImplementedError: _description_\n\n        Returns:\n            torch.Tensor | tuple[torch.Tensor, dict[str, Any]]: _description_\n        \"\"\"\n        retrieval_scores, lm_scores = self.collect_scores(inputs)\n        loss = self.loss(retrieval_scores, lm_scores)\n\n        # inputs are actually the outputs of RAGSystem's \"forward\" pass\n        return (loss, inputs) if return_outputs else loss\n</code></pre>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.lsr.LSRSentenceTransformerTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(\n    model,\n    inputs,\n    return_outputs=False,\n    num_items_in_batch=None,\n)\n</code></pre> <p>Compute LSR loss.</p> <p>NOTE: the forward pass of the model is taken care of in the DataCollatorForLSR.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SentenceTransformer</code> <p>description</p> required <code>inputs</code> <code>dict[str, Tensor | Any]</code> <p>description</p> required <code>return_outputs</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <code>num_items_in_batch</code> <code>Any | None</code> <p>description. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <p>Returns:</p> Type Description <code>Tensor | tuple[Tensor, dict[str, Any]]</code> <p>torch.Tensor | tuple[torch.Tensor, dict[str, Any]]: description</p> Source code in <code>src/fed_rag/trainers/huggingface/lsr.py</code> <pre><code>def compute_loss(\n    self,\n    model: \"SentenceTransformer\",\n    inputs: dict[str, torch.Tensor | Any],\n    return_outputs: bool = False,\n    num_items_in_batch: Any | None = None,\n) -&gt; torch.Tensor | tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"Compute LSR loss.\n\n    NOTE: the forward pass of the model is taken care of in the DataCollatorForLSR.\n\n    Args:\n        model (SentenceTransformer): _description_\n        inputs (dict[str, torch.Tensor  |  Any]): _description_\n        return_outputs (bool, optional): _description_. Defaults to False.\n        num_items_in_batch (Any | None, optional): _description_. Defaults to None.\n\n    Raises:\n        NotImplementedError: _description_\n\n    Returns:\n        torch.Tensor | tuple[torch.Tensor, dict[str, Any]]: _description_\n    \"\"\"\n    retrieval_scores, lm_scores = self.collect_scores(inputs)\n    loss = self.loss(retrieval_scores, lm_scores)\n\n    # inputs are actually the outputs of RAGSystem's \"forward\" pass\n    return (loss, inputs) if return_outputs else loss\n</code></pre>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.ralt.HuggingFaceTrainerForRALT","title":"HuggingFaceTrainerForRALT","text":"<p>               Bases: <code>HuggingFaceTrainerMixin</code>, <code>BaseGeneratorTrainer</code></p> <p>HuggingFace Trainer for Retrieval-Augmented LM Training/Fine-Tuning.</p> Source code in <code>src/fed_rag/trainers/huggingface/ralt.py</code> <pre><code>class HuggingFaceTrainerForRALT(HuggingFaceTrainerMixin, BaseGeneratorTrainer):\n    \"\"\"HuggingFace Trainer for Retrieval-Augmented LM Training/Fine-Tuning.\"\"\"\n\n    _hf_trainer: Optional[\"Trainer\"] = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        rag_system: RAGSystem,\n        train_dataset: \"Dataset\",\n        training_arguments: Optional[\"TrainingArguments\"] = None,\n        **kwargs: Any,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        if training_arguments is None:\n            training_arguments = _get_default_training_args()\n        else:\n            training_arguments.remove_unused_columns = (\n                False  # pragma: no cover\n            )\n\n        super().__init__(\n            train_dataset=train_dataset,\n            rag_system=rag_system,\n            training_arguments=training_arguments,\n            **kwargs,\n        )\n\n    @model_validator(mode=\"after\")\n    def set_private_attributes(self) -&gt; \"HuggingFaceTrainerForRALT\":\n        # if made it to here, then this import is available\n        from transformers import Trainer\n\n        # validate rag system\n        _validate_rag_system(self.rag_system)\n\n        self._hf_trainer = Trainer(\n            model=self.model,\n            args=self.training_arguments,\n            data_collator=DataCollatorForRALT(rag_system=self.rag_system),\n            train_dataset=self.train_dataset,\n        )\n\n        return self\n\n    def train(self, **kwargs: Any) -&gt; TrainResult:\n        output: TrainOutput = self.hf_trainer_obj.train(**kwargs)\n        return TrainResult(loss=output.training_loss)\n\n    def evaluate(self) -&gt; TestResult:\n        # TODO: implement this\n        raise NotImplementedError\n\n    @property\n    def hf_trainer_obj(self) -&gt; \"Trainer\":\n        return self._hf_trainer\n</code></pre>"},{"location":"api_reference/trainers/pytorch/","title":"Pytorch","text":"<p>PyTorch Trainer Mixin</p>"},{"location":"api_reference/trainers/pytorch/#src.fed_rag.trainers.pytorch.mixin.PyTorchTrainerProtocol","title":"PyTorchTrainerProtocol","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/fed_rag/trainers/pytorch/mixin.py</code> <pre><code>@runtime_checkable\nclass PyTorchTrainerProtocol(Protocol):\n    train_dataset: Dataset\n    training_arguments: TrainingArgs | None\n    train_dataloader: DataLoader\n\n    def model(self) -&gt; nn.Module:\n        pass  # pragma: no cover\n</code></pre>"},{"location":"api_reference/trainers/pytorch/#src.fed_rag.trainers.pytorch.mixin.PyTorchTrainerMixin","title":"PyTorchTrainerMixin","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>HuggingFace Trainer Mixin.</p> Source code in <code>src/fed_rag/trainers/pytorch/mixin.py</code> <pre><code>class PyTorchTrainerMixin(BaseModel, ABC):\n    \"\"\"HuggingFace Trainer Mixin.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n    train_dataset: Dataset\n    train_dataloader: DataLoader\n    training_arguments: TrainingArgs | None = None\n\n    def __init__(\n        self,\n        train_dataloader: DataLoader,\n        train_dataset: Dataset | None = None,\n        training_arguments: TrainingArgs | None = None,\n        **kwargs: Any,\n    ):\n        if train_dataset is None:\n            train_dataset = train_dataloader.dataset\n        else:\n            # ensure consistency between loader.dataset and the supplied one\n            if id(train_dataset) != id(train_dataloader.dataset):\n                raise InconsistentDatasetError(\n                    \"Inconsistent datasets detected between supplied `train_dataset` and that \"\n                    \"associated with the `train_dataloader`. These two datasets must be the same.\"\n                )\n\n        super().__init__(\n            train_dataset=train_dataset,\n            train_dataloader=train_dataloader,\n            training_arguments=training_arguments,\n            **kwargs,\n        )\n</code></pre>"},{"location":"community/changelog/","title":"Changelog","text":""},{"location":"community/changelog/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file. The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"community/changelog/#unreleased","title":"Unreleased","text":""},{"location":"community/changelog/#added","title":"Added","text":"<ul> <li>Add <code>NoEncodeRAGSystem</code> (#386)</li> <li>Add <code>NoEncodeBaseKnowledgeStore</code> and <code>NoEncodeAsyncBaseKnowledgeStore</code> (#385)</li> </ul>"},{"location":"community/changelog/#0023-2025-05-27","title":"[0.0.23] - 2025-05-27","text":""},{"location":"community/changelog/#added_1","title":"Added","text":"<ul> <li>Add <code>BaseAsyncKnowledgeStore</code> (#262)</li> <li>Add ability to save benchmark evaluations to a JSONL file of <code>BenchmarkEvaluationExamples</code> (#377)</li> </ul>"},{"location":"community/changelog/#0022-2025-05-26","title":"[0.0.22] - 2025-05-26","text":""},{"location":"community/changelog/#added_2","title":"Added","text":"<ul> <li>Feature/348 onboard pubmedqa (#363)</li> </ul>"},{"location":"community/changelog/#changed","title":"Changed","text":"<ul> <li>Fix bug in <code>HuggingFaceGeneratorMixin.generate()</code> to return newly generated tokens only (#345)</li> <li>Passing kwargs for trainer (#365)</li> </ul>"},{"location":"community/changelog/#0021-2025-05-23","title":"[0.0.21] - 2025-05-23","text":""},{"location":"community/changelog/#changed_1","title":"Changed","text":"<ul> <li>Fix bug in <code>UnslothFastModelGenerator</code> when calling <code>to_peft</code> and LoRA adapters mismatch dtype with base (#360)</li> <li>Fix bug in <code>HFPreTrainedTokenizer.encode()</code> where shape of raw encoder result <code>tokenizer()</code> is of dimension 2 i.e., batched (#360)</li> <li>Improved <code>DataCollatorForRALT</code> final dtypes after padding is applied (#360)</li> </ul>"},{"location":"community/changelog/#0020-2025-05-23","title":"[0.0.20] - 2025-05-23","text":""},{"location":"community/changelog/#added_3","title":"Added","text":"<ul> <li>Support <code>get_peft_model</code> in <code>UnslothFastModelGenerator</code> (#356)</li> <li>New integration: Unsloth \ud83e\udda5 <code>UnslothFastModelGenerator</code> (#356)</li> </ul>"},{"location":"community/changelog/#changed_2","title":"Changed","text":"<ul> <li>Make knowledge node embedding optional and remove redundant embedding in Qdrant upserts (#353)</li> </ul>"},{"location":"community/changelog/#0019-2025-05-20","title":"[0.0.19] - 2025-05-20","text":""},{"location":"community/changelog/#added_4","title":"Added","text":"<ul> <li>Add support for in-memory Qdrant instances (#350)</li> <li>Add <code>EvalError</code> (#339)</li> <li>Add streaming support for HF benchmarks (#337)</li> <li>[Feature] Add HuggingFaceBenchmarkMixin and first HuggingFaceMMLU benchmark (#334)</li> <li>[Feature] Add ExactMatchEvaluationMetric (#333)</li> <li>[Feature] Add base.evals module and BaseBenchmark BaseBenchmarker classes (#326)</li> </ul>"},{"location":"community/changelog/#changed_3","title":"Changed","text":"<ul> <li>Add <code>num_examples</code> to <code>BaseBenchmark</code> (#344)</li> </ul>"},{"location":"community/changelog/#0018-2025-05-16","title":"[0.0.18] - 2025-05-16","text":""},{"location":"community/changelog/#added_5","title":"Added","text":"<ul> <li>Trainer managers to root import (#320)</li> </ul>"},{"location":"community/changelog/#changed_4","title":"Changed","text":"<ul> <li>Don't raise <code>MissingExtraError</code> for HF generators at import time (#320)</li> <li>Add public api for <code>fed_rag.trainers</code> (#320)</li> <li>Refactor <code>types</code> to <code>data_structures</code> (#319)</li> </ul>"},{"location":"community/changelog/#0017-2025-05-16","title":"[0.0.17] - 2025-05-16","text":""},{"location":"community/changelog/#changed_5","title":"Changed","text":"<ul> <li>Rip out public api, as its not standard (#317)</li> </ul>"},{"location":"community/changelog/#0016-2025-05-16","title":"[0.0.16] - 2025-05-16","text":""},{"location":"community/changelog/#public-api-002","title":"public API [0.0.2]","text":"<ul> <li>added retrievers, generators, knowledge stores, and rest of types (#310)</li> </ul>"},{"location":"community/changelog/#0015-2025-05-16","title":"[0.0.15] - 2025-05-16","text":""},{"location":"community/changelog/#added_6","title":"Added","text":"<ul> <li><code>.api</code> public module (#307)</li> <li>New <code>core</code> module that houses <code>RAGSystem</code> (#306)</li> </ul>"},{"location":"community/changelog/#changed_6","title":"Changed","text":"<ul> <li>Refactor: move aux RAG system types to <code>fed_rag.types.rag</code> (#306)</li> </ul>"},{"location":"community/changelog/#0014-2025-05-14","title":"[0.0.14] - 2025-05-14","text":""},{"location":"community/changelog/#added_7","title":"Added","text":"<ul> <li><code>LlamaIndexBridge</code> (#285)</li> <li>Add base bridge mixin (#284)</li> </ul>"},{"location":"community/changelog/#0013-2025-05-10","title":"[0.0.13] - 2025-05-10","text":""},{"location":"community/changelog/#added_8","title":"Added","text":"<ul> <li>[Feature] Add Qdrant knowledge store (sync) (#259)</li> </ul>"},{"location":"community/changelog/#changed_7","title":"Changed","text":"<ul> <li><code>QdrantKnowledgeStore</code> align with qdrant sdk (#273)</li> <li>[Fix] Set timeout param to ~qdrant_client.QdrantClient and use contextmanager for client creation and teardown (#272)</li> <li>add load_nodes_kwargs and unit test <code>QdrantKnowledgeStore</code> (#266)</li> <li>fix count <code>QdrantKnowledgeStore</code> (#265)</li> <li>update federate fine-tune (#256)</li> <li>Move validation of the presence of trainers to BaseTrainerManager (#257)</li> </ul>"},{"location":"community/changelog/#0012-2025-05-03","title":"[0.0.12] - 2025-05-03","text":""},{"location":"community/changelog/#changed_8","title":"Changed","text":"<ul> <li>Manually handle the padding versus delegating to <code>DataCollatorForCausalLM</code> (#248)</li> <li>Ensure that <code>TrainingArguments.remove_unused_columns</code> is set to <code>False</code> (#248)</li> </ul>"},{"location":"community/changelog/#0011-2025-05-03","title":"[0.0.11] - 2025-05-03","text":""},{"location":"community/changelog/#added_9","title":"Added","text":"<ul> <li><code>HuggingFaceTrainerManager</code> missing implementations for preparing retriever/generator models for training (#245)</li> </ul>"},{"location":"community/changelog/#changed_9","title":"Changed","text":"<ul> <li>[Fix] LSRLoss should have first input in KL div be in log space (#245)</li> <li>[Fix] <code>DataCollatorForLSR</code> should subclass <code>SentenceTransformerDataCollator</code> (#245)</li> <li>[Fix] <code>DataCollatorForLSR</code> should require grads for retriever's for retriever scores, but not for lm scores (#245)</li> </ul>"},{"location":"community/changelog/#0010-2025-05-02","title":"[0.0.10] - 2025-05-02","text":""},{"location":"community/changelog/#added_10","title":"Added","text":"<ul> <li>[Feature] Add HuggingFaceTrainerForRALT and associated DataCollatorForRALT (#241)</li> <li>[Feature] Add PyTorchTrainerMixin (#239)</li> <li>add trainers and data collators (#232)</li> <li>[Feature] Add fed_rag.base.data_collators and BaseDataCollator (#231)</li> <li>[Feature] Add target template to DataCollatorForLSR (#230)</li> <li>[Feature] Implement compute_loss for LSRSentenceTransformerTrainer (#229)</li> <li>[Feature] Add HuggingFaceLSRTrainer (#227)</li> <li>[Feature] Adds HuggingFaceTrainerMixin (#226)</li> <li>Add BaseTrainer (#225)</li> <li>[Feature] Add HFRAGTrainer (#220)</li> <li>[Feature] Add PyTorchRAGTrainer (#219)</li> <li>[Feature] Data collators for LSR (both huggingface and torch) (#187)</li> <li>Add exception for missing extra error (#208)</li> </ul>"},{"location":"community/changelog/#changed_10","title":"Changed","text":"<ul> <li>[refactor] Improvements to TrainerManagers and Trainers classes and adds BaseRetrieverTrainer &amp; BaseGeneratorTrainer (#238)</li> <li>[Refactor] Move DataCollatorForLSR to new fed_rag.data_collators.huggingface module (#233)</li> <li>[chore] rename to trainer manager (#223)</li> <li>[chore] Move HFSentenceTransformerRetriever to huggingface module for consistency (#207)</li> </ul>"},{"location":"community/changelog/#009-2025-04-25","title":"[0.0.9] - 2025-04-25","text":""},{"location":"community/changelog/#added_11","title":"Added","text":"<ul> <li>[Feature] Add abstract method BaseGenerator.compute_target_sequence_proba (cherry picked) (#201)</li> <li>[Feature] Add LM Supervised Retriever Loss LSRLoss (#182)</li> </ul>"},{"location":"community/changelog/#changed_11","title":"Changed","text":"<ul> <li>Make huggingface generators more dry (#202)</li> <li>Improve/validate LSRLoss forward interface (#185)</li> </ul>"},{"location":"community/changelog/#008-2025-04-09","title":"[0.0.8] - 2025-04-09","text":""},{"location":"community/changelog/#added_12","title":"Added","text":"<ul> <li><code>BaseTokenizer.encode</code> now retursn new type <code>EncoderResult</code> (#150)</li> </ul>"},{"location":"community/changelog/#007-2025-04-06","title":"[0.0.7] - 2025-04-06","text":""},{"location":"community/changelog/#added_13","title":"Added","text":"<ul> <li>ManagedInMemoryKnowledgeStore and ManagedMixin for KnowledgeStore [#135]</li> <li>Added <code>name</code> attribute to <code>BaseKnowledgeStore</code> [#135]</li> <li>Knowledge store exceptions [#135]</li> </ul>"},{"location":"community/changelog/#changed_12","title":"Changed","text":"<ul> <li>Persist and load uses <code>name</code> (<code>ks_id</code> only exists for managed version) [#135]</li> <li>Exception modules have their own respective base exception [#135]</li> </ul>"},{"location":"community/changelog/#006-2025-03-26","title":"[0.0.6] - 2025-03-26","text":""},{"location":"community/changelog/#post","title":"Post","text":"<ul> <li>Test Zenodo</li> </ul>"},{"location":"community/changelog/#added_14","title":"Added","text":"<ul> <li>[Feature] Add utils.data module for building RAG Fine-tuning datasets given a RAGSystem and Sequence of examples (#98)</li> <li>[Feature] Adds BaseTokenizer and HFPretrainedTokenizer subclass (#99)</li> <li>[Feature] Enable BaseGenerator and BaseRetriever used in RAGSystem to access the rag system (#95)</li> </ul>"},{"location":"community/changelog/#changed_13","title":"Changed","text":"<ul> <li>Small fixes to problems found while running the quick start example (#93)</li> </ul>"},{"location":"community/changelog/#other","title":"Other","text":"<ul> <li>Update LICENSE (#92)</li> </ul>"},{"location":"community/changelog/#005-2025-03-17","title":"[0.0.5] - 2025-03-17","text":""},{"location":"community/changelog/#added_15","title":"Added","text":"<ul> <li>[Feature] Added HFPeftModelGenerator for efficient fine-tuning (#84)</li> <li>[Feature] Added HuggingFace's peft.PeftModel as an acceptable model type for decoration (#76)</li> <li>Added vector compute submodule (#70)</li> <li>[Example] Added RA-DIT implementation using HFPeftModelGenerator for llama-2-7b (#86)</li> <li>[Example] Added mock generator training loop; federated for a 350M model (#67)</li> </ul>"},{"location":"community/changelog/#changed_14","title":"Changed","text":"<ul> <li>[Example] RA-DIT default generator model to load with device map auto (#88)</li> <li>[Example] RA-DIT re-factor example for better organization (#87)</li> <li>[Feature] Updated set_weights and get_weights for HuggingFaceFlowerClient to include logic for PeftModel (#82)</li> <li>Re-organized RA-DIT example app (#69)</li> </ul>"},{"location":"community/changelog/#004-2025-03-14","title":"[0.0.4] - 2025-03-14","text":""},{"location":"community/changelog/#added_16","title":"Added","text":"<ul> <li>[EXAMPLE] Adds RA-DIT retriever train loop using HuggingFace integrations (#64)</li> <li>Added HuggingFaceFLTask for federated learning with HuggingFace models (#61)</li> <li>Added federate.tester.huggingface and associated inspectors (#59)</li> <li>Added federate.trainer.huggingface decorators (#56)</li> <li>Added ability to specify device on load for HF Models (#55)</li> </ul>"},{"location":"community/changelog/#003-2025-03-12","title":"[0.0.3] - 2025-03-12","text":""},{"location":"community/changelog/#added_17","title":"Added","text":"<ul> <li>Build RAG system implementation (#50)</li> <li>Added RAGSystem class (#45)</li> <li>Implemented InMemoryKnowledgeStore (#43)</li> <li>Added BaseKnowledgeStore and KnowledgeNode models (#41)</li> <li>Added HFSentenceTransformerRetriever (#38)</li> <li>Added BaseRetriever class (#35)</li> <li>Added HFPreTrainedGenerator class for generative models (#34)</li> <li>Implemented BaseGenerator and HFPretrainedModelGenerator classes (#33)</li> <li>Added support for llama3 models (#30)</li> <li>Added example Retrieval-Augmented Generation with LLM integration (#28)</li> <li>Implemented DragonRetriever for example RAG system (#26)</li> </ul>"},{"location":"community/changelog/#changed_15","title":"Changed","text":"<ul> <li>Updated to use HFSentenceTransformerRetriever in examples (#48)</li> </ul>"},{"location":"community/changelog/#002-2025-03-01","title":"[0.0.2] - 2025-03-01","text":""},{"location":"community/changelog/#changed_16","title":"Changed","text":"<ul> <li>Quickstart as a workspace member, smaller package builds (#24)</li> </ul>"},{"location":"community/changelog/#001-2025-03-01","title":"[0.0.1] - 2025-03-01","text":""},{"location":"community/changelog/#added_18","title":"Added","text":"<ul> <li>Working QuickStart! (#20)</li> <li>Implementation PyTorchFLTask.server() (#17)</li> <li>PyTorchFLTask and PyTorchFlowerClient (#16)</li> <li>Inspection of tester callable for pytorch.tester decorator (#14)</li> <li>federate.trainer.pytorch decorate inspects trainer loop to extract spec (#12)</li> <li>BaseTaskModel and PyTorchTaskModel (#5)</li> </ul>"},{"location":"community/contributing/","title":"Contributing to FedRAG","text":"<p>Thank you for your interest in contributing to FedRAG! This document provides guidelines and instructions for contributing.</p> <p>We welcome contributions from developers of all skill levels. Whether you're fixing a typo or implementing a complex feature, your help is valuable to the FedRAG project.</p>"},{"location":"community/contributing/ask_question/","title":"Ask a Question","text":"<p>We welcome questions from users and contributors at all levels of experience with FedRAG. Having questions is a natural part of engaging with a complex project, and we're here to help.</p>"},{"location":"community/contributing/ask_question/#where-to-ask-questions","title":"Where to Ask Questions","text":"<p>FedRAG offers several channels for asking questions:</p> <ul> <li> <p>Discord Community: Join our Discord community for real-time discussions and quick questions.</p> </li> <li> <p>GitHub Discussions: For longer, more detailed questions, use GitHub Discussions. This is ideal for questions that might benefit the wider community.</p> </li> </ul>"},{"location":"community/contributing/submit_issue/","title":"Submitting an Issue","text":"<p>Issues are an important way to track bugs, feature requests, and improvements to FedRAG.</p>"},{"location":"community/contributing/submit_issue/#before-creating-an-issue","title":"Before Creating an Issue","text":"<p>Before submitting a new issue:</p> <ol> <li> <p>Search existing issues: Check GitHub Issues to see if your problem has already been reported or if a related feature request exists.</p> </li> <li> <p>Check the documentation: Verify that your question isn't already addressed in our documentation.</p> </li> <li> <p>Confirm it's an issue: For general questions, please use GitHub Discussions or our Discord community instead.</p> </li> </ol> <p>We appreciate your contributions to making FedRAG better through thoughtful issue submissions!</p>"},{"location":"community/contributing/submit_pr/","title":"Submitting a Pull Request","text":"<p>Pull requests (PRs) are the primary way to contribute code changes to FedRAG. We welcome contributions for bug fixes, documentation improvements, new features, and enhancements to existing functionality.</p>"},{"location":"community/contributing/submit_pr/#developing-your-contribution","title":"Developing your Contribution","text":"<p>Follow these steps to create a well-structured pull request:</p> <ol> <li> <p>Create a descriptive branch</p> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <p>Tip</p> <p>Choose a branch name that reflects your contribution (e.g., <code>fix/memory-leak</code>, <code>docs/improve-tutorials</code>, <code>feature/add-transformer-support</code>).</p> </li> <li> <p>Make your changes following our Development Guidelines</p> </li> <li> <p>Commit with clear messages</p> <pre><code>git commit -m \"Add feature: description of changes\"\n</code></pre> <p>Tip</p> <p>Write commit messages that explain both what and why. Include issue numbers when applicable (e.g., \"Fix #42: Resolve memory leak in vector store\").</p> </li> <li> <p>Push to your fork</p> <pre><code>git push origin feature/your-feature-name\n</code></pre> </li> <li> <p>Open a pull request against the <code>main</code> branch and fill in the provided     PR message template.</p> </li> </ol>"},{"location":"community/contributing/submit_pr/#documentation-contributions","title":"Documentation Contributions","text":"<p>When making documentation changes:</p> <pre><code># Preview documentation changes locally\nmkdocs serve\n</code></pre> <p>This launches a development server at <code>http://127.0.0.1:8000/</code>, allowing you to preview changes in real-time as you edit. We encourage screenshots or animated GIFs for UI-related changes.</p>"},{"location":"community/contributing/submit_pr/#code-review-process","title":"Code Review Process","text":"<p>Our review process ensures high-quality contributions:</p> <ul> <li>At least one maintainer will review your PR</li> <li>Address any feedback promptly and thoroughly</li> <li>Once approved, a maintainer will merge your PR</li> <li>Significant changes may require multiple reviewers</li> <li>Be responsive to comments and questions</li> </ul>"},{"location":"community/contributing/submit_pr/#development-guidelines","title":"Development Guidelines","text":""},{"location":"community/contributing/submit_pr/#coding-style","title":"Coding Style","text":"<p>We maintain high code quality standards through consistent style and automated tools:</p> <ul> <li>Follow PEP 8 conventions</li> <li>Use Black for consistent formatting</li> <li>Apply isort for organized imports</li> <li>Run Ruff for comprehensive linting</li> </ul> <p>Our pre-commit hooks automatically enforce these standards when you commit changes. You can also invoke these hooks manually via the following commands:</p> <pre><code># Run formatter\nmake format\n\n# Run linters\nmake lint\n</code></pre>"},{"location":"community/contributing/submit_pr/#documentation","title":"Documentation","text":"<p>Clear documentation is essential:</p> <ul> <li>Document all public APIs using Google docstring format</li> <li>Update relevant documentation when modifying features</li> <li>Include practical examples to demonstrate functionality</li> <li>Ensure code comments explain \"why\" rather than \"what\"</li> </ul>"},{"location":"community/contributing/submit_pr/#testing","title":"Testing","text":"<p>Comprehensive testing ensures reliability:</p> <ul> <li>Write tests for all new features and bug fixes</li> <li>Aim to maintain or improve overall test coverage</li> <li>Use pytest for writing clear, effective tests</li> <li>Verify all tests pass before submitting:</li> </ul> <pre><code># Run the full test suite\nmake test\n</code></pre> <p>We appreciate your contributions to making FedRAG better!</p>"},{"location":"community/resources/pocket_references/","title":"AI Pocket References","text":"<p>The AI Pocket Reference project is maintained by Vector AI Engineering as an accessible resource for the AI community. It provides a collection of pocket references offering concise information on a wide range of AI topics, including Natural Language Processing (NLP) and Federated Learning (FL).</p>"},{"location":"community/resources/pocket_references/#recommended-collections","title":"Recommended Collections","text":"<ul> <li> <p>NLP Collection \u2014 Covers various topics within NLP, including RAG, LoRA, Quantization, Chain of Thought, Agents, and more.</p> </li> <li> <p>FL Collection \u2014 Encompasses the fundamentals of federated learning along with advanced topics such as personalized federated learning and vertical federated learning.</p> </li> </ul>"},{"location":"examples/","title":"Case Studies","text":"<p>Here are some in-depth case studies to further demonstrate FedRAG's usage and its overall utilty.</p> <ul> <li> RA-DIT \u2014 A comprehensive   reproduction of the RA-DIT method, adapted for practical demonstration.</li> </ul>"},{"location":"examples/ra_dit/","title":"A comprehensive implementation of RA-DIT","text":"<p>We consider the paper \"RA-DIT: Retrieval-Augmented Dual Instruction Tuning\" by Lin, Xi Victoria et al. (2023)<sup>1</sup> and implement simplified versions of their experiments using FedRAG. In this work, the authors build a RAG system and fine-tune both the generator and retriever using a diverse question-answering (QA) datasets. Their experimental results demonstrate that a fine-tuned RAG system consistently outperforms two key baselines: a standalone generator LLM and an un-fine-tuned RAG system. These findings highlight the substantial benefits of applying the RA-DIT approach to enhance RAG system performance.</p> <p>This comprehensive implementation demonstrates the key concepts and techniques from the original research while adapting them for practical demonstration. More specifically, in this example, we:</p> <ol> <li> <p>Build a Qdrant Knowledge Store \u2014 Take   artifacts derived from Wikipedia to populate a<code>QdrantKnowledgeStore</code>.</p> </li> <li> <p>Fine-tune with QA datasets \u2014 Build a   <code>RAGSystem</code> and fine-tune it with   some QA datasets using LSR and RALT trainers.</p> </li> <li> <p>Evaluate with Benchmarks \u2014 Benchmark our fine-tuned RAG system   on MMLU and compare it to a few appropriate baselines.</p> </li> <li> <p>Federated Fine-tuning \u2014 Demonstrate how we can go   from centralized to federated fine-tuning of our RAG system.</p> </li> </ol> <p>Note</p> <p>Federated fine-tuning was not considered in Lin, Xi Victoria et al (2023)<sup>1</sup>.</p> <ol> <li> <p>Lin, Xi Victoria, et al. \"Ra-dit: Retrieval-augmented dual instruction tuning.\" The Twelfth International Conference on Learning Representations. 2023.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"examples/ra_dit/benchmarking/","title":"Evaluate with Benchmarks","text":"<p>Coming Soon!</p> <p>This documentation page is currently under development.</p>"},{"location":"examples/ra_dit/federated_finetune/","title":"Federated Fine-tuning","text":"<p>Coming Soon!</p> <p>This documentation page is currently under development.</p>"},{"location":"examples/ra_dit/finetune/","title":"Fine-tuning with QA Datasets","text":"<p>Coming Soon!</p> <p>This documentation page is currently under development.</p>"},{"location":"examples/ra_dit/qdrant_knowledge_store_wikipedia/","title":"Build a Qdrant Knowledge Store","text":""},{"location":"examples/ra_dit/qdrant_knowledge_store_wikipedia/#introduction","title":"Introduction","text":"<p>For their RAG system, the authors of the RA-DIT<sup>1</sup> paper, used a knowledge store that was comprised of artifacts from two sources:</p> <ol> <li>Text chunks from the Dec. 20, 2021 Wikipedia dump. <sup>2</sup></li> <li>A sample of text chunks from the 2017-2020 CommonCrawl dumps. <sup>3</sup></li> </ol> <p>In total, their knowledge store (which they referred to as a \"retrieval corpus\") consisted of 399M knowledge nodes, each having text content with no more than 200 words.</p>"},{"location":"examples/ra_dit/qdrant_knowledge_store_wikipedia/#our-scaled-down-knowledge-store","title":"Our Scaled Down Knowledge Store","text":""},{"location":"examples/ra_dit/qdrant_knowledge_store_wikipedia/#the-raw-text-chunks","title":"The raw text chunks","text":"<p>For practical purposes, in this example, we only consider the text chunks from the Dec. 20, 2021 Wikipedia dump. Moreover, we only make use of the file <code>text-list-100-sec.jsonl</code> from this dump, which contains 33,176,581 chunks that follow the JSON schema depicted below.</p> A text chunk<pre><code>chunk = {\n    \"id\": \"...\",\n    \"title\": \"...\",\n    \"section\": \"...\",\n    \"text\": \"...\"\n}\n</code></pre>"},{"location":"examples/ra_dit/qdrant_knowledge_store_wikipedia/#creating-a-knowledgenode","title":"Creating a <code>KnowledgeNode</code>","text":"<p>Each raw text chunk is converted to a <code>KnowledgeNode</code>, using a simple template for preparing the node's <code>text_content</code>.</p> Creating a KnowledgeNode code snippet<pre><code>import json\nfrom fed_rag.data_structures.knowledge_node import KnowledgeNode\n\nchunk = json.loads(chunk_json_str)\ncontext_text = (\n    f\"title: {chunk.pop('title')]}\"  # (1)!\n    f\"\\nsection: {chunk.pop('section')}\"\n    f\"\\ntext: {chunk.pop('text')}\"\n)\nembedding = retriever.encode_context(context_text).tolist()  # (2)!\n\nnode = KnowledgeNode(\n    embedding=embedding,\n    node_type=NodeType.TEXT,\n    text_content=context_text,\n    metadata=chunk,\n)\n</code></pre> <ol> <li>A simple template for creating the node's context from the chunk's data.</li> <li>A <code>HuggingFaceSentenceTransformerRetriever</code> was built using the models: <code>nthakur/dragon-plus-context-encoder</code> and <code>nthakur/dragon-plus-query-encoder</code>.</li> </ol>"},{"location":"examples/ra_dit/qdrant_knowledge_store_wikipedia/#adding-nodes-to-a-qdrantknowledgestore","title":"Adding nodes to a <code>QdrantKnowledgeStore</code>","text":"<p>In this example, we use the Qdrant extra and add our knowledge nodes to a locally running <code>QdrantKnowledgeStore</code>. The code snippet below shows how to instantiate such a knowledge store and subsequently add a node to it.</p> <p>In the accompanying code, we provide users the options to change their retriever <code>~sentence_transformer.SentenceTransformer</code> model, and also change the number of raw text chunks to index.</p> Adding our nodes to a QdrantKnowledgeStore<pre><code>from fed_rag.knowledge_stores.qdrant import QdrantKnowledgeStore\n\nknowledge_store = QdrantKnowledgeStore(\n    collection_name=\"nthakur.dragon-plus-context-encoder\"\n)\n\n# add node to knowledge store\nknowledge_store.load_node(node)\n</code></pre>"},{"location":"examples/ra_dit/qdrant_knowledge_store_wikipedia/#getting-the-knowledge-store","title":"Getting The Knowledge Store","text":"<p>For convenience, we provide a Docker image that contains a pre-built Qdrant vector database that builds the knowledge store as described above.</p> <p>To get this image, you can pull it from Vector Institute's docker hub:</p> <pre><code># Pull the image\ndocker pull vectorinstitute/qdrant-atlas-dec-wiki-2021:latest\n</code></pre> <p>Note</p> <p>This Docker image is approximately 3.6GB in size due to the included Python environment and ML libraries. Ensure you have sufficient disk space and bandwidth when pulling the image.</p> <p>The image can then be executed with the command provided below. This code will download the raw Wikipedia text chunks, encode them using the default retriever model (i.e., <code>nthakur/dragon-plus-context-encoder</code>) and add them to the Qdrant vector database.</p> Running the docker image<pre><code># Run the container with basic settings and gpu acceleration\ndocker run -d \\\n  --name qdrant-vector-db \\\n  --gpus all \\\n  -p 6333:6333 \\\n  -p 6334:6334 \\  # needed for gRPC\n  -v qdrant_data:/qdrant_storage \\\n  vectorinstitute/qdrant-atlas-dec-wiki-2021:latest\n</code></pre> <p>The above command runs in detached mode and will create a container called <code>qdrant-vector-db</code>. Monitor the logs of the container to determine the progress. Once the container is <code>healthy</code> then it can be used.</p> <p>The command above launches a detached container named <code>qdrant-vector-db</code>. You can monitor its progress through the container logs via:</p> <pre><code>docker logs qdrant-vector-db\n</code></pre> <p>The knowledge store is ready for use once the container status shows as <code>healthy</code>.</p> <p>Note</p> <p>Building the knowledge store with all 33,176,581 text chunks can take 4-7 days, depending on your hardware setup. While our code provides a solid foundation, you can implement further optimizations based on your specific performanc requirements and infrastructure.</p> <p>Tip</p> <p>To quickly verify the Docker image works correctly, use the parameter <code>-e SAMPLE_SIZE=tiny</code> when running the container. This executes the process on a small subset of Wikipedia text chunks, allowing for rapid validation before committing to a larger subset or the full dataset.</p>"},{"location":"examples/ra_dit/qdrant_knowledge_store_wikipedia/#testing-the-knowledge-store","title":"Testing the knowledge store","text":"<p>Once the container shows <code>healthy</code>, the knowledge store can be used. Below is a code snippet for quickly testing that nodes can be successfully retrieved from it.</p> Testing the knowledge store with FedRAG<pre><code>from fed_rag.knowledge_stores.qdrant import QdrantKnowledgeStore\nfrom fed_rag.retrievers.huggingface.hf_sentence_transformer import (\n    HFSentenceTransformerRetriever,\n)\n\n# build retriever for encoding queries\nretriever = HFSentenceTransformerRetriever(\n    query_model_name=\"nthakur/dragon-plus-query-encoder\",\n    context_model_name=\"nthakur/dragon-plus-context-encoder\",\n    load_model_at_init=False,\n)\n\n# Connect to the containerized knowledge store\nknowledge_store = QdrantKnowledgeStore(\n    collection_name=\"nthakur.dragon-plus-context-encoder\",\n)\n\n# Retrieve documents\nquery = \"What is the history of marine biology?\"\nquery_emb = retriever.encode_query(query).tolist()\n\nresults = knowledge_store.retrieve(query_emb=query_emb, top_k=3)\nfor node in results:\n    print(f\"Score: {node.score}, Content: {str(node.node)}\")\n</code></pre>"},{"location":"examples/ra_dit/qdrant_knowledge_store_wikipedia/#more-details-about-the-docker-image","title":"More details about the Docker image","text":"<p>For comprehensive information about the prepared Docker image and its available configuration options, visit: https://github.com/VectorInstitute/fed-rag/tree/main/examples/knowledge_stores/ra-dit-ks.</p>"},{"location":"examples/ra_dit/qdrant_knowledge_store_wikipedia/#whats-next","title":"What's Next?","text":"<p>Now that our knowledge store is complete, we'll proceed to construct the RAG system for fine-tuning.</p> <p>All code related to the knowledge store implementation can be found in the <code>examples/ra-dit/knowledge_store/</code> directory.</p> <ol> <li> <p>Lin, Xi Victoria, et al. \"Ra-dit: Retrieval-augmented dual instruction tuning.\"   The Twelfth International Conference on Learning Representations. 2023.\u00a0\u21a9</p> </li> <li> <p>Common Crawl Foundation. (2017-2020). Common Crawl. Retrieved from https://commoncrawl.org/ \u21a9</p> </li> <li> <p>Izacard, Gautier, et al. \"Few-shot learning with retrieval augmented language   models.\" arXiv preprint arXiv:2208.03299 1.2 (2022): 4.\u00a0\u21a9</p> </li> </ol>"},{"location":"getting_started/essentials/","title":"Essentials","text":"<p>To get to know FedRAG a bit better and understand its purpose, we provide the answers to the following four essential questions.</p>"},{"location":"getting_started/essentials/#four-essential-questions","title":"Four Essential Questions","text":""},{"location":"getting_started/essentials/#what-is-rag","title":"What is RAG?","text":"<p>Retrieval-Augmented Generation (RAG) is a widely used technique that addresses a main drawback of Large Language Models (LLM), which is that they're trained on historical corpora and thus answering user queries that heavily rely on recent data are not really possible. Further, using the parametric knowledge of LLMs alone has yielded subpar performance on knowledge-intensive benchmarks.</p> <p>RAG provides access to relevant (and potentially more recent) non-parametric knowledge (i.e. data) that are stored in Knowledge Stores to the LLM so that it can use it in order to more accurately respond to user queries.</p>"},{"location":"getting_started/essentials/#what-is-federated-learning","title":"What is Federated Learning?","text":"<p>Federated Learning (FL) is a technique for building machine learning (as well as deep learning) models when the data is decentralized. Rather than first centralizing the datasets to a central location, which may not be possible due to strict data residency regulations or may be uneconomical due to the significant monetary costs in moving massive datasets, FL enables collaborative model building by facilitating the sharing of the model weights between the data providers.</p>"},{"location":"getting_started/essentials/#why-federated-fine-tuning-of-rag","title":"Why Federated Fine-Tuning of RAG?","text":"<p>Fine-tuning is a technique that is used to enhance the performance of LLMs by speciliazing its general capabilities towards a specific domain. It has also been shown that fine-tuning the model components of RAG systems, namely the generator and retriever, on domain-specific datasets can lead to its overall improved performance.</p> <p>Accessing fine-tuning datasets may be challenging. And, in situations where the data is dispersed across several nodes, and centralizing is either not possible or uneconomical, the fine-tuning of these RAG systems can be made possible through FL.</p>"},{"location":"getting_started/essentials/#who-is-fedrag-for","title":"Who is FedRAG for?","text":"<p>FedRAG is for the model builders, data scientists, and researchers who wish to fine-tune their RAG systems on their own datasets.</p> <p>Note \u2014 FedRAG prioritizes both centralized and federated RAG fine-tuning</p> <p>While FedRAG supports federated learning scenarios, it's designed first and foremost as a comprehensive RAG fine-tuning library. Most users deploy FedRAG in completely centralized environments to take advantage of its intuitive API, powerful abstractions, and integration with popular frameworks.</p> <p>Centralized mode offers the full range of RAG fine-tuning techniques with zero federation overhead. The federated capabilities are available when you need them for privacy-sensitive or distributed data scenarios.</p>"},{"location":"getting_started/import_patterns/","title":"Import Patterns","text":"<p>FedRAG provides a carefully designed public API for working with RAG and both centralized and federated fine-tuning components. All components exported at the root level and from public subpackages are considered stable and follow semantic versioning guidelines.</p>"},{"location":"getting_started/import_patterns/#root-imports","title":"Root Imports","text":"<p>Import core components directly from the root:</p> <pre><code>from fed_rag import (\n    RAGSystem,\n    RAGConfig,\n    HFPretrainedModelGenerator,\n    HFSentenceTransformerRetriever,\n    InMemoryKnowledgeStore,\n)\n\n# Now use the components directly\nsystem = RAGSystem(\n    retriever=HFSentenceTransformerRetriever(...),\n    generator=HFPretrainedModelGenerator(...),\n    knowledge_store=InMemoryKnowledgeStore(),\n    rag_config=RAGConfig(...),\n)\n</code></pre>"},{"location":"getting_started/import_patterns/#namespaced-imports","title":"Namespaced Imports","text":"<p>For better organization and increased clarity, you can import from specific component categories:</p> <pre><code>from fed_rag.core import RAGSystem\nfrom fed_rag.data_structures.rag import RAGConfig\nfrom fed_rag.generators import HFPretrainedModelGenerator\nfrom fed_rag.retrievers import HFSentenceTransformerRetriever\nfrom fed_rag.knowledge_stores import InMemoryKnowledgeStore\n\n# Create system with components from different namespaces\nsystem = RAGSystem(\n    retriever=HFSentenceTransformerRetriever(...),\n    generator=HFPretrainedModelGenerator(...),\n    knowledge_store=InMemoryKnowledgeStore(),\n    rag_config=RAGConfig(...),\n)\n</code></pre> <p>Note</p> <p>Modules and functions prefixed with an underscore (e.g., <code>_internal</code>) are considered implementation details and may change between versions.</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#installing-from-package-managers","title":"Installing from package managers","text":""},{"location":"getting_started/installation/#pypi","title":"PyPi","text":"<p>As seen in the previous quickstart examples, we can install FedRAG via <code>pip</code>:</p> <pre><code>pip install fed-rag\n</code></pre>"},{"location":"getting_started/installation/#conda","title":"Conda","text":"<p>For <code>conda</code> users, <code>fed-rag</code> has been published to the <code>conda-forge</code> channel, and thus can be installed with <code>conda</code> using the below command:</p> <pre><code>conda install -c conda-forge fed-rag\n</code></pre>"},{"location":"getting_started/installation/#installing-from-source","title":"Installing from source","text":"<p>To install from source, first clone the repository:</p> <pre><code># https\ngit clone https://github.com/VectorInstitute/fed-rag.git\n\n# ssh\ngit clone git@github.com:VectorInstitute/fed-rag.git\n</code></pre> <p>After cloning the repository, you have a few options for installing the library. The next two subsections outline how to complete the installation using either <code>pip</code> or <code>uv</code>, respectively.</p>"},{"location":"getting_started/installation/#using-pip","title":"Using <code>pip</code>","text":"<p>To complete the installation, first <code>cd</code> into the <code>fed-rag</code> directory and then run the following <code>pip install</code> command:</p> <pre><code>cd fed-rag\npip install -e .\n</code></pre> <p>Tip</p> <p>We recommended to always use a fresh virtual environment for new projects. Before running the above command, ensure that your dedicated virtual environment is active.</p>"},{"location":"getting_started/installation/#using-uv","title":"Using <code>uv</code>","text":"<p>FedRAG uses <code>uv</code> for dependency management, publishing to PyPi, and for setting up development environments.</p> <p>Users can also use <code>uv</code> to complete the source installation of FedRAG.</p> <p>Note</p> <p>This method requires <code>uv</code> to be installed onto the users development machine. For installation instructions visit <code>uv</code>'s official documentation.</p> <pre><code>cd fed-rag\nuv sync\n</code></pre> <p>To install with desired extras and groups, add the flags <code>--extra &lt;extra-name&gt;</code> and <code>--optional &lt;optional-name&gt;</code>, respectively. As an example:</p> <pre><code>cd fed-rag\nuv sync --extra huggingface --group dev\n</code></pre>"},{"location":"getting_started/integrations/","title":"Integrations","text":"<p>FedRAG offers integrations with popular frameworks and tools across the RAG ecosystem. This page documents currently supported integrations and our roadmap for future compatibility.</p> <p>Status Legend</p> <p> \u2014 Currently supported;  \u2014 Planned (linked to GitHub issue); Empty \u2014 Not currently planned</p>"},{"location":"getting_started/integrations/#deep-learning-libraries","title":"Deep learning libraries","text":"Framework Status PyTorch Keras TensorFlow Jax"},{"location":"getting_started/integrations/#fine-tuning-frameworks","title":"Fine-tuning frameworks","text":"Framework Status HuggingFace Unsloth"},{"location":"getting_started/integrations/#rag-inference-frameworks","title":"RAG inference frameworks","text":"Framework Status LlamaIndex LangChain Haystack"},{"location":"getting_started/integrations/#knowledge-stores","title":"Knowledge Stores","text":"Storage Solution Status Qdrant ChromaDB FAISS PGVector <p>Contributing Integrations</p> <p>We welcome community contributions for additional integrations. See our CONTRIBUTING guidelines for more information on implementing and submitting new integrations.</p>"},{"location":"getting_started/standard_usage/","title":"Standard Usage","text":"<p>The standard usage pattern for fine-tuning a RAG system with FedRAG follows the below listed steps:</p> <ol> <li>Build a <code>train_dataset</code> that contains examples of (query, response) pairs.</li> <li>Specify a retriever trainer as well as a generator trainer.</li> <li>Construct a RAG trainer manager and invoke the <code>train()</code> method</li> <li>(Optional) Get the associated <code>FLTask</code> <code>RAGTrainerManager.get_federated_task()</code></li> </ol> <p>Info</p> <p>These steps assume that you have already constructed your <code>RAGSystem</code> that you intend to fine-tune.</p> <p>Info</p> <p>The below code snippets require the <code>hugginface</code> extra to be installed, which can be done via a <code>pip install fed-rag[huggingface]</code>.</p>"},{"location":"getting_started/standard_usage/#build-a-train_dataset","title":"Build a <code>train_dataset</code>","text":"<p>For now, all FedRAG trainers deal with datasets that comprise of examples with (query, answer) pairs.</p> Example: a train dataset for HuggingFace<pre><code>from datasets import Dataset\n\ntrain_dataset = Dataset.from_dict(\n    {\n        \"query\": [\"a query\", \"another query\", ...],\n        \"response\": [\n            \"reponse to a query\",\n            \"another response to another query\",\n            ...,\n        ],\n    }\n)\n</code></pre>"},{"location":"getting_started/standard_usage/#specify-a-retriever-and-generator-trainer","title":"Specify a retriever and generator trainer","text":"<p>FedRAG trainer classes bear the responsibility of training the associated retriever or generator on the training dataset. It has an attached data collator that takes a batch of the training dataset and applies the \"forward\" pass of the RAG system (i.e., retrieval from the knowledge store and if required, the subsequent generation step), and returns the <code>~torch.Tensors</code> required for computing the desire loss.</p> <p>These trainer classes take your <code>RAGSystem</code> as input amongst possibly other parameters.</p> Example HuggingFaceTrainers<pre><code>from fed_rag.trainers.huggingface import (\n    HuggingFaceTrainerForRALT,\n    HuggingFaceTrainerForLSR,\n)\n\nretriever_trainer = HuggingFaceTrainerForLSR(rag_system)\ngenerator_trainer = HuggingFaceTrainerForRALT(rag_system)\n</code></pre>"},{"location":"getting_started/standard_usage/#create-a-ragtrainermanager","title":"Create a <code>RAGTrainerManager</code>","text":"<p>The trainer manager class is responsible for orchestrating the training of the RAG system.</p> Example HuggingFaceRAGTrainerManager<pre><code>from fed_rag.trainer_managers.huggingface import HuggingFaceRAGTrainerManager\n\ntrainer_manager = HuggingFaceRAGTrainerManager(\n    mode=\"retriever\",\n    retriever_trainer=retriever_trainer,\n    generator_trainer=generator_trainer,\n)\n\n# train\nresult = trainer_manager.train()\nprint(result.loss)\n</code></pre> <p>Note</p> <p>Alternating training of the retriever and generator can be done by modifying the <code>mode</code> attribute of the manager and calling <code>train()</code>. In the future, the trainer manager will be able to orchestrate between retriever and generator fine-tuning within a single epoch.</p>"},{"location":"getting_started/standard_usage/#optional-get-the-fltask-for-federated-training","title":"(Optional) Get the <code>FLTask</code> for federated training","text":"<p>FedRAG trainer managers offer a simple way to get the associated <code>FLTask</code> for federated fine-tuning.</p> Convert centralized to federated task<pre><code>fl_task = trainer_manager.get_federated_task()  # (1)!\n</code></pre> <ol> <li>This will return an <code>FLTask</code> for either the retriever trainer or the generator trainer task, depending on the <code>mode</code> that the trainer manager is currently set on.</li> </ol>"},{"location":"getting_started/standard_usage/#spin-up-fl-servers-and-clients","title":"Spin up FL servers and clients","text":"<p>With an <code>FLTask</code>, we can obtain an FL server as well as clients. Starting a server and required number of clients will commence the federated training.</p> getting server and clients<pre><code>import flwr as fl  # (1)!\n\n# federate generator fine-tuning\nmodel = rag_system.generator.model\n\n# server\nserver = fl_task.server(model, ...)  # (2)!\n\n# client\nclient = fl_task.client(...)  # (3)!\n\n# the below commands are blocking and would need to be run in separate processes\nfl.server.start_server(server=server, server_address=\"[::]:8080\")\nfl.client.start_client(client=client, server_address=\"[::]:8080\")\n</code></pre> <ol> <li><code>flwr</code> is the backend federated learning framework for FedRAG and comes included with the installation of <code>fed-rag</code>.</li> <li>Can pass in FL aggregation strategy, otherwise defaults to federated averaging.</li> <li>Requires the same arguments as the centralized <code>training_loop</code>.</li> </ol> <p>Note</p> <p>Under the hood, <code>FLTask.server()</code> and <code>FLTask.client()</code> build <code>~flwr.Server</code> and <code>~flwr.Client</code> objects, respectively.</p>"},{"location":"getting_started/advanced_usage/lower_level_ralt/","title":"Low-level RALT Implementation","text":"<p>Here, we demonstrate an alternative manner in which users can perform RALT training that utilizes the lower level API of FedRAG. In particular, rather than working with FedRAG's <code>BaseTrainer</code> and <code>BaseDataCollator</code>.</p> <ol> <li>Build a <code>RAGSystem</code></li> <li>Create a <code>RAGFinetuningDataset</code></li> <li>Define a training loop and evaluation function and decorate both of these with the appropriate <code>decorators</code>.</li> <li>Create an <code>FLTask</code></li> <li>Spin up FL servers and clients to begin federated fine-tuning!</li> </ol> <p>In the following subsections, we briefly elaborate on what's involved in each of these listed steps.</p> <p>Note</p> <p>Steps 1. through 3. are\u2014minus the decoration of trainers and testers\u2014typical steps one would perform for a centralized RAG fine-tuning task.</p> <p>Tip</p> <p>Before proceeding to federated learning, one should verify that the centralized task runs as intended with a representative dataset. In fact, centralized learning represents a standard baseline with which to compare federated learning results.</p>"},{"location":"getting_started/advanced_usage/lower_level_ralt/#build-a-ragsystem","title":"Build a <code>RAGSystem</code>","text":"<p>Building a <code>RAGSystem</code> involves defining a <code>Retriever</code>, <code>KnowledgeStore</code> as well as <code>Generator</code>, and subsequently supplying these along with a <code>RAGConfig</code> (to define parameters such a <code>top_k</code>) to the <code>RAGSystem</code> constructor.</p> building a rag system<pre><code>from fed_rag import RAGSystem, RAGConfig\n\n# three main components\nretriever = ...\nknowledge_store = ...\ngenerator = ...\n\nrag = RAGSystem(\n    generator=generator,\n    retriever=retriever,\n    knowledge_store=knowledge_store,\n    rag_config=RAGConfig(top_k=2),\n)\n</code></pre>"},{"location":"getting_started/advanced_usage/lower_level_ralt/#create-a-ragfinetuningdataset","title":"Create a <code>RAGFinetuningDataset</code>","text":"<p>With a <code>RAGSystem</code> in place, we can create a fine-tuning dataset using examples that contain queries and their associated answers. In retrieval-augmented generator fine-tuning, we process each example by calling the <code>RAGSystem.retrieve()</code> method with the query to fetch relevant knowledge nodes from the connected <code>KnowledgeStore</code>. These contextual nodes enhance each example, creating a collection of retrieval-augmented examples that form the RAG fine-tuning dataset for generator model training. Our how-to guides provide detailed instructions on performing this type of fine-tuning, as well as other approaches.</p> pytorchhuggingface creating a RAG fine-tuning dataset<pre><code>from fed_rag.utils.data import build_finetune_dataset\n\nexamples: list[dict[str, str]] = [{\"query\": ..., \"answer\": ...}, ...]\n\ndataset = build_finetune_dataset(\n    rag_system=rag_system, examples=examples, return_dataset=\"pt\", ...  # (1)!\n)\n</code></pre> <ol> <li>Check the API Reference for the remaining required parameters</li> </ol> creating a RAG fine-tuning dataset<pre><code>from fed_rag.utils.data import build_finetune_dataset\n\nexamples: list[dict[str, str]] = [{\"query\": ..., \"answer\": ...}, ...]\n\ndataset = build_finetune_dataset(\n    rag_system=rag_system, examples=examples, return_dataset=\"hf\", ...  # (1)!\n)\n</code></pre> <ol> <li>Check the API Reference for the remaining required parameters</li> </ol>"},{"location":"getting_started/advanced_usage/lower_level_ralt/#define-a-training-loop-and-evaluation-function","title":"Define a training loop and evaluation function","text":"<p>Like any model training process, a training loop establishes how the model learns from the dataset. Since RAG systems are essentially assemblies of component models (namely retriever and generator), we need to define a specific training loop to effectively learn from RAG fine-tuning datasets.</p> <p>The lift to transform this from a centralized task is to a federated one is minimal with FedRAG, and the first step towards this endeavour amounts to the application of trainer and tester <code>decorators</code> on the respective functions.</p> pytorchhuggingface decorating training loops and evaluation functions<pre><code>from fed_rag.decorators import federate\n\n\n@federate.trainer.pytorch\ndef training_loop():\n    ...\n\n\n@federate.tester.pytorch\ndef evaluate():\n    ...\n</code></pre> decorating training loops and evaluation functions<pre><code>from fed_rag.decorators import federate\n\n\n@federate.trainer.huggingface\ndef training_loop():\n    ...\n\n\n@federate.tester.huggingface\ndef evaluate():\n    ...\n</code></pre> <p>These decorators perform inspection on these functions to automatically parse the model as well as training and validation datasets.</p>"},{"location":"getting_started/advanced_usage/lower_level_ralt/#create-an-fltask","title":"Create an <code>FLTask</code>","text":"<p>The final step in the federation transformation involves building an <code>FLTask</code> using the decorated trainer and evaluation function.</p> pytorchhuggingface defining the FL task<pre><code>from fed_rag.fl_tasks.pytorch import PyTorchFLTask\n\n# use from_trainer_tester class method\nfl_task = PyTorchFLTask.from_trainer_and_tester(\n    trainer=decorated_trainer, tester=decorated_tester  # (1)!\n)\n</code></pre> <ol> <li>decorated with <code>federate.trainer.pytorch</code> and <code>federate.tester.pytorch</code>, respectively</li> </ol> defining the FL task<pre><code>from fed_rag.fl_tasks.huggingface import HuggingFaceFLTask\n\n# use from_trainer_tester class method\nfl_task = HuggingFaceFLTask.from_trainer_and_tester(\n    trainer=decorated_trainer, tester=decorated_tester  # (1)!\n)\n</code></pre> <ol> <li>decorated with <code>federate.trainer.huggingface</code> and <code>federate.tester.huggingface</code>, respectively</li> </ol>"},{"location":"getting_started/advanced_usage/lower_level_ralt/#spin-up-fl-servers-and-clients","title":"Spin up FL servers and clients","text":"<p>With an <code>FLTask</code>, we can obtain an FL server as well as clients. Starting a server and required number of clients will commence the federated training.</p> getting server and clients<pre><code>import flwr as fl  # (1)!\n\n# federate generator fine-tuning\nmodel = rag_system.generator.model\n\n# server\nserver = fl_task.server(model, ...)  # (2)!\n\n# client\nclient = fl_task.client(...)  # (3)!\n\n# the below commands are blocking and would need to be run in separate processes\nfl.server.start_server(server=server, server_address=\"[::]:8080\")\nfl.client.start_client(client=client, server_address=\"[::]:8080\")\n</code></pre> <ol> <li><code>flwr</code> is the backend federated learning framework for FedRAG and comes included with the installation of <code>fed-rag</code>.</li> <li>Can pass in FL aggregation strategy, otherwise defaults to federated averaging.</li> <li>Requires the same arguments as the centralized <code>training_loop</code>.</li> </ol> <p>Note</p> <p>Under the hood, <code>FLTask.server()</code> and <code>FLTask.client()</code> build <code>~flwr.Server</code> and <code>~flwr.Client</code> objects, respectively.</p>"},{"location":"getting_started/quick_starts/","title":"Quick Starts","text":"<p>In this next part in getting to know FedRAG, we provide a mini series of quick start examples in order to get a better feeling of the library.</p> <ul> <li> Centralized to Federated \u2014 Transform   a centralized training task into a federated learning task.</li> <li> Build a RAG System \u2014 Assemble   a RAG system using FedRAG's lightweight abstractions.</li> <li> Fine-tune a RAG System \u2014 Fine-tune   a RAG system on custom QA data, demonstrating both centralized training and   optional federation capabilities.</li> <li> Benchmark a RAG System \u2014   Evaluate a RAG system on popular benchmarks like MMLU.</li> </ul>"},{"location":"getting_started/quick_starts/benchmark_mmlu/","title":"Benchmark a RAG System","text":"<p>In this quick start guide, we'll demonstrate how to leverage the <code>evals</code> module within the <code>fed-rag</code> library to benchmark your <code>RAGSystem</code>. For conciseness, we won't cover the detailed process of assembling a <code>RAGSystem</code> here\u2014please refer to our other quick start guides for comprehensive instructions on system assembly.</p>"},{"location":"getting_started/quick_starts/benchmark_mmlu/#the-benchmarker-class","title":"The <code>Benchmarker</code> Class","text":"<p>Within the <code>evals</code> module is a core class called <code>Benchmarker</code>. It bears the responsibility of running a benchmark for your <code>RAGSystem</code>.</p> Creating a benchmarker<pre><code>from fed_rag.evals import Benchmarker\n\nbenchmarker = Benchmarker(rag_system=rag_system)  # (1)!\n</code></pre> <ol> <li>Your previously assembled <code>RAGSystem</code></li> </ol>"},{"location":"getting_started/quick_starts/benchmark_mmlu/#importing-a-benchmark-to-run","title":"Importing a <code>Benchmark</code> to Run","text":"<p>The <code>evals</code> module contains various benchmarks that can be used to evaluate a <code>RAGSystem</code>. A FedRAG benchmark contains <code>BenchmarkExamples</code> that carry the query, response, and context for any given example.</p> <p>Inspired by how datasets are imported from familiar libraries like <code>torchvision</code>, the benchmarks can be imported as follows:</p> Importing the benchmarks module<pre><code>import fed_rag.evals.benchmarks as benchmarks\n</code></pre> <p>From here, we can choose to use any of the defined benchmarks! The snippet below makes use of the <code>HuggingFaceMMLU</code> benchmark.</p> Using a supported benchmark<pre><code>mmlu = benchmarks.HuggingFaceMMLU(streaming=True)  # (1)!\n\n# get the example stream\nexamples_stream = mmlu.as_stream()\nprint(next(examples_stream))  # will yield the next BenchmarkExample for MMLU\n</code></pre> <ol> <li>The HuggingFace benchmarks integration supports the underlying streaming mechanism of <code>~datasets.Dataset</code>.</li> </ol> <p>Info</p> <p>Using a HuggingFace supported benchmark, requires the <code>huggingface-evals</code> extra. This can be installed via <code>pip-install fed-rag[huggingface-evals]</code>. Note that the more comprehensive <code>huggingface</code> extra also includes all necessary packages for <code>huggingface-evals</code>.</p>"},{"location":"getting_started/quick_starts/benchmark_mmlu/#choosing-your-evaluation-metric","title":"Choosing your Evaluation Metric","text":"<p>To run a benchmark, you must also supply a <code>EvaluationMetric</code>. The code snippet below imports the <code>ExactMatchEvaluationMetric</code>.</p> Defining an evaluation metric<pre><code>from fed_rag.evals.metrics import ExactMatchEvaluationMetric\n\nmetric = ExactMatchEvaluationMetric()\n\n# using the metric\nmetric(prediction=\"A\", acutal=\"a\")  # case in-sensitive returns 1.0\n</code></pre> <p>Info</p> <p>All subclasses of <code>BaseEvaluationMetric</code>, like <code>ExactMatchEvaluationMetric</code> are callable. We can see the signature of this method by using the help builtin i.e., <code>help(metric.__call__)</code>.</p>"},{"location":"getting_started/quick_starts/benchmark_mmlu/#running-the-benchmark","title":"Running the Benchmark","text":"<p>We now have all the elements in place in order to run the benchmark. To do so, we invoke the <code>run()</code> method of the <code>Benchmarker</code> object, passing in the elements we defined in previous sections.</p> Running the chose benchmark with specific metric<pre><code>result = benchmark.run(\n    benchmark=mmlu,\n    metric=metric,\n    is_streaming=True,\n    num_examples=3,  # (1)!\n    agg=\"avg\",  # (2)!\n)\n\nprint(result)\n</code></pre> <ol> <li>(Optional) useful for rapid testing of your benchmark rig.</li> <li>Can be 'avg', 'sum', 'max', 'min', see <code>AggregationMode</code></li> </ol> <p>A successful run of a benchmark will result in a <code>BenchmarkResult</code> object that contains summary information about the benchmark including the final aggregated score, the number of examples used, as well as the total number of examples that the benchmark contains.</p>"},{"location":"getting_started/quick_starts/federated/","title":"Centralized to Federated","text":"<p>In this quick start example, we'll demonstrate how to easily transform a centralized training task into a federated one with just a few additional lines of code.</p> <p>Let's start by installing the <code>fed-rag</code> library by using <code>pip</code>:</p> <pre><code>pip install fed-rag\n</code></pre>"},{"location":"getting_started/quick_starts/federated/#defining-the-centralized-task","title":"Defining the centralized task","text":"<p>As with any model training endeavour, we define the model, its training loop, and finally a function for evaluations. Experienced model builders will find this workflow comfortably familiar, as FedRAG maintains the same essential structure they're accustomed to while seamlessly introducing federation capabilities (as we will see shortly in the next sub section).</p>"},{"location":"getting_started/quick_starts/federated/#model","title":"Model","text":"<p>We define a simple multi-layer perceptron as our model.</p> the model<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# the model\nclass Net(torch.nn.Module):\n    def __init__(self) -&gt; None:\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(42, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n</code></pre> <p>Note</p> <p>FedRAG uses PyTorch as its main deep learning framework and so installing <code>fed-rag</code> also comes with the <code>torch</code> library.</p>"},{"location":"getting_started/quick_starts/federated/#training-loop","title":"Training loop","text":"<p>We use a standard training loop that is PyTorch native, making use of the <code>~torch.utils.data.DataLoader</code> class.</p> training loop<pre><code>...  # (1)!\nfrom torch.types import Device\nfrom torch.utils.data import DataLoader\nfrom fed_rag.data_structures import TestResult\n\n\ndef train_loop(\n    model: torch.nn.Module,\n    train_data: DataLoader,\n    val_data: DataLoader,\n    device: Device,\n    num_epochs: int,\n    learning_rate: float | None,\n) -&gt; TrainResult:\n    \"\"\"My custom train loop.\"\"\"\n\n    model.to(device)  # move model to GPU if available\n    criterion = torch.nn.CrossEntropyLoss().to(device)\n    optimizer = torch.optim.SGD(\n        model.parameters(), lr=learning_rate, momentum=0.9\n    )\n    model.train()\n    running_loss = 0.0\n    for _ in range(num_epochs):\n        for batch in train_data:\n            features = batch[\"features\"]\n            labels = batch[\"label\"]\n            optimizer.zero_grad()\n            loss = criterion(model(features.to(device)), labels.to(device))\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n    avg_trainloss = running_loss / len(train_data)\n    return TrainResult(loss=avg_trainloss)\n</code></pre> <ol> <li>Includes the import statements from the previous code block.</li> </ol>"},{"location":"getting_started/quick_starts/federated/#evaluation-function","title":"Evaluation function","text":"<p>Finally, a typical evaluation function that computes the accuracy of the model.</p> evaluation function<pre><code>...  # (1)!\nfrom fed_rag.data_structures import TestResult\n\n\ndef test(m: torch.nn.Module, test_loader: DataLoader) -&gt; TestResult:\n    \"\"\"My custom tester.\"\"\"\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    m.to(device)\n    criterion = torch.nn.CrossEntropyLoss()\n    correct, loss = 0, 0.0\n    with torch.no_grad():\n        for batch in test_loader:\n            features = batch[\"features\"].to(device)\n            labels = batch[\"label\"].to(device)\n            outputs = m(images)\n            loss += criterion(outputs, labels).item()\n            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n    accuracy = correct / len(test_loader.dataset)\n    return TestResult(loss=loss, metrics={\"accuracy\": accuracy})\n</code></pre> <ol> <li>Includes the import statements from the two previous code blocks.</li> </ol>"},{"location":"getting_started/quick_starts/federated/#centralized-training","title":"Centralized training","text":"<p>Training the model under the centralized setting is a simple matter of instantiating a model and invoking the <code>train()</code> loop.</p> Centralized training<pre><code>train_data = ...  # (1)!\nval_data = ...  # (2)!\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = Net()\ntrain(\n    model=model,\n    train_data=train_data,\n    val_data=val_data,\n    device=device,\n    num_epochs=3,\n    learning_rate=0.1,\n)\n</code></pre> <ol> <li>Pass in a train data loader.</li> <li>Pass in a validation data loader.</li> </ol>"},{"location":"getting_started/quick_starts/federated/#federating-the-centralized-task","title":"Federating the centralized task","text":"<p>In this section, we demonstrate how we can take the centralized task above, sensibly represented by the triple (model, trainer, tester) where trainer is the training loop, and tester is the evaluation function.</p>"},{"location":"getting_started/quick_starts/federated/#defining-the-fl-task","title":"Defining the FL task","text":"<p>The code block below shows how to define a <code>PyTorchFLTask</code> from the centralized trainer and tester functions, but not before automatically performing some required inspection on them.</p> Getting an FL Task<pre><code>from fed_rag.decorators import federate\nfrom fed_rag.fl_tasks.pytorch import PyTorchFLTask\n\n\n# apply decorators on the previously established trainer\ntrain_loop = federate.trainer.pytorch(train_loop)  # (1)!\ntest = federate.tester.pytorch(test)  # (2)!\n\n# define the fl task\nfl_task = PyTorchFLTask.from_trainer_and_tester(\n    trainer=train_loop, tester=test\n)\n</code></pre> <ol> <li><code>train_loop</code> as defined in the training loop code block.</li> <li><code>test</code> as defined in the evaluation function code block.</li> </ol> <p>Note</p> <p><code>federate.trainer.pytorch</code> and <code>federate.tester.pytorch</code> are both decorators and could have been incorporated in the training loop and evaluation function code blocks, respectively. We separated them here to clearly demonstrate the progression from centralized to federated implementation, making the transformation process more explicit. In typical usage, you would apply these decorators directly to your functions when defining them.</p>"},{"location":"getting_started/quick_starts/federated/#getting-a-server-and-clients","title":"Getting a server and clients","text":"<p>With the <code>FLTask</code> in hand, we can create a server and some clients in order to establish a federated network.</p> Getting a server and two clients<pre><code># the server\nmodel = Net()\nserver = fl_task.server(model=model)\n\n# defining two clients\nclients = []\nfor i in range(2):\n    train_data, val_data = get_loaders(partition_id=i)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    num_epochs = 1\n    learning_rate = 0.1\n\n    client = fl_task.client(\n        # train params\n        model=model,\n        train_data=train_data,\n        val_data=val_data,\n        device=device,\n        num_epochs=num_epochs,\n        learning_rate=learning_rate,\n    )\n    clients.append(client)\n</code></pre>"},{"location":"getting_started/quick_starts/federated/#federated-training","title":"Federated training","text":"<p>To perform the training, we simply need to start the servers and clients!</p> Starting the server and clients<pre><code>import flwr as fl\n\n# the below commands are blocking and would need to be run in separate processes\nfl.server.start_server(server=server, server_address=\"[::]:8080\")\nfl.client.start_client(client=clients[0], server_address=\"[::]:8080\")\nfl.client.start_client(client=clients[1], server_address=\"[::]:8080\")\n</code></pre> <p>Note</p> <p>FedRAG uses the <code>flwr</code> library as its backend federated learning framework, and like <code>torch</code>, comes bundled with installation of <code>fed-rag</code>.</p>"},{"location":"getting_started/quick_starts/rag_finetuning/","title":"Fine-tune a RAG System","text":"<p>In this quick start, we'll go over how we can take the RAG system we built from the previous quick start example, and fine-tune it.</p> <p>Note</p> <p>For this fine-tuning tutorial, you'll need the <code>huggingface</code> extra installed. If you haven't added it yet, run:</p> <p><code>pip install fed-rag[huggingface]</code></p> <p>This provides access to the HuggingFace models and training utilities we'll use for both retriever and generator fine-tuning.</p>"},{"location":"getting_started/quick_starts/rag_finetuning/#the-train-dataset","title":"The Train Dataset","text":"<p>Training a RAG system requires a train dataset that is familiarly shaped as a question-answering dataset.</p> training examples for RAG fine-tuning<pre><code>from datasets import Dataset\n\ntrain_dataset = Dataset.from_dict(  # (1)!\n    {\n        \"query\": [\n            \"What is machine learning?\",\n            \"Tell me about climate change\",\n            \"How do computers work?\",\n        ],\n        \"response\": [\n            \"Machine learning is a field of AI focused on algorithms that learn from data.\",\n            \"Climate change refers to long-term shifts in temperatures and weather patterns.\",\n            \"Computers work by processing information using logic gates and electronic components.\",\n        ],\n    }\n)\n</code></pre> <ol> <li>A train example is essentially a (<code>query</code>, <code>response</code>) pair.</li> </ol>"},{"location":"getting_started/quick_starts/rag_finetuning/#define-our-trainer-objects","title":"Define our Trainer objects","text":"<p>To perform RAG fine-tuning, FedRAG offers both a <code>BaseGeneratorTrainer</code> and a <code>BaseRetrieverTrainer</code> that incorporate the training logic for each of these respective RAG components.</p> <p>For this quick start, we make use of the following trainers:</p> <ul> <li><code>HuggingFaceTrainerForRALT</code> \u2014 A   generator trainer that fine-tunes the LLM using retrieval-augmented instruction   examples.</li> <li><code>HuggingFaceTrainerForLSR</code> \u2014 A   retriever trainer that fine-tunes the retriever model using retrieval chunk scores   and the log probabilities derived from the generator LLM using the ground truth   response.</li> </ul> retrieval-augmented fine-tuning<pre><code>from fed_rag.trainers.huggingface.ralt import HuggingFaceTrainerForRALT\nfrom fed_rag.trainers.huggingface.lsr import HuggingFaceTrainerForLSR\n\n\nrag_system = ...  # from previous quick start\ngenerator_trainer = HuggingFaceTrainerForRALT(\n    rag_system=rag_system,\n    train_dataset=train_dataset,\n)\nretriever_trainer = HuggingFaceTrainerForLSR(\n    rag_system=rag_system,\n    train_dataset=train_dataset,\n)\n</code></pre>"},{"location":"getting_started/quick_starts/rag_finetuning/#define-our-trainer-manager-object","title":"Define our Trainer Manager object","text":"<p>To orchestrate training between the two RAG components, FedRAG offers a manager class called <code>BaseRAGTrainerManager</code>. The training manager contains logic to prepare the component and system for the specific training task (i.e., retriever or generator), and also contains a simple method to transform the task into a federated one.</p> training with managers<pre><code>from fed_rag.trainer_managers.huggingface import HuggingFaceRAGTrainerManager\n\nmanager = HuggingFaceRAGTrainerManager(\n    mode=\"retriever\",  # (1)!\n    retriever_trainer=retriever_trainer,\n    generator_trainer=generator_trainer,\n)\ntrain_result = manager.train()\nprint(f\"loss: {train_result.loss}\")\n\n# get your federated learning task (optional)\nfl_task = manager.get_federated_task()\n</code></pre> <ol> <li>Mode can be \"retriever\" or \"generator\"\u2014see <code>RAGTrainMode</code></li> </ol>"},{"location":"getting_started/quick_starts/rag_inference/","title":"Build a RAG System","text":"<p>In this quick start example, we'll demonstrate how to build a RAG system and subequently query it using FedRAG. We begin with a short primer on the components of RAG which mirror the abstractions that FedRAG uses to build such systems.</p>"},{"location":"getting_started/quick_starts/rag_inference/#components-of-rag","title":"Components of RAG","text":"The three main components of RAG. The three main components of RAG. <p>A RAG system is comprised of three main components, namely:</p> <ul> <li>Knowledge Store \u2014 contains non-parametric knowledge facts that the system   can use at inference time in order to produce more accurate responses to queries.</li> <li>Retriever \u2014 a model that takes in a user query and retrieves the most relevant   knowledge facts from the knowledge store.</li> <li>Generator \u2014 a model that takes in the user's query and additional context   and provides a response to that query.</li> </ul> <p>Info</p> <p>The Retriever is also used to populate (i.e index) the Knowledge Store during setup.</p>"},{"location":"getting_started/quick_starts/rag_inference/#building-a-rag-system","title":"Building a RAG system","text":"<p>We'll install FedRAG with the <code>huggingface</code> extra this time in order to build our RAG system using HuggingFace models.</p> <pre><code>pip install \"fed-rag[huggingface]\"\n</code></pre>"},{"location":"getting_started/quick_starts/rag_inference/#retriever","title":"Retriever","text":"<p>With the <code>huggingface</code> extra, we can use any <code>~sentence_transformers.SentenceTransfomer</code> as the retriever model, including dual encoders like the one used below. This HuggingFace encoder is used to define a <code>HFSentenceTransformerRetriever</code>.</p> retriever<pre><code>from fed_rag.retrievers.huggingface.hf_sentence_transformer import (\n    HFSentenceTransformerRetriever,\n)\n\nQUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\"\nCONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\"\n\nretriever = HFSentenceTransformerRetriever(\n    query_model_name=QUERY_ENCODER_NAME,\n    context_model_name=CONTEXT_ENCODER_NAME,\n    load_model_at_init=False,\n)\n</code></pre>"},{"location":"getting_started/quick_starts/rag_inference/#knowledge-store","title":"Knowledge Store","text":"<p>To create a knowledge store, we've create a toy set of only two knowledge artifacts that we'll encode and subsequently load into an <code>InMemoryKnowledgeStore</code>.</p> knowledge artifacts<pre><code>import json\n\n# knowledge chunks\nchunks_json_strs = [\n    '{\"id\": \"0\", \"title\": \"Orchid\", \"text\": \"Orchids are easily distinguished from other plants, as they share some very evident derived characteristics or synapomorphies. Among these are: bilateral symmetry of the flower (zygomorphism), many resupinate flowers, a nearly always highly modified petal (labellum), fused stamens and carpels, and extremely small seeds\"}',\n    '{\"id\": \"1\", \"title\": \"Tulip\", \"text\": \"Tulips are easily distinguished from other plants, as they share some very evident derived characteristics or synapomorphies. Among these are: bilateral symmetry of the flower (zygomorphism), many resupinate flowers, a nearly always highly modified petal (labellum), fused stamens and carpels, and extremely small seeds\"}',\n]\nchunks = [json.loads(line) for line in chunks_json_strs]\n</code></pre> knowledge store<pre><code>from fed_rag.knowledge_stores.in_memory import InMemoryKnowledgeStore\nfrom fed_rag.data_structures import KnowledgeNode, NodeType\n\nknowledge_store = InMemoryKnowledgeStore()\n\n# create knowledge nodes\nnodes = []\nfor c in chunks:\n    node = KnowledgeNode(\n        embedding=retriever.encode_context(c[\"text\"]).tolist(),\n        node_type=NodeType.TEXT,\n        text_content=c[\"text\"],\n        metadata={\"title\": c[\"title\"], \"id\": c[\"id\"]},\n    )\n    nodes.append(node)\n\n# load into knowledge_store\nknowledge_store.load_nodes(nodes=nodes)\n</code></pre>"},{"location":"getting_started/quick_starts/rag_inference/#generator","title":"Generator","text":"<p>With the <code>huggingface</code> extra installed, we can use any <code>~transformers.PreTrainedModel</code> as well as any <code>~peft.PeftModel</code>. For this example, we use the latter and define a HFPeftModelGenerator.</p> generator<pre><code>from fed_rag.generators.huggingface import HFPeftModelGenerator\nfrom transformers.generation.utils import GenerationConfig\nfrom transformers.utils.quantization_config import BitsAndBytesConfig\n\nPEFT_MODEL_NAME = \"Styxxxx/llama2_7b_lora-quac\"\nBASE_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n\ngeneration_cfg = GenerationConfig(\n    do_sample=True,\n    eos_token_id=[128000, 128009],\n    bos_token_id=128000,\n    max_new_tokens=4096,\n    top_p=0.9,\n    temperature=0.6,\n    cache_implementation=\"offloaded\",\n    stop_strings=\"&lt;/response&gt;\",\n)\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ngenerator = HFPeftModelGenerator(\n    model_name=PEFT_MODEL_NAME,\n    base_model_name=BASE_MODEL_NAME,\n    generation_config=generation_cfg,\n    load_model_at_init=False,\n    load_model_kwargs={\"is_trainable\": True, \"device_map\": \"auto\"},\n    load_base_model_kwargs={\n        \"device_map\": \"auto\",\n        \"quantization_config\": quantization_config,\n    },\n)\n</code></pre>"},{"location":"getting_started/quick_starts/rag_inference/#rag-system","title":"RAG System","text":"<p>Finally, with our three main components in hand, we can build our first <code>RAGSystem</code>!</p> RAG system<pre><code>from fed_rag import RAGConfig, RAGSystem\n\nrag_config = RAGConfig(top_k=2)\nrag_system = RAGSystem(\n    knowledge_store=knowledge_store,\n    generator=generator,\n    retriever=retriever,\n    rag_config=rag_config,\n)\n</code></pre> querying our RAGSystem<pre><code># query the rag system\nresponse = rag_system.query(\"What is a Tulip?\")\n\nprint(f\"\\n{response}\")\n\n# inspect source nodes\nprint(response.source_nodes)\n</code></pre>"},{"location":"getting_started/quick_starts/rag_inference/#whats-next","title":"What's next?","text":"<p>Having learned how to build a RAG system, let's move on to learning how to fine-tune one!</p>"},{"location":"getting_started/tutorials/","title":"Tutorials","text":"<p>We've prepared the following tutorials on some of the core concepts and methods that underpin the important features and capabilities of FedRAG.</p> <ul> <li> LSR Fine-tuning \u2014 A tutorial on the   LM-Supervised Retriever (LSR) fine-tuning method.</li> <li> RALT Fine-tuning \u2014 A tutorial on the   Retriever-Augmented LM Training (RALT) method.</li> </ul>"},{"location":"getting_started/tutorials/lsr/","title":"Retriever Fine-Tuning with LSR","text":"<p>RAG systems integrate three key components: a knowledge store, a retriever model, and a generator model. Effective fine-tuning approaches should enhance not just individual components, but the cohesive performance of the entire system.</p> <p>This tutorial focuses on the LM-Supervised Retriever (LSR) fine-tuning method, which was first introduced in Shi, Weijia et al. (2023)<sup>1</sup> and later generalized in Lin, Xi Victoria et al. (2023)<sup>2</sup>. With this method the retriever is fine-tuned by leveraging signals from the language model (i.e., generator).</p>"},{"location":"getting_started/tutorials/lsr/#the-lsr-method","title":"The LSR Method","text":"<p>The LSR method is applied over a training dataset of (query, response) pairs. It involves the computation of two probability distributions for every pair, namely: a probability distribution derived the from the retrieval scores of each of the retrieved knowledge nodes, and a probability distribution derived from the LLM generator conditioned on prompt and each of the knowledge node's context.</p> <p>Mathematically, let \\((q, r)\\) represent the query and response, respectively of a given training example. Moreover let \\(c_i\\) represent the context from the \\(i\\)-th knowledge node retrieved by the RAG system for query, \\(q\\).</p>"},{"location":"getting_started/tutorials/lsr/#retrieval-probabilities","title":"Retrieval probabilities","text":"<p>The retrieval probability distribution is defined by applying the softmax function to the retrieval scores:</p> \\[ p_{R}(c_i|q) = \\frac{\\exp s(q, c_i)}{\\sum_{j=1}^k \\exp s(q, c_j)}, \\quad i=1,\\ldots,k, \\] <p>where \\(s(q, c_i)\\) represents the similarity score between query, \\(q\\) and context, \\(c_i\\).</p>"},{"location":"getting_started/tutorials/lsr/#generation-llm-probabilities","title":"Generation (LLM) probabilities","text":"<p>Similarly, the probabilities derived from the LLM involves another application of the softmax function, but this time over the probabilities that the LLM generator produces the ground-truth response, \\(r\\) when given the input sequence of \\(q \\circ c_i\\) for \\(i=1,\\ldots,k\\), where \"\\(\\circ\\)\" is the concatenation operator. That is,</p> \\[ p_{LSR}(c_i|q,r) = \\frac{\\exp (p_{LLM}(r| q \\circ c_i)/\\tau)}{\\sum_{j=1}^k \\exp(p_{LLM}(r | q \\circ c_j)/\\tau)}, \\quad i=1,\\ldots,k, \\] <p>where \\(\\tau\\) is a temperature hyperparameter.</p>"},{"location":"getting_started/tutorials/lsr/#lsr-training-objective","title":"LSR training objective","text":"<p>The LSR loss is defined as the Kullback-Liebler divergence between these two probability distributions:</p> \\[ \\mathcal{L}_{LSR} = \\mathbb{E}_{(q,r)\\in\\mathcal{D}_{\\text{train}}} KL\\big(p_{R}(c|q)\\|p_{LSR}(c|q,r)\\big), \\] <p>where \\(\\mathbb{E}_{{(q,r)\\in\\mathcal{D}_{\\text{train}}}}(\\cdot)\\) is the expectation operator under the distribution defined by the (query, response) pairs from the training dataset \\(\\mathcal{D}_{\\text{train}}\\).</p> <p>In minimizing the LSR loss, we are adapting the retriever model to assign higher scores to the knowledge nodes that increase the generator's likelihood of producing the ground-truth response, \\(r\\).</p>"},{"location":"getting_started/tutorials/lsr/#notes-on-the-fedrag-implementation-of-lsr","title":"Notes on the FedRAG Implementation of LSR","text":"<p>In FedRAG, we implement the LSR method with the typical coordination of a data collator and a trainer. The <code>DataCollatorForLSR</code> takes a batch of (query, response) pairs and produces the PyTorch tensors for retrieval scores as well as LLM scores for each knowledge node retrieved by the <code>RAGSystem</code>. This batch is then used by the <code>TrainerForLSR</code> which computes the <code>LSRLoss</code> for the batch and then performs the optimization step for the given training step.</p> <ol> <li> <p>Shi, Weijia, et al. \"Replug: Retrieval-augmented black-box language models.\"   arXiv preprint arXiv:2301.12652 (2023).\u00a0\u21a9</p> </li> <li> <p>Lin, Xi Victoria, et al. \"Ra-dit: Retrieval-augmented dual instruction tuning.\"   The Twelfth International Conference on Learning Representations. 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"getting_started/tutorials/ralt/","title":"Generator Fine-Tuning with RALT","text":"<p>The overall success of RAG systems depends heavily on the generator model's ability to effectively utilize retrieved knowledge. This capability is crucial because LLM generators are typically trained on corpora that differ from the sources populating the knowledge store. As a result, generators may struggle to properly integrate or reason with retrieved information that contains domain-specific terminology, formatting, or content structures.</p> <p>Retriever-Augmented LM Training (RALT), introduced by Lin et al. (2023)<sup>1</sup>, addresses this challenge by fine-tuning the generator model specifically on examples that incorporate retrieved knowledge nodes. This process helps the generator learn to better contextualize, interpret, and integrate information from the knowledge store into its responses (and, even learn to ignore it when it deems it irrelevant to the original query).</p>"},{"location":"getting_started/tutorials/ralt/#the-ralt-method","title":"The RALT Method","text":"<p>Like the LSR method, the RALT method is also applied over a training dataset (query, response) pairs. For each training example, we first retrieve the top-\\(k\\) knowledge nodes, and then create \\(k\\) independent instruction fine-tuning examples. The instruction fine-tuning template involves placeholders for the query, response, and the knowledge nodes' content (i.e., context for the query).</p> an example instruction template<pre><code>instruction_template = \"\"\"You are a helpful assistant. Given the user query below,\nprovide a response making use of the provided background context.\n\n&lt;query&gt;\n{query}\n&lt;/query&gt;\n\n&lt;context&gt;\n{context}\n&lt;/context&gt;\n\n&lt;response&gt;\n{response}\n&lt;/response&gt;\n\"\"\"\n</code></pre>"},{"location":"getting_started/tutorials/ralt/#ralt-training-objective","title":"RALT training objective","text":"<p>For RALT, we apply the usual masked causal language modelling task, which trains the model to predict the next token given the previously seen tokens. Mathematically, if we let \\(\\{(q_i, r_i)\\}_{i=1}^N\\) represent the training dataset of (query, response), pairs, and further, let \\(c_{i,j}\\) represent the context from the \\(j\\)-th knowledge node retrieved by the RAG system for query, \\(q_i\\), then we can write the RALT loss as follows:</p> \\[ \\mathcal{L}_{RALT} = - \\sum_{i}^N\\sum_{j}^k \\log p_{LLM}(r_i | q_i \\circ c_{i,j}), \\] <p>where \\(\\log p_{LLM}(r_i | q_i \\circ c_{i,j})\\) is the log probability that response, \\(r_i\\), is produced by the LLM given the input sequence \\(q_i \\circ c_{i,j}\\) (with \"\\(\\circ\\)\" representing concatenation).</p>"},{"location":"getting_started/tutorials/ralt/#notes-on-the-fedrag-implementation-of-ralt","title":"Notes on the FedRAG Implementation of RALT","text":"<p>The RALT implementation in FedRAG involves the typical coordination between a data collator and a trainer object. The <code>DataCollatorForRALT</code> takes on the responsibility of retrieving the \\(k\\) nodes for every query in the batch, and creating the \\(k\\) instruction-tuning instances. Tokenization and padding are also applied in the data collator. The <code>TrainerForRALT</code> then performs the causal language modelling training on the collated data for the generator model.</p> <p>Note</p> <p>An alternative implementation would be to pass the creation of the instruction instances from the data collator to a pre-processing step that creates a training dataset. In other words, the train dataset is not the (query, response) pair, but an already processed instruction fine-tuning dataset. For unification purposes, the former was chosen to promote consistency between retriever and generator trainer workflows.</p> <ol> <li> <p>Lin, Xi Victoria, et al. \"Ra-dit: Retrieval-augmented dual instruction tuning.\"   The Twelfth International Conference on Learning Representations. 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"notebooks/basic_starter_hf/","title":"Basic Starter Example","text":"<p>(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)</p> In\u00a0[\u00a0]: Copied! <pre># If running in a Google Colab, the first attempt at installing fed-rag may fail,\n# though for reasons unknown to me yet, if you try a second time, it magically works...\n!pip install fed-rag[huggingface] -q\n</pre> # If running in a Google Colab, the first attempt at installing fed-rag may fail, # though for reasons unknown to me yet, if you try a second time, it magically works... !pip install fed-rag[huggingface] -q In\u00a0[\u00a0]: Copied! <pre>from fed_rag.knowledge_stores.in_memory import InMemoryKnowledgeStore\nfrom fed_rag.retrievers.huggingface.hf_sentence_transformer import (\n    HFSentenceTransformerRetriever,\n)\n</pre> from fed_rag.knowledge_stores.in_memory import InMemoryKnowledgeStore from fed_rag.retrievers.huggingface.hf_sentence_transformer import (     HFSentenceTransformerRetriever, ) In\u00a0[\u00a0]: Copied! <pre>knowledge_store = InMemoryKnowledgeStore()\n\nretriever = HFSentenceTransformerRetriever(\n    query_model_name=\"nthakur/dragon-plus-query-encoder\",\n    context_model_name=\"nthakur/dragon-plus-context-encoder\",\n    load_model_at_init=False,\n)\n</pre> knowledge_store = InMemoryKnowledgeStore()  retriever = HFSentenceTransformerRetriever(     query_model_name=\"nthakur/dragon-plus-query-encoder\",     context_model_name=\"nthakur/dragon-plus-context-encoder\",     load_model_at_init=False, ) In\u00a0[\u00a0]: Copied! <pre># a small sample from the Dec 2021 Wikipedia dump\ntext_chunks = [\n    {\n        \"id\": \"140\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"James Cook\",\n        \"text\": \" James Cook is well known for his voyages of exploration for the British Navy in which he mapped out a significant amount of the world's uncharted waters. Cook's explorations took him around the world twice and led to countless descriptions of previously unknown plants and animals. Cook's explorations influenced many others and led to a number of scientists examining marine life more closely. Among those influenced was Charles Darwin who went on to make many contributions of his own. \",\n    },\n    {\n        \"id\": \"141\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Charles Darwin\",\n        \"text\": \" Charles Darwin, best known for his theory of evolution, made many significant contributions to the early study of marine biology. He spent much of his time from 1831 to 1836 on the voyage of HMS Beagle collecting and studying specimens from a variety of marine organisms. It was also on this expedition where Darwin began to study coral reefs and their formation. He came up with the theory that the overall growth of corals is a balance between the growth of corals upward and the sinking of the sea floor. He then came up with the idea that wherever coral atolls would be found, the central island where the coral had started to grow would be gradually subsiding\",\n    },\n    {\n        \"id\": \"142\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Charles Wyville Thomson\",\n        \"text\": \" Another influential expedition was the voyage of HMS Challenger from 1872 to 1876, organized and later led by Charles Wyville Thomson. It was the first expedition purely devoted to marine science. The expedition collected and analyzed thousands of marine specimens, laying the foundation for present knowledge about life near the deep-sea floor. The findings from the expedition were a summary of the known natural, physical and chemical ocean science to that time.\",\n    },\n    {\n        \"id\": \"143\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Later exploration\",\n        \"text\": \" This era of marine exploration came to a close with the first and second round-the-world voyages of the Danish Galathea expeditions and Atlantic voyages by the USS Albatross, the first research vessel purpose built for marine research. These voyages further cleared the way for modern marine biology by building a base of knowledge about marine biology. This was followed by the progressive development of more advanced technologies which began to allow more extensive explorations of ocean depths that were once thought too deep to sustain life.\",\n    },\n    {\n        \"id\": \"144\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Marine biology labs\",\n        \"text\": \" In the 1960s and 1970s, ecological research into the life of the ocean was undertaken at institutions set up specifically to study marine biology. Notable was the Woods Hole Oceanographic Institution in America, which established a model for other marine laboratories subsequently set up around the world. Their findings of unexpectedly high species diversity in places thought to be inhabitable stimulated much theorizing by population ecologists on how high diversification could be maintained in such a food-poor and seemingly hostile environment. \",\n    },\n    {\n        \"id\": \"145\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Exploration technology\",\n        \"text\": \" In the past, the study of marine biology has been limited by a lack of technology as researchers could only go so deep to examine life in the ocean. Before the mid-twentieth century, the deep-sea bottom could not be seen unless one dredged a piece of it and brought it to the surface. This has changed dramatically due to the development of new technologies in both the laboratory and the open sea. These new technological developments have allowed scientists to explore parts of the ocean they didn't even know existed. The development of scuba gear allowed researchers to visually explore the oceans as it contains a self-contained underwater breathing apparatus allowing a person to breathe while being submerged 100 to 200 feet \",\n    },\n    {\n        \"id\": \"146\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Exploration technology\",\n        \"text\": \" the ocean. Submersibles were built like small submarines with the purpose of taking marine scientists to deeper depths of the ocean while protecting them from increasing atmospheric pressures that cause complications deep under water. The first models could hold several individuals and allowed limited visibility but enabled marine biologists to see and photograph the deeper portions of the oceans. Remotely operated underwater vehicles are now used with and without submersibles to see the deepest areas of the ocean that would be too dangerous for humans. ROVs are fully equipped with cameras and sampling equipment which allows researchers to see and control everything the vehicle does. ROVs have become the dominant type of technology used to view the deepest parts of the ocean.\",\n    },\n    {\n        \"id\": \"147\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Romanticization\",\n        \"text\": ' In the late 20th century and into the 21st, marine biology was \"glorified and romanticized through films and television shows,\" leading to an influx in interested students who required a damping on their enthusiasm with the day-to-day realities of the field.',\n    },\n    {\n        \"id\": \"148\",\n        \"title\": \"Wynthryth\",\n        \"section\": \"\",\n        \"text\": \" Wynthryth of March was an early medieval saint of Anglo Saxon England. He is known to history from the Secgan Hagiography and The Confraternity Book of  St Gallen. Very little is known of his life or career. However, he was associated with the town of March, Cambridgeshire, and he may have been a relative of King Ethelstan.\",\n    },\n    {\n        \"id\": \"149\",\n        \"title\": \"James M. Safford\",\n        \"section\": \"\",\n        \"text\": \" James Merrill Safford (1822\u20131907) was an American geologist, chemist and university professor.\",\n    },\n    {\n        \"id\": \"150\",\n        \"title\": \"James M. Safford\",\n        \"section\": \"Early life\",\n        \"text\": \" James M. Safford was born in Putnam, Ohio on August 13, 1822. He received an M.D. and a PhD. He was trained as a chemist at Yale University. He married Catherine K. Owens in 1859, and they had two children.\",\n    },\n    {\n        \"id\": \"151\",\n        \"title\": \"James M. Safford\",\n        \"section\": \"Career\",\n        \"text\": \" Safford taught at Cumberland University in Lebanon, Tennessee from 1848 to 1873. He served as a Professor of Mineralogy, Botany, and Economical Geology at Vanderbilt University in Nashville, Tennessee from 1875 to 1900. He was a Presbyterian, and often started his lessons with a prayer. He served on the Tennessee Board of Health. Additionally, he acted as a chemist for the Tennessee Bureau of Agriculture in the 1870s and 1880s. He published fifty-four books, reports, and maps.\",\n    },\n    {\n        \"id\": \"152\",\n        \"title\": \"James M. Safford\",\n        \"section\": \"Death\",\n        \"text\": \" He died in Dallas on July 2, 1907.\",\n    },\n]\n</pre> # a small sample from the Dec 2021 Wikipedia dump text_chunks = [     {         \"id\": \"140\",         \"title\": \"History of marine biology\",         \"section\": \"James Cook\",         \"text\": \" James Cook is well known for his voyages of exploration for the British Navy in which he mapped out a significant amount of the world's uncharted waters. Cook's explorations took him around the world twice and led to countless descriptions of previously unknown plants and animals. Cook's explorations influenced many others and led to a number of scientists examining marine life more closely. Among those influenced was Charles Darwin who went on to make many contributions of his own. \",     },     {         \"id\": \"141\",         \"title\": \"History of marine biology\",         \"section\": \"Charles Darwin\",         \"text\": \" Charles Darwin, best known for his theory of evolution, made many significant contributions to the early study of marine biology. He spent much of his time from 1831 to 1836 on the voyage of HMS Beagle collecting and studying specimens from a variety of marine organisms. It was also on this expedition where Darwin began to study coral reefs and their formation. He came up with the theory that the overall growth of corals is a balance between the growth of corals upward and the sinking of the sea floor. He then came up with the idea that wherever coral atolls would be found, the central island where the coral had started to grow would be gradually subsiding\",     },     {         \"id\": \"142\",         \"title\": \"History of marine biology\",         \"section\": \"Charles Wyville Thomson\",         \"text\": \" Another influential expedition was the voyage of HMS Challenger from 1872 to 1876, organized and later led by Charles Wyville Thomson. It was the first expedition purely devoted to marine science. The expedition collected and analyzed thousands of marine specimens, laying the foundation for present knowledge about life near the deep-sea floor. The findings from the expedition were a summary of the known natural, physical and chemical ocean science to that time.\",     },     {         \"id\": \"143\",         \"title\": \"History of marine biology\",         \"section\": \"Later exploration\",         \"text\": \" This era of marine exploration came to a close with the first and second round-the-world voyages of the Danish Galathea expeditions and Atlantic voyages by the USS Albatross, the first research vessel purpose built for marine research. These voyages further cleared the way for modern marine biology by building a base of knowledge about marine biology. This was followed by the progressive development of more advanced technologies which began to allow more extensive explorations of ocean depths that were once thought too deep to sustain life.\",     },     {         \"id\": \"144\",         \"title\": \"History of marine biology\",         \"section\": \"Marine biology labs\",         \"text\": \" In the 1960s and 1970s, ecological research into the life of the ocean was undertaken at institutions set up specifically to study marine biology. Notable was the Woods Hole Oceanographic Institution in America, which established a model for other marine laboratories subsequently set up around the world. Their findings of unexpectedly high species diversity in places thought to be inhabitable stimulated much theorizing by population ecologists on how high diversification could be maintained in such a food-poor and seemingly hostile environment. \",     },     {         \"id\": \"145\",         \"title\": \"History of marine biology\",         \"section\": \"Exploration technology\",         \"text\": \" In the past, the study of marine biology has been limited by a lack of technology as researchers could only go so deep to examine life in the ocean. Before the mid-twentieth century, the deep-sea bottom could not be seen unless one dredged a piece of it and brought it to the surface. This has changed dramatically due to the development of new technologies in both the laboratory and the open sea. These new technological developments have allowed scientists to explore parts of the ocean they didn't even know existed. The development of scuba gear allowed researchers to visually explore the oceans as it contains a self-contained underwater breathing apparatus allowing a person to breathe while being submerged 100 to 200 feet \",     },     {         \"id\": \"146\",         \"title\": \"History of marine biology\",         \"section\": \"Exploration technology\",         \"text\": \" the ocean. Submersibles were built like small submarines with the purpose of taking marine scientists to deeper depths of the ocean while protecting them from increasing atmospheric pressures that cause complications deep under water. The first models could hold several individuals and allowed limited visibility but enabled marine biologists to see and photograph the deeper portions of the oceans. Remotely operated underwater vehicles are now used with and without submersibles to see the deepest areas of the ocean that would be too dangerous for humans. ROVs are fully equipped with cameras and sampling equipment which allows researchers to see and control everything the vehicle does. ROVs have become the dominant type of technology used to view the deepest parts of the ocean.\",     },     {         \"id\": \"147\",         \"title\": \"History of marine biology\",         \"section\": \"Romanticization\",         \"text\": ' In the late 20th century and into the 21st, marine biology was \"glorified and romanticized through films and television shows,\" leading to an influx in interested students who required a damping on their enthusiasm with the day-to-day realities of the field.',     },     {         \"id\": \"148\",         \"title\": \"Wynthryth\",         \"section\": \"\",         \"text\": \" Wynthryth of March was an early medieval saint of Anglo Saxon England. He is known to history from the Secgan Hagiography and The Confraternity Book of  St Gallen. Very little is known of his life or career. However, he was associated with the town of March, Cambridgeshire, and he may have been a relative of King Ethelstan.\",     },     {         \"id\": \"149\",         \"title\": \"James M. Safford\",         \"section\": \"\",         \"text\": \" James Merrill Safford (1822\u20131907) was an American geologist, chemist and university professor.\",     },     {         \"id\": \"150\",         \"title\": \"James M. Safford\",         \"section\": \"Early life\",         \"text\": \" James M. Safford was born in Putnam, Ohio on August 13, 1822. He received an M.D. and a PhD. He was trained as a chemist at Yale University. He married Catherine K. Owens in 1859, and they had two children.\",     },     {         \"id\": \"151\",         \"title\": \"James M. Safford\",         \"section\": \"Career\",         \"text\": \" Safford taught at Cumberland University in Lebanon, Tennessee from 1848 to 1873. He served as a Professor of Mineralogy, Botany, and Economical Geology at Vanderbilt University in Nashville, Tennessee from 1875 to 1900. He was a Presbyterian, and often started his lessons with a prayer. He served on the Tennessee Board of Health. Additionally, he acted as a chemist for the Tennessee Bureau of Agriculture in the 1870s and 1880s. He published fifty-four books, reports, and maps.\",     },     {         \"id\": \"152\",         \"title\": \"James M. Safford\",         \"section\": \"Death\",         \"text\": \" He died in Dallas on July 2, 1907.\",     }, ] <p>From these text chunks, we can create our <code>KnowledgeNodes</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from fed_rag.data_structures import KnowledgeNode, NodeType\n\n# create knowledge nodes\nnodes = []\ntexts = []\nfor c in text_chunks:\n    text = c.pop(\"text\")\n    title = c.pop(\"title\")\n    section = c.pop(\"section\")\n    context_text = f\"title: {title}\\nsection: {section}\\ntext: {text}\"\n    texts.append(context_text)\n\n# batch encode\nbatch_embeddings = retriever.encode_context(texts)\n\nfor jx, c in enumerate(text_chunks):\n    node = KnowledgeNode(\n        embedding=batch_embeddings[jx].tolist(),\n        node_type=NodeType.TEXT,\n        text_content=texts[jx],\n        metadata=c,\n    )\n    nodes.append(node)\n</pre> from fed_rag.data_structures import KnowledgeNode, NodeType  # create knowledge nodes nodes = [] texts = [] for c in text_chunks:     text = c.pop(\"text\")     title = c.pop(\"title\")     section = c.pop(\"section\")     context_text = f\"title: {title}\\nsection: {section}\\ntext: {text}\"     texts.append(context_text)  # batch encode batch_embeddings = retriever.encode_context(texts)  for jx, c in enumerate(text_chunks):     node = KnowledgeNode(         embedding=batch_embeddings[jx].tolist(),         node_type=NodeType.TEXT,         text_content=texts[jx],         metadata=c,     )     nodes.append(node) In\u00a0[\u00a0]: Copied! <pre>nodes[0].model_dump()\n</pre> nodes[0].model_dump() In\u00a0[\u00a0]: Copied! <pre># load nodes\nknowledge_store.load_nodes(nodes)\n</pre> # load nodes knowledge_store.load_nodes(nodes) In\u00a0[\u00a0]: Copied! <pre>knowledge_store.count\n</pre> knowledge_store.count In\u00a0[\u00a0]: Copied! <pre>from fed_rag.generators.huggingface import HFPretrainedModelGenerator\nimport torch\nfrom transformers.generation.utils import GenerationConfig\n\ngeneration_cfg = GenerationConfig(\n    do_sample=True,\n    eos_token_id=151643,\n    bos_token_id=151643,\n    max_new_tokens=2048,\n    top_p=0.9,\n    temperature=0.6,\n    cache_implementation=\"offloaded\",\n    stop_strings=\"&lt;/response&gt;\",\n)\ngenerator = HFPretrainedModelGenerator(\n    model_name=\"Qwen/Qwen2.5-0.5B\",\n    load_model_at_init=False,\n    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n    generation_config=generation_cfg,\n)\n</pre> from fed_rag.generators.huggingface import HFPretrainedModelGenerator import torch from transformers.generation.utils import GenerationConfig  generation_cfg = GenerationConfig(     do_sample=True,     eos_token_id=151643,     bos_token_id=151643,     max_new_tokens=2048,     top_p=0.9,     temperature=0.6,     cache_implementation=\"offloaded\",     stop_strings=\"\", ) generator = HFPretrainedModelGenerator(     model_name=\"Qwen/Qwen2.5-0.5B\",     load_model_at_init=False,     load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},     generation_config=generation_cfg, ) In\u00a0[\u00a0]: Copied! <pre>from fed_rag import RAGSystem, RAGConfig\n\nrag_config = RAGConfig(top_k=2)\nrag_system = RAGSystem(\n    knowledge_store=knowledge_store,  # knowledge store loaded from knowledge_store.py\n    generator=generator,\n    retriever=retriever,\n    rag_config=rag_config,\n)\n</pre> from fed_rag import RAGSystem, RAGConfig  rag_config = RAGConfig(top_k=2) rag_system = RAGSystem(     knowledge_store=knowledge_store,  # knowledge store loaded from knowledge_store.py     generator=generator,     retriever=retriever,     rag_config=rag_config, ) In\u00a0[\u00a0]: Copied! <pre># test a query\nresponse = rag_system.query(\"Who is James Cook?\")\n</pre> # test a query response = rag_system.query(\"Who is James Cook?\") In\u00a0[\u00a0]: Copied! <pre>print(response)\n</pre> print(response) In\u00a0[\u00a0]: Copied! <pre>from datasets import Dataset\n\n\ntrain_dataset = Dataset.from_dict(\n    # examples from Commonsense QA\n    {\n        \"query\": [\n            \"The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\",\n            \"Sammy wanted to go to where the people were.  Where might he go?\",\n            \"To locate a choker not located in a jewelry box or boutique where would you go?\",\n            \"Google Maps and other highway and street GPS services have replaced what?\",\n            \"The fox walked from the city into the forest, what was it looking for?\",\n        ],\n        \"response\": [\n            \"ignore\",\n            \"populated areas\",\n            \"jewelry store\",\n            \"atlas\",\n            \"natural habitat\",\n        ],\n    }\n)\n</pre> from datasets import Dataset   train_dataset = Dataset.from_dict(     # examples from Commonsense QA     {         \"query\": [             \"The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\",             \"Sammy wanted to go to where the people were.  Where might he go?\",             \"To locate a choker not located in a jewelry box or boutique where would you go?\",             \"Google Maps and other highway and street GPS services have replaced what?\",             \"The fox walked from the city into the forest, what was it looking for?\",         ],         \"response\": [             \"ignore\",             \"populated areas\",             \"jewelry store\",             \"atlas\",             \"natural habitat\",         ],     } ) In\u00a0[\u00a0]: Copied! <pre>from fed_rag.trainers.huggingface.lsr import HuggingFaceTrainerForLSR\n\n# the trainer object\nretriever_trainer = HuggingFaceTrainerForLSR(\n    rag_system=rag_system,\n    train_dataset=train_dataset,\n    # training_arguments=...  # Optional ~transformers.TrainingArguments\n)\n</pre> from fed_rag.trainers.huggingface.lsr import HuggingFaceTrainerForLSR  # the trainer object retriever_trainer = HuggingFaceTrainerForLSR(     rag_system=rag_system,     train_dataset=train_dataset,     # training_arguments=...  # Optional ~transformers.TrainingArguments ) In\u00a0[\u00a0]: Copied! <pre># raw HF trainer object\nretriever_trainer.hf_trainer_obj\n</pre> # raw HF trainer object retriever_trainer.hf_trainer_obj In\u00a0[\u00a0]: Copied! <pre>result = retriever_trainer.train()\n</pre> result = retriever_trainer.train() In\u00a0[\u00a0]: Copied! <pre>result.loss\n</pre> result.loss In\u00a0[\u00a0]: Copied! <pre>from fed_rag.trainers.huggingface.ralt import HuggingFaceTrainerForRALT\n\n# the trainer object\ngenerator_trainer = HuggingFaceTrainerForRALT(\n    rag_system=rag_system,\n    train_dataset=train_dataset,\n    # training_arguments=...  # Optional ~transformers.TrainingArguments\n)\n</pre> from fed_rag.trainers.huggingface.ralt import HuggingFaceTrainerForRALT  # the trainer object generator_trainer = HuggingFaceTrainerForRALT(     rag_system=rag_system,     train_dataset=train_dataset,     # training_arguments=...  # Optional ~transformers.TrainingArguments ) In\u00a0[\u00a0]: Copied! <pre># raw HF trainer object\ngenerator_trainer.hf_trainer_obj\n</pre> # raw HF trainer object generator_trainer.hf_trainer_obj In\u00a0[\u00a0]: Copied! <pre>result = generator_trainer.train()\n</pre> result = generator_trainer.train() In\u00a0[\u00a0]: Copied! <pre>result.loss\n</pre> result.loss <p>In this notebook, we used a simplified example to demonstrate building and fine-tuning a RAG system with HuggingFace models.</p>"},{"location":"notebooks/basic_starter_hf/#basic-starter-example","title":"Basic Starter Example\u00b6","text":"<p>In this notebook, we'll build a <code>RAGSystem</code> and fine-tune both the generator and retriever using the <code>huggingface</code> extra.</p>"},{"location":"notebooks/basic_starter_hf/#install-dependencies","title":"Install dependencies\u00b6","text":""},{"location":"notebooks/basic_starter_hf/#build-the-rag-system","title":"Build the RAG System\u00b6","text":""},{"location":"notebooks/basic_starter_hf/#knowledge-store-and-retriever","title":"Knowledge Store and Retriever\u00b6","text":""},{"location":"notebooks/basic_starter_hf/#lets-add-some-knowledge","title":"Let's Add Some Knowledge\u00b6","text":""},{"location":"notebooks/basic_starter_hf/#define-an-llm-generator","title":"Define an LLM Generator\u00b6","text":""},{"location":"notebooks/basic_starter_hf/#assemble-the-rag-system","title":"Assemble the RAG System\u00b6","text":""},{"location":"notebooks/basic_starter_hf/#rag-fine-tuning","title":"RAG Fine-tuning\u00b6","text":"<p>In this part of the notebook, we demonstrate how to fine-tune the <code>RAGSystem</code> we just built and queried. To do so, we'll use a <code>RetrieverTrainer</code> and a <code>GeneratorTrainer</code> to fine-tune the retriever and generator, respectively.</p>"},{"location":"notebooks/basic_starter_hf/#the-train-dataset","title":"The Train Dataset\u00b6","text":"<p>Although the retriever and generator are trained independently, both follow a standardized process. The first step involves building the training dataset which are essentially examples of (query, response) pairs.</p>"},{"location":"notebooks/basic_starter_hf/#retriever-fine-tuning-lsr","title":"Retriever Fine-Tuning (LSR)\u00b6","text":"<p>Here, we'll perform LM-Supervised retriever fine-tuning. For a tutorial on this trainer, see our docs. The <code>HuggingFaceTrainerForLSR</code> is a container class for a custom-built <code>~sentence_transformers.SentenceTransformerTrainer</code> that performs training of the retriever model using the LSR loss.</p>"},{"location":"notebooks/basic_starter_hf/#generator-fine-tuning-ralt","title":"Generator Fine-tuning (RALT)\u00b6","text":"<p>Here, we'll perform Retrieval-Augmented LM (Generator) fine-tuning. For a tutorial on this trainer, see our docs. The <code>HuggingFaceTrainerForRALT</code> is a container class for a custom-built <code>~transformers.Trainer</code> that performs training of the generator model using the causal language modelling task.</p>"},{"location":"notebooks/basic_starter_hf/#closing-remarks","title":"Closing Remarks\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/","title":"Benchmarking RAG Systems (MMLU)","text":"<p>(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)</p> In\u00a0[1]: Copied! <pre># If running in a Google Colab, the first attempt at installing fed-rag may fail,\n# though for reasons unknown to me yet, if you try a second time, it magically works...\n!pip install fed-rag[huggingface,huggingface-evals] -q\n</pre> # If running in a Google Colab, the first attempt at installing fed-rag may fail, # though for reasons unknown to me yet, if you try a second time, it magically works... !pip install fed-rag[huggingface,huggingface-evals] -q <pre>zsh:1: no matches found: fed-rag[huggingface,huggingface-evals]\n</pre> In\u00a0[2]: Copied! <pre>from fed_rag.knowledge_stores.in_memory import InMemoryKnowledgeStore\nfrom fed_rag.retrievers.huggingface.hf_sentence_transformer import (\n    HFSentenceTransformerRetriever,\n)\n\nknowledge_store = InMemoryKnowledgeStore()\n\nretriever = HFSentenceTransformerRetriever(\n    query_model_name=\"nthakur/dragon-plus-query-encoder\",\n    context_model_name=\"nthakur/dragon-plus-context-encoder\",\n    load_model_at_init=False,\n)\n</pre> from fed_rag.knowledge_stores.in_memory import InMemoryKnowledgeStore from fed_rag.retrievers.huggingface.hf_sentence_transformer import (     HFSentenceTransformerRetriever, )  knowledge_store = InMemoryKnowledgeStore()  retriever = HFSentenceTransformerRetriever(     query_model_name=\"nthakur/dragon-plus-query-encoder\",     context_model_name=\"nthakur/dragon-plus-context-encoder\",     load_model_at_init=False, ) In\u00a0[3]: Copied! <pre># a small sample from the Dec 2021 Wikipedia dump\ntext_chunks = [\n    {\n        \"id\": \"140\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"James Cook\",\n        \"text\": \" James Cook is well known for his voyages of exploration for the British Navy in which he mapped out a significant amount of the world's uncharted waters. Cook's explorations took him around the world twice and led to countless descriptions of previously unknown plants and animals. Cook's explorations influenced many others and led to a number of scientists examining marine life more closely. Among those influenced was Charles Darwin who went on to make many contributions of his own. \",\n    },\n    {\n        \"id\": \"141\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Charles Darwin\",\n        \"text\": \" Charles Darwin, best known for his theory of evolution, made many significant contributions to the early study of marine biology. He spent much of his time from 1831 to 1836 on the voyage of HMS Beagle collecting and studying specimens from a variety of marine organisms. It was also on this expedition where Darwin began to study coral reefs and their formation. He came up with the theory that the overall growth of corals is a balance between the growth of corals upward and the sinking of the sea floor. He then came up with the idea that wherever coral atolls would be found, the central island where the coral had started to grow would be gradually subsiding\",\n    },\n    {\n        \"id\": \"142\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Charles Wyville Thomson\",\n        \"text\": \" Another influential expedition was the voyage of HMS Challenger from 1872 to 1876, organized and later led by Charles Wyville Thomson. It was the first expedition purely devoted to marine science. The expedition collected and analyzed thousands of marine specimens, laying the foundation for present knowledge about life near the deep-sea floor. The findings from the expedition were a summary of the known natural, physical and chemical ocean science to that time.\",\n    },\n]\n</pre> # a small sample from the Dec 2021 Wikipedia dump text_chunks = [     {         \"id\": \"140\",         \"title\": \"History of marine biology\",         \"section\": \"James Cook\",         \"text\": \" James Cook is well known for his voyages of exploration for the British Navy in which he mapped out a significant amount of the world's uncharted waters. Cook's explorations took him around the world twice and led to countless descriptions of previously unknown plants and animals. Cook's explorations influenced many others and led to a number of scientists examining marine life more closely. Among those influenced was Charles Darwin who went on to make many contributions of his own. \",     },     {         \"id\": \"141\",         \"title\": \"History of marine biology\",         \"section\": \"Charles Darwin\",         \"text\": \" Charles Darwin, best known for his theory of evolution, made many significant contributions to the early study of marine biology. He spent much of his time from 1831 to 1836 on the voyage of HMS Beagle collecting and studying specimens from a variety of marine organisms. It was also on this expedition where Darwin began to study coral reefs and their formation. He came up with the theory that the overall growth of corals is a balance between the growth of corals upward and the sinking of the sea floor. He then came up with the idea that wherever coral atolls would be found, the central island where the coral had started to grow would be gradually subsiding\",     },     {         \"id\": \"142\",         \"title\": \"History of marine biology\",         \"section\": \"Charles Wyville Thomson\",         \"text\": \" Another influential expedition was the voyage of HMS Challenger from 1872 to 1876, organized and later led by Charles Wyville Thomson. It was the first expedition purely devoted to marine science. The expedition collected and analyzed thousands of marine specimens, laying the foundation for present knowledge about life near the deep-sea floor. The findings from the expedition were a summary of the known natural, physical and chemical ocean science to that time.\",     }, ] In\u00a0[4]: Copied! <pre>from fed_rag.data_structures import KnowledgeNode, NodeType\n\n# create knowledge nodes\nnodes = []\ntexts = []\nfor c in text_chunks:\n    text = c.pop(\"text\")\n    title = c.pop(\"title\")\n    section = c.pop(\"section\")\n    context_text = f\"title: {title}\\nsection: {section}\\ntext: {text}\"\n    texts.append(context_text)\n\n# batch encode\nbatch_embeddings = retriever.encode_context(texts)\n\nfor jx, c in enumerate(text_chunks):\n    node = KnowledgeNode(\n        embedding=batch_embeddings[jx].tolist(),\n        node_type=NodeType.TEXT,\n        text_content=texts[jx],\n        metadata=c,\n    )\n    nodes.append(node)\n</pre> from fed_rag.data_structures import KnowledgeNode, NodeType  # create knowledge nodes nodes = [] texts = [] for c in text_chunks:     text = c.pop(\"text\")     title = c.pop(\"title\")     section = c.pop(\"section\")     context_text = f\"title: {title}\\nsection: {section}\\ntext: {text}\"     texts.append(context_text)  # batch encode batch_embeddings = retriever.encode_context(texts)  for jx, c in enumerate(text_chunks):     node = KnowledgeNode(         embedding=batch_embeddings[jx].tolist(),         node_type=NodeType.TEXT,         text_content=texts[jx],         metadata=c,     )     nodes.append(node) In\u00a0[5]: Copied! <pre>from fed_rag.generators.huggingface import HFPretrainedModelGenerator\nimport torch\nfrom transformers.generation.utils import GenerationConfig\n\ngeneration_cfg = GenerationConfig(\n    do_sample=True,\n    eos_token_id=151643,\n    bos_token_id=151643,\n    max_new_tokens=2048,\n    top_p=0.9,\n    temperature=0.6,\n    cache_implementation=\"offloaded\",\n    stop_strings=\"&lt;/response&gt;\",\n)\ngenerator = HFPretrainedModelGenerator(\n    model_name=\"Qwen/Qwen2.5-0.5B\",\n    load_model_at_init=False,\n    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n    generation_config=generation_cfg,\n)\n</pre> from fed_rag.generators.huggingface import HFPretrainedModelGenerator import torch from transformers.generation.utils import GenerationConfig  generation_cfg = GenerationConfig(     do_sample=True,     eos_token_id=151643,     bos_token_id=151643,     max_new_tokens=2048,     top_p=0.9,     temperature=0.6,     cache_implementation=\"offloaded\",     stop_strings=\"\", ) generator = HFPretrainedModelGenerator(     model_name=\"Qwen/Qwen2.5-0.5B\",     load_model_at_init=False,     load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},     generation_config=generation_cfg, ) In\u00a0[6]: Copied! <pre># load nodes\nknowledge_store.load_nodes(nodes)\n</pre> # load nodes knowledge_store.load_nodes(nodes) In\u00a0[7]: Copied! <pre>knowledge_store.count\n</pre> knowledge_store.count Out[7]: <pre>3</pre> In\u00a0[8]: Copied! <pre>from fed_rag.generators.huggingface import HFPretrainedModelGenerator\nimport torch\nfrom transformers.generation.utils import GenerationConfig\n\ngeneration_cfg = GenerationConfig(\n    do_sample=True,\n    eos_token_id=151643,\n    bos_token_id=151643,\n    max_new_tokens=2048,\n    top_p=0.9,\n    temperature=0.6,\n    cache_implementation=\"offloaded\",\n    stop_strings=\"&lt;/response&gt;\",\n)\ngenerator = HFPretrainedModelGenerator(\n    model_name=\"Qwen/Qwen2.5-0.5B\",\n    load_model_at_init=False,\n    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n    generation_config=generation_cfg,\n)\n</pre> from fed_rag.generators.huggingface import HFPretrainedModelGenerator import torch from transformers.generation.utils import GenerationConfig  generation_cfg = GenerationConfig(     do_sample=True,     eos_token_id=151643,     bos_token_id=151643,     max_new_tokens=2048,     top_p=0.9,     temperature=0.6,     cache_implementation=\"offloaded\",     stop_strings=\"\", ) generator = HFPretrainedModelGenerator(     model_name=\"Qwen/Qwen2.5-0.5B\",     load_model_at_init=False,     load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},     generation_config=generation_cfg, ) In\u00a0[9]: Copied! <pre>from fed_rag import RAGSystem, RAGConfig\n\nrag_config = RAGConfig(top_k=2)\nrag_system = RAGSystem(\n    knowledge_store=knowledge_store,  # knowledge store loaded from knowledge_store.py\n    generator=generator,\n    retriever=retriever,\n    rag_config=rag_config,\n)\n</pre> from fed_rag import RAGSystem, RAGConfig  rag_config = RAGConfig(top_k=2) rag_system = RAGSystem(     knowledge_store=knowledge_store,  # knowledge store loaded from knowledge_store.py     generator=generator,     retriever=retriever,     rag_config=rag_config, ) In\u00a0[10]: Copied! <pre># test a query\nresponse = rag_system.query(\"Who is James Cook?\")\n</pre> # test a query response = rag_system.query(\"Who is James Cook?\") <pre>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n</pre> In\u00a0[11]: Copied! <pre>print(response)\n</pre> print(response) <pre>Assistant: James Cook is well known for his voyages of exploration for the British Navy in which he mapped out a significant amount of the world's uncharted waters. Cook's explorations took him around the world twice and led to countless descriptions of previously unknown plants and animals. Cook's explorations influenced many others and led to a number of scientists examining marine life more closely. Among those influenced was Charles Darwin who went on to make many contributions of his own.\n</pre> In\u00a0[12]: Copied! <pre>from fed_rag.evals import Benchmarker\n</pre> from fed_rag.evals import Benchmarker In\u00a0[13]: Copied! <pre>benchmarker = Benchmarker(rag_system=rag_system)\n</pre> benchmarker = Benchmarker(rag_system=rag_system) <p>For this notebook, we'll use a HuggingFace benchmark, namely the MMLU one. The recommended pattern for loading benchmarks from <code>fed_rag</code> is illustrated in the cells found below.</p> In\u00a0[14]: Copied! <pre>import fed_rag.evals.benchmarks as benchmarks\n\n# define the mmlu benchmark\nmmlu = benchmarks.HuggingFaceMMLU(streaming=True)\n</pre> import fed_rag.evals.benchmarks as benchmarks  # define the mmlu benchmark mmlu = benchmarks.HuggingFaceMMLU(streaming=True) <p>In the above, we set <code>streaming</code> to <code>True</code> since the underlying dataset is quite large. By doing so, we can get a stream of <code>~fed_rag.data_structures.BenchmarkExample</code> that we can process.</p> In\u00a0[15]: Copied! <pre>example_stream = mmlu.as_stream()\nnext(example_stream)\n</pre> example_stream = mmlu.as_stream() next(example_stream) Out[15]: <pre>BenchmarkExample(query='Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\\n\\nA: 0\\nB: 4\\nC: 2\\nD: 6', response='B', context=None)</pre> In\u00a0[16]: Copied! <pre>example_stream.close()  # close the stream\n</pre> example_stream.close()  # close the stream <p>In this notebook, we'll make use of the <code>ExactMatchEvaluationMetric</code>.</p> In\u00a0[17]: Copied! <pre>from fed_rag.evals.metrics import ExactMatchEvaluationMetric\n\nmetric = ExactMatchEvaluationMetric()\n</pre> from fed_rag.evals.metrics import ExactMatchEvaluationMetric  metric = ExactMatchEvaluationMetric() <p>All <code>BaseEvaluationMetric</code> are direcly callable (i.e., their special <code>__call__</code> methods are implemented). We can see the signature of this method by using the <code>help</code> builtin.</p> In\u00a0[18]: Copied! <pre>help(metric.__call__)\n</pre> help(metric.__call__) <pre>Help on method __call__ in module fed_rag.evals.metrics.exact_match:\n\n__call__(prediction: str, actual: str, *args: Any, **kwargs: Any) -&gt; float method of fed_rag.evals.metrics.exact_match.ExactMatchEvaluationMetric instance\n    Evaluate an example prediction against the actual response.\n\n</pre> <p>Exact match is case insensitive.</p> In\u00a0[19]: Copied! <pre>metric(prediction=\"A\", actual=\"A\")  # scores 1\n</pre> metric(prediction=\"A\", actual=\"A\")  # scores 1 Out[19]: <pre>1.0</pre> In\u00a0[20]: Copied! <pre>metric(prediction=\"A\", actual=\"a\")  # also scores 1\n</pre> metric(prediction=\"A\", actual=\"a\")  # also scores 1 Out[20]: <pre>1.0</pre> In\u00a0[21]: Copied! <pre>metric(prediction=\"A\", actual=\"b\")  # scores 0\n</pre> metric(prediction=\"A\", actual=\"b\")  # scores 0 Out[21]: <pre>0.0</pre> In\u00a0[31]: Copied! <pre>result = benchmarker.run(\n    benchmark=mmlu,\n    metric=metric,\n    is_streaming=True,\n    num_examples=3,  # for quick testing only run it on 3 examples\n    agg=\"avg\",  # can be 'avg', 'sum', 'max', 'min'\n    save_evaluations=True,  # needs fed-rag v0.0.23 or above\n)\n</pre> result = benchmarker.run(     benchmark=mmlu,     metric=metric,     is_streaming=True,     num_examples=3,  # for quick testing only run it on 3 examples     agg=\"avg\",  # can be 'avg', 'sum', 'max', 'min'     save_evaluations=True,  # needs fed-rag v0.0.23 or above ) <pre>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n</pre> In\u00a0[32]: Copied! <pre>print(result)\n</pre> print(result) <pre>score=0.0 metric_name='ExactMatchEvaluationMetric' num_examples_used=3 num_total_examples=14042 evaluations_file='.fed_rag/benchmark_results/HuggingFaceMMLU-20250527_031156.jsonl'\n</pre> In\u00a0[33]: Copied! <pre>from fed_rag.evals.utils import load_evaluations\n</pre> from fed_rag.evals.utils import load_evaluations In\u00a0[34]: Copied! <pre>evaluations = load_evaluations(result.evaluations_file)\n</pre> evaluations = load_evaluations(result.evaluations_file) In\u00a0[37]: Copied! <pre>evaluations[1].score\n</pre> evaluations[1].score Out[37]: <pre>0.0</pre> In\u00a0[39]: Copied! <pre>print(evaluations[0].rag_response)\n</pre> print(evaluations[0].rag_response) <pre>Assistant: The degree of the field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q is 0.\n</pre>"},{"location":"notebooks/rag_benchmarking_hf_mmlu/#benchmarking-rag-systems-mmlu","title":"Benchmarking RAG Systems (MMLU)\u00b6","text":"<p>In this note book, we demonstrate how one benchmark a RAG system with the <code>fed-rag</code> library. Doing so, involves the following steps:</p> <ol> <li>Build your <code>RAGSystem</code> to be benchmarked</li> <li>Create a <code>Benchmarker</code> object</li> <li>Choose your <code>Benchmark</code> and run it with the <code>BenchMarker</code></li> </ol> <p>In this notebook, we'll make use of the <code>huggingface-evals</code> extra which will allow us to utilize the benchmarks defined in the <code>fed_rag.evals.benchmarks.huggingface</code> module.</p>"},{"location":"notebooks/rag_benchmarking_hf_mmlu/#install-dependencies","title":"Install dependencies\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/#build-the-rag-system","title":"Build the RAG System\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/#knowledge-store-and-retriever","title":"Knowledge Store and Retriever\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/#lets-add-some-knowledge","title":"Let's Add Some Knowledge\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/#define-an-llm-generator","title":"Define an LLM Generator\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/#assemble-the-rag-system","title":"Assemble the RAG System\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/#create-benchmarker","title":"Create <code>Benchmarker</code>\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/#get-the-desired-benchmark-mmlu","title":"Get the desired Benchmark (MMLU)\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/#define-our-evaluation-metric","title":"Define our Evaluation Metric\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/#run-the-benchmark","title":"Run the benchmark\u00b6","text":""},{"location":"notebooks/rag_benchmarking_hf_mmlu/#load-evaluated-examples","title":"Load evaluated examples\u00b6","text":""},{"location":"notebooks/integrations/llama_index/","title":"Using LlamaIndex for Inference","text":"<p>(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)</p> In\u00a0[1]: Copied! <pre># If running in a Google Colab, the first attempt at installing fed-rag may fail,\n# though for reasons unknown to me yet, if you try a second time, it magically works...\n!pip install fed-rag[huggingface,llama-index] -q\n</pre> # If running in a Google Colab, the first attempt at installing fed-rag may fail, # though for reasons unknown to me yet, if you try a second time, it magically works... !pip install fed-rag[huggingface,llama-index] -q <pre>zsh:1: no matches found: fed-rag[huggingface,llama-index]\n</pre> In\u00a0[1]: Copied! <pre>import torch\nfrom transformers.generation.utils import GenerationConfig\n\nfrom fed_rag import RAGSystem, RAGConfig\nfrom fed_rag.generators.huggingface import HFPretrainedModelGenerator\nfrom fed_rag.retrievers.huggingface import (\n    HFSentenceTransformerRetriever,\n)\nfrom fed_rag.knowledge_stores import InMemoryKnowledgeStore\nfrom fed_rag.data_structures import KnowledgeNode, NodeType\n\n\nQUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\"\nCONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\"\nPRETRAINED_MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n\n# Retriever\nretriever = HFSentenceTransformerRetriever(\n    query_model_name=QUERY_ENCODER_NAME,\n    context_model_name=CONTEXT_ENCODER_NAME,\n    load_model_at_init=False,\n)\n\n# Generator\ngeneration_cfg = GenerationConfig(\n    do_sample=True,\n    eos_token_id=151643,\n    bos_token_id=151643,\n    max_new_tokens=2048,\n    top_p=0.9,\n    temperature=0.6,\n    cache_implementation=\"offloaded\",\n    stop_strings=\"&lt;/response&gt;\",\n)\ngenerator = HFPretrainedModelGenerator(\n    model_name=\"Qwen/Qwen2.5-1.5B\",\n    load_model_at_init=False,\n    load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},\n    generation_config=generation_cfg,\n)\n\n# Knowledge store\nknowledge_store = InMemoryKnowledgeStore()\n\n\n# Create the RAG system\nrag_system = RAGSystem(\n    retriever=retriever,\n    generator=generator,\n    knowledge_store=knowledge_store,\n    rag_config=RAGConfig(top_k=1),\n)\n</pre> import torch from transformers.generation.utils import GenerationConfig  from fed_rag import RAGSystem, RAGConfig from fed_rag.generators.huggingface import HFPretrainedModelGenerator from fed_rag.retrievers.huggingface import (     HFSentenceTransformerRetriever, ) from fed_rag.knowledge_stores import InMemoryKnowledgeStore from fed_rag.data_structures import KnowledgeNode, NodeType   QUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\" CONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\" PRETRAINED_MODEL_NAME = \"Qwen/Qwen3-0.6B\"  # Retriever retriever = HFSentenceTransformerRetriever(     query_model_name=QUERY_ENCODER_NAME,     context_model_name=CONTEXT_ENCODER_NAME,     load_model_at_init=False, )  # Generator generation_cfg = GenerationConfig(     do_sample=True,     eos_token_id=151643,     bos_token_id=151643,     max_new_tokens=2048,     top_p=0.9,     temperature=0.6,     cache_implementation=\"offloaded\",     stop_strings=\"\", ) generator = HFPretrainedModelGenerator(     model_name=\"Qwen/Qwen2.5-1.5B\",     load_model_at_init=False,     load_model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16},     generation_config=generation_cfg, )  # Knowledge store knowledge_store = InMemoryKnowledgeStore()   # Create the RAG system rag_system = RAGSystem(     retriever=retriever,     generator=generator,     knowledge_store=knowledge_store,     rag_config=RAGConfig(top_k=1), ) In\u00a0[2]: Copied! <pre>text_chunks = [\n    \"Retrieval-Augmented Generation (RAG) combines retrieval with generation.\",\n    \"LLMs can hallucinate information when they lack context.\",\n]\nknowledge_nodes = [\n    KnowledgeNode(\n        node_type=\"text\",\n        embedding=retriever.encode_context(ct).tolist(),\n        text_content=ct,\n    )\n    for ct in text_chunks\n]\nknowledge_store.load_nodes(knowledge_nodes)\n</pre> text_chunks = [     \"Retrieval-Augmented Generation (RAG) combines retrieval with generation.\",     \"LLMs can hallucinate information when they lack context.\", ] knowledge_nodes = [     KnowledgeNode(         node_type=\"text\",         embedding=retriever.encode_context(ct).tolist(),         text_content=ct,     )     for ct in text_chunks ] knowledge_store.load_nodes(knowledge_nodes) In\u00a0[3]: Copied! <pre>rag_system.knowledge_store.count\n</pre> rag_system.knowledge_store.count Out[3]: <pre>2</pre> In\u00a0[4]: Copied! <pre># Create a llamaindex object\nindex = rag_system.to_llamaindex()\n\n# Use it like any other LlamaIndex object to get a query engine\nquery = \"What happens if LLMs lack context?\"\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(query)\nprint(response, \"\\n\")\n\n# Or, get a retriever\nretriever = index.as_retriever()\nresults = retriever.retrieve(query)\nfor node in results:\n    print(f\"Score: {node.score}, Content: {node.node}\")\n</pre> # Create a llamaindex object index = rag_system.to_llamaindex()  # Use it like any other LlamaIndex object to get a query engine query = \"What happens if LLMs lack context?\" query_engine = index.as_query_engine() response = query_engine.query(query) print(response, \"\\n\")  # Or, get a retriever retriever = index.as_retriever() results = retriever.retrieve(query) for node in results:     print(f\"Score: {node.score}, Content: {node.node}\") <pre>Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n</pre> <pre>Context information is below.\n---------------------\nLLMs can hallucinate information when they lack context.\n\nRetrieval-Augmented Generation (RAG) combines retrieval with generation.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What happens if LLMs lack context?\nAnswer: 1. LLMs (Language Model Generators) can hallucinate information when they lack context. This means that without sufficient information or context, LLMs may generate responses that are not based on real-world facts or previous knowledge.\n\n2. LLMs are designed to generate responses based on the input provided, but when the input is incomplete or lacks context, they may fill in the gaps with their own assumptions or generate incorrect or irrelevant information.\n\n3. LLMs rely on their training data and the context in which it was generated to make predictions. If the context is not present or incomplete, the LLM may generate responses that are not relevant or accurate.\n\n4. LLMs can be trained on large datasets, but they still require context to understand the meaning of the input. Without context, LLMs may generate responses that are not relevant or accurate.\n\n5. It is important to provide context when using LLMs to ensure that the generated responses are accurate and relevant. This can be done by providing the necessary information or context to the LLM, or by using a combination of LLMs and other tools to generate responses. \n\nScore: 0.5453173113645673, Content: Node ID: 8864707f-9fce-49f3-aa34-7370b41bfc4f\nText: LLMs can hallucinate information when they lack context.\nScore: 0.5065647593667755, Content: Node ID: eb4b722f-1e0c-4434-915c-4cd9db604dba\nText: Retrieval-Augmented Generation (RAG) combines retrieval with\ngeneration.\n</pre> In\u00a0[5]: Copied! <pre>from llama_index.core.schema import Node, MediaResource\n\nllama_nodes = [\n    Node(\n        embedding=[1, 1, 1],\n        text_resource=MediaResource(text=\"some arbitrary text\"),\n    ),\n    Node(\n        embedding=[2, 2, 2],\n        text_resource=MediaResource(text=\"some more arbitrary text\"),\n    ),\n]\nindex.insert_nodes(llama_nodes)\n</pre> from llama_index.core.schema import Node, MediaResource  llama_nodes = [     Node(         embedding=[1, 1, 1],         text_resource=MediaResource(text=\"some arbitrary text\"),     ),     Node(         embedding=[2, 2, 2],         text_resource=MediaResource(text=\"some more arbitrary text\"),     ), ] index.insert_nodes(llama_nodes) In\u00a0[6]: Copied! <pre># confirm that what we added above is indeed in the knowledge store\nrag_system.knowledge_store.count\n</pre> # confirm that what we added above is indeed in the knowledge store rag_system.knowledge_store.count Out[6]: <pre>4</pre> In\u00a0[7]: Copied! <pre># you can also delete nodes\nindex.delete_nodes(node_ids=[node.node_id for node in llama_nodes])\n</pre> # you can also delete nodes index.delete_nodes(node_ids=[node.node_id for node in llama_nodes]) In\u00a0[8]: Copied! <pre># confirm that what we deleted above is indeed removed from the knowledge store\nrag_system.knowledge_store.count\n</pre> # confirm that what we deleted above is indeed removed from the knowledge store rag_system.knowledge_store.count Out[8]: <pre>2</pre> In\u00a0[9]: Copied! <pre>from llama_index.core.postprocessor import SentenceTransformerRerank\n\nrerank = SentenceTransformerRerank(\n    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n)\n\nquery_engine = index.as_query_engine(\n    similarity_top_k=2, node_postprocessors=[rerank]\n)\n\n# Execute the query with the advanced configuration\nresponse = query_engine.query(\"Explain the benefits of RAG systems\")\nprint(response)\n</pre> from llama_index.core.postprocessor import SentenceTransformerRerank  rerank = SentenceTransformerRerank(     model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3 )  query_engine = index.as_query_engine(     similarity_top_k=2, node_postprocessors=[rerank] )  # Execute the query with the advanced configuration response = query_engine.query(\"Explain the benefits of RAG systems\") print(response) <pre>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n</pre> <pre>Context information is below.\n---------------------\nRetrieval-Augmented Generation (RAG) combines retrieval with generation.\n\nLLMs can hallucinate information when they lack context.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Explain the benefits of RAG systems\nAnswer: 1. Retrieval-Augmented Generation (RAG) systems combine retrieval and generation to enhance the performance of LLMs. 2. RAG systems can provide more accurate and relevant results by leveraging external knowledge sources. 3. RAG systems can handle complex queries by breaking them down into smaller subqueries and combining the results. 4. RAG systems can improve the quality of generated text by providing context and guidance from external sources. 5. RAG systems can be used in various applications, such as search engines, chatbots, and virtual assistants, to provide better user experiences.\n</pre> In\u00a0[10]: Copied! <pre>response.source_nodes\n</pre> response.source_nodes Out[10]: <pre>[NodeWithScore(node=Node(id_='eb4b722f-1e0c-4434-915c-4cd9db604dba', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Retrieval-Augmented Generation (RAG) combines retrieval with generation.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), score=-2.525338),\n NodeWithScore(node=Node(id_='8864707f-9fce-49f3-aa34-7370b41bfc4f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LLMs can hallucinate information when they lack context.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), score=-11.860242)]</pre> In\u00a0[11]: Copied! <pre># see available bridges\nprint(RAGSystem.bridges)\n\n# see the LlamaIndex bridge metadata\nprint(RAGSystem.bridges[\"llama-index\"])\n</pre> # see available bridges print(RAGSystem.bridges)  # see the LlamaIndex bridge metadata print(RAGSystem.bridges[\"llama-index\"]) <pre>{'llama-index': {'bridge_version': '0.1.0', 'framework': 'llama-index', 'compatible_versions': ['0.12.35'], 'method_name': 'to_llamaindex'}}\n{'bridge_version': '0.1.0', 'framework': 'llama-index', 'compatible_versions': ['0.12.35'], 'method_name': 'to_llamaindex'}\n</pre>"},{"location":"notebooks/integrations/llama_index/#using-llamaindex-for-inference","title":"Using LlamaIndex for Inference\u00b6","text":""},{"location":"notebooks/integrations/llama_index/#introduction","title":"Introduction\u00b6","text":"<p>After fine-tuning your RAG system to achieve desired performance, you'll want to deploy it for inference. While FedRAG's <code>RAGSystem</code> provides complete inference capabilities out of the box, you may need additional features for production deployments or want to leverage the ecosystem of existing RAG frameworks.</p> <p>FedRAG offers a seamless integration into LlamaIndex through our bridges system, giving you the best of both worlds: FedRAG's fine-tuning capabilities combined with the extensive inference features of LlamaIndex.</p> <p>In this example, we demonstrate how you can convert a <code>RAGSystem</code> to a <code>~llama_index.BaseManagedIndex</code> from which you can obtain <code>~llama_index.QueryEngine</code> as well as <code>~llama_index.Retriever</code>.</p> <p>NOTE: Streaming and async functionalities are not yet supported.</p>"},{"location":"notebooks/integrations/llama_index/#install-dependencies","title":"Install dependencies\u00b6","text":""},{"location":"notebooks/integrations/llama_index/#setup-the-rag-system","title":"Setup \u2014 The RAG System\u00b6","text":""},{"location":"notebooks/integrations/llama_index/#add-some-knowledge","title":"Add some knowledge\u00b6","text":""},{"location":"notebooks/integrations/llama_index/#using-the-bridge","title":"Using the Bridge\u00b6","text":"<p>Converting your RAG system to a LlamaIndex object is seamless since the bridge functionality is already built into the <code>RAGSystem</code> class. The <code>RAGSystem</code> inherits from <code>LlamaIndexBridgeMixin</code>, which provides the <code>to_llamaindex()</code> method for effortless conversion.</p> <p>NOTE: The <code>to_llamaindex()</code> method returns a <code>FedRAGManagedIndex</code> object, which is a custom implementation of the <code>~llama_index.BaseManagedIndex</code> class.</p>"},{"location":"notebooks/integrations/llama_index/#modifying-knowledge","title":"Modifying Knowledge\u00b6","text":"<p>In addition to querying the bridged index, you can also make changes to the underlying KnowledgeStore using LlamaIndex's API:</p>"},{"location":"notebooks/integrations/llama_index/#advanced-usage","title":"Advanced Usage\u00b6","text":"<p>You can combine your bridged index with LlamaIndex's advanced features:</p>"},{"location":"notebooks/integrations/llama_index/#bridge-metadata","title":"Bridge Metadata\u00b6","text":"<p>To view the metadata of the LlamaIndex bridge, you can access the class attribute <code>bridge</code> of the <code>RAGSystem</code> class, which is a dictionary object that contains the <code>BridgeMetadata</code> for all of the installed bridges.</p>"},{"location":"notebooks/integrations/qdrant/","title":"Using Qdrant for Knowledge Storage","text":"<p>(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)</p> In\u00a0[\u00a0]: Copied! <pre># If running in a Google Colab, the first attempt at installing fed-rag may fail,\n# though for reasons unknown to me yet, if you try a second time, it magically works...\n!uv pip install fed-rag[huggingface,qdrant] -q\n</pre> # If running in a Google Colab, the first attempt at installing fed-rag may fail, # though for reasons unknown to me yet, if you try a second time, it magically works... !uv pip install fed-rag[huggingface,qdrant] -q In\u00a0[\u00a0]: Copied! <pre># We'll use the docker SDK to launch the Qdrant docker image\n!uv pip install docker -q\n</pre> # We'll use the docker SDK to launch the Qdrant docker image !uv pip install docker -q In\u00a0[1]: Copied! <pre>WITH_DOCKER = True\n</pre> WITH_DOCKER = True In\u00a0[2]: Copied! <pre>if WITH_DOCKER:\n    import docker\n    import os\n    import time\n\n    client = docker.from_env()\n    image_name = \"qdrant/qdrant\"\n\n    # first see if we need to pull the docker image\n    try:\n        client.images.get(image_name)\n        print(f\"Image '{image_name}' already exists locally\")\n    except docker.errors.ImageNotFound:\n        print(f\"Image '{image_name}' not found locally. Pulling...\")\n        # Pull with progress information\n        for line in client.api.pull(image_name, stream=True, decode=True):\n            if \"progress\" in line:\n                print(f\"\\r{line['status']}: {line['progress']}\", end=\"\")\n            elif \"status\" in line:\n                print(f\"\\r{line['status']}\", end=\"\")\n        print(\"\\nPull complete!\")\n\n    # run the Qdrant container\n    container = client.containers.run(\n        \"qdrant/qdrant\",\n        detach=True,  # Run in background\n        ports={\"6333/tcp\": 6333, \"6334/tcp\": 6334},\n        volumes={\n            f\"{os.getcwd()}/qdrant_storage\": {\n                \"bind\": \"/qdrant/storage\",\n                \"mode\": \"rw\",\n            }\n        },\n        name=\"qdrant-demo-fedrag-nb\",\n    )\n\n    print(f\"Container started with ID: {container.id}\")\n\n    # wait a moment for the container to initialize\n    time.sleep(3)\n\n    # Check container status\n    container.reload()  # Refresh container data\n    print(f\"Container status: {container.status}\")\n    print(f\"Container logs:\")\n    print(container.logs().decode(\"utf-8\"))\n</pre> if WITH_DOCKER:     import docker     import os     import time      client = docker.from_env()     image_name = \"qdrant/qdrant\"      # first see if we need to pull the docker image     try:         client.images.get(image_name)         print(f\"Image '{image_name}' already exists locally\")     except docker.errors.ImageNotFound:         print(f\"Image '{image_name}' not found locally. Pulling...\")         # Pull with progress information         for line in client.api.pull(image_name, stream=True, decode=True):             if \"progress\" in line:                 print(f\"\\r{line['status']}: {line['progress']}\", end=\"\")             elif \"status\" in line:                 print(f\"\\r{line['status']}\", end=\"\")         print(\"\\nPull complete!\")      # run the Qdrant container     container = client.containers.run(         \"qdrant/qdrant\",         detach=True,  # Run in background         ports={\"6333/tcp\": 6333, \"6334/tcp\": 6334},         volumes={             f\"{os.getcwd()}/qdrant_storage\": {                 \"bind\": \"/qdrant/storage\",                 \"mode\": \"rw\",             }         },         name=\"qdrant-demo-fedrag-nb\",     )      print(f\"Container started with ID: {container.id}\")      # wait a moment for the container to initialize     time.sleep(3)      # Check container status     container.reload()  # Refresh container data     print(f\"Container status: {container.status}\")     print(f\"Container logs:\")     print(container.logs().decode(\"utf-8\")) <pre>Image 'qdrant/qdrant' already exists locally\nContainer started with ID: 33dec7bc01280a8b8343ff4da87e8822cbfdd3a242249d1935b581031ae48784\nContainer status: running\nContainer logs:\n           _                 _    \n  __ _  __| |_ __ __ _ _ __ | |_  \n / _` |/ _` | '__/ _` | '_ \\| __| \n| (_| | (_| | | | (_| | | | | |_  \n \\__, |\\__,_|_|  \\__,_|_| |_|\\__| \n    |_|                           \n\nVersion: 1.14.0, build: 3617a011\nAccess web UI at http://localhost:6333/dashboard\n\n2025-05-21T19:20:02.141355Z  INFO storage::content_manager::consensus::persistent: Loading raft state from ./storage/raft_state.json    \n2025-05-21T19:20:02.145747Z  INFO qdrant: Distributed mode disabled    \n2025-05-21T19:20:02.145819Z  INFO qdrant: Telemetry reporting enabled, id: 92001ec9-eb22-4dbf-bcfd-5a800bac4ffb    \n2025-05-21T19:20:02.145906Z  INFO qdrant: Inference service is not configured.    \n2025-05-21T19:20:02.147806Z  INFO qdrant::actix: TLS disabled for REST API    \n2025-05-21T19:20:02.147861Z  INFO qdrant::actix: Qdrant HTTP listening on 6333    \n2025-05-21T19:20:02.147878Z  INFO actix_server::builder: Starting 11 workers\n2025-05-21T19:20:02.147884Z  INFO actix_server::server: Actix runtime found; starting in Actix runtime\n2025-05-21T19:20:02.151909Z  INFO qdrant::tonic: Qdrant gRPC listening on 6334    \n2025-05-21T19:20:02.151920Z  INFO qdrant::tonic: TLS disabled for gRPC API    \n\n</pre> In\u00a0[3]: Copied! <pre>from fed_rag.knowledge_stores import QdrantKnowledgeStore\nfrom fed_rag.retrievers.huggingface import (\n    HFSentenceTransformerRetriever,\n)\nfrom fed_rag.data_structures import KnowledgeNode, NodeType\n</pre> from fed_rag.knowledge_stores import QdrantKnowledgeStore from fed_rag.retrievers.huggingface import (     HFSentenceTransformerRetriever, ) from fed_rag.data_structures import KnowledgeNode, NodeType In\u00a0[4]: Copied! <pre>QUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\"\nCONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\"\n\n# retriever\nretriever = HFSentenceTransformerRetriever(\n    query_model_name=QUERY_ENCODER_NAME,\n    context_model_name=CONTEXT_ENCODER_NAME,\n    load_model_at_init=False,\n)\n\n# knowledge store\nif WITH_DOCKER:\n    knowledge_store = QdrantKnowledgeStore(\n        collection_name=\"nthakur.dragon-plus-context-encoder\"\n    )\nelse:\n    knowledge_store = QdrantKnowledgeStore(\n        collection_name=\"nthakur.dragon-plus-context-encoder\", in_memory=True\n    )\n</pre> QUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\" CONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\"  # retriever retriever = HFSentenceTransformerRetriever(     query_model_name=QUERY_ENCODER_NAME,     context_model_name=CONTEXT_ENCODER_NAME,     load_model_at_init=False, )  # knowledge store if WITH_DOCKER:     knowledge_store = QdrantKnowledgeStore(         collection_name=\"nthakur.dragon-plus-context-encoder\"     ) else:     knowledge_store = QdrantKnowledgeStore(         collection_name=\"nthakur.dragon-plus-context-encoder\", in_memory=True     ) In\u00a0[5]: Copied! <pre># a small sample from the Dec 2021 Wikipedia dump\ntext_chunks = [\n    {\n        \"id\": \"140\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"James Cook\",\n        \"text\": \" James Cook is well known for his voyages of exploration for the British Navy in which he mapped out a significant amount of the world's uncharted waters. Cook's explorations took him around the world twice and led to countless descriptions of previously unknown plants and animals. Cook's explorations influenced many others and led to a number of scientists examining marine life more closely. Among those influenced was Charles Darwin who went on to make many contributions of his own. \",\n    },\n    {\n        \"id\": \"141\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Charles Darwin\",\n        \"text\": \" Charles Darwin, best known for his theory of evolution, made many significant contributions to the early study of marine biology. He spent much of his time from 1831 to 1836 on the voyage of HMS Beagle collecting and studying specimens from a variety of marine organisms. It was also on this expedition where Darwin began to study coral reefs and their formation. He came up with the theory that the overall growth of corals is a balance between the growth of corals upward and the sinking of the sea floor. He then came up with the idea that wherever coral atolls would be found, the central island where the coral had started to grow would be gradually subsiding\",\n    },\n    {\n        \"id\": \"142\",\n        \"title\": \"History of marine biology\",\n        \"section\": \"Charles Wyville Thomson\",\n        \"text\": \" Another influential expedition was the voyage of HMS Challenger from 1872 to 1876, organized and later led by Charles Wyville Thomson. It was the first expedition purely devoted to marine science. The expedition collected and analyzed thousands of marine specimens, laying the foundation for present knowledge about life near the deep-sea floor. The findings from the expedition were a summary of the known natural, physical and chemical ocean science to that time.\",\n    },\n]\n</pre> # a small sample from the Dec 2021 Wikipedia dump text_chunks = [     {         \"id\": \"140\",         \"title\": \"History of marine biology\",         \"section\": \"James Cook\",         \"text\": \" James Cook is well known for his voyages of exploration for the British Navy in which he mapped out a significant amount of the world's uncharted waters. Cook's explorations took him around the world twice and led to countless descriptions of previously unknown plants and animals. Cook's explorations influenced many others and led to a number of scientists examining marine life more closely. Among those influenced was Charles Darwin who went on to make many contributions of his own. \",     },     {         \"id\": \"141\",         \"title\": \"History of marine biology\",         \"section\": \"Charles Darwin\",         \"text\": \" Charles Darwin, best known for his theory of evolution, made many significant contributions to the early study of marine biology. He spent much of his time from 1831 to 1836 on the voyage of HMS Beagle collecting and studying specimens from a variety of marine organisms. It was also on this expedition where Darwin began to study coral reefs and their formation. He came up with the theory that the overall growth of corals is a balance between the growth of corals upward and the sinking of the sea floor. He then came up with the idea that wherever coral atolls would be found, the central island where the coral had started to grow would be gradually subsiding\",     },     {         \"id\": \"142\",         \"title\": \"History of marine biology\",         \"section\": \"Charles Wyville Thomson\",         \"text\": \" Another influential expedition was the voyage of HMS Challenger from 1872 to 1876, organized and later led by Charles Wyville Thomson. It was the first expedition purely devoted to marine science. The expedition collected and analyzed thousands of marine specimens, laying the foundation for present knowledge about life near the deep-sea floor. The findings from the expedition were a summary of the known natural, physical and chemical ocean science to that time.\",     }, ] In\u00a0[6]: Copied! <pre>from fed_rag.data_structures import KnowledgeNode, NodeType\n\n# create knowledge nodes\nnodes = []\ntexts = []\nfor c in text_chunks:\n    text = c.pop(\"text\")\n    title = c.pop(\"title\")\n    section = c.pop(\"section\")\n    context_text = f\"title: {title}\\nsection: {section}\\ntext: {text}\"\n    texts.append(context_text)\n\n# batch encode\nbatch_embeddings = retriever.encode_context(texts)\n\nfor jx, c in enumerate(text_chunks):\n    node = KnowledgeNode(\n        embedding=batch_embeddings[jx].tolist(),\n        node_type=NodeType.TEXT,\n        text_content=texts[jx],\n        metadata=c,\n    )\n    nodes.append(node)\n</pre> from fed_rag.data_structures import KnowledgeNode, NodeType  # create knowledge nodes nodes = [] texts = [] for c in text_chunks:     text = c.pop(\"text\")     title = c.pop(\"title\")     section = c.pop(\"section\")     context_text = f\"title: {title}\\nsection: {section}\\ntext: {text}\"     texts.append(context_text)  # batch encode batch_embeddings = retriever.encode_context(texts)  for jx, c in enumerate(text_chunks):     node = KnowledgeNode(         embedding=batch_embeddings[jx].tolist(),         node_type=NodeType.TEXT,         text_content=texts[jx],         metadata=c,     )     nodes.append(node) In\u00a0[7]: Copied! <pre>knowledge_store.load_nodes(nodes)\n</pre> knowledge_store.load_nodes(nodes) In\u00a0[8]: Copied! <pre>knowledge_store.count\n</pre> knowledge_store.count Out[8]: <pre>3</pre> In\u00a0[9]: Copied! <pre>query = \"Who is James Cook?\"\nquery_emb = retriever.encode_query(query).tolist()\n</pre> query = \"Who is James Cook?\" query_emb = retriever.encode_query(query).tolist() In\u00a0[10]: Copied! <pre>retrieved_nodes = knowledge_store.retrieve(query_emb=query_emb, top_k=1)\n</pre> retrieved_nodes = knowledge_store.retrieve(query_emb=query_emb, top_k=1) In\u00a0[11]: Copied! <pre>similarity_score, knowledge_node = retrieved_nodes[0]\n</pre> similarity_score, knowledge_node = retrieved_nodes[0] In\u00a0[12]: Copied! <pre>print(\"Similarity score: \", similarity_score)\nprint(\"KnowledgeNode: \", knowledge_node)\n</pre> print(\"Similarity score: \", similarity_score) print(\"KnowledgeNode: \", knowledge_node) <pre>Similarity score:  0.49984106\nKnowledgeNode:  node_id='d641b713-62dd-46e7-86c2-4e3968ed5339' embedding=None node_type=&lt;NodeType.TEXT: 'text'&gt; text_content=\"title: History of marine biology\\nsection: James Cook\\ntext:  James Cook is well known for his voyages of exploration for the British Navy in which he mapped out a significant amount of the world's uncharted waters. Cook's explorations took him around the world twice and led to countless descriptions of previously unknown plants and animals. Cook's explorations influenced many others and led to a number of scientists examining marine life more closely. Among those influenced was Charles Darwin who went on to make many contributions of his own. \" image_content=None metadata={'id': '140'}\n</pre> In\u00a0[13]: Copied! <pre>if WITH_DOCKER:\n    # stop and remove container\n    container.stop()\n    container.remove()\n</pre> if WITH_DOCKER:     # stop and remove container     container.stop()     container.remove() In\u00a0[\u00a0]: Copied! <pre>knowledge_store = QdrantKnowledgeStore(\n    # qdrant credentials\n    api_key=\"...\",\n    host=\"...\",\n    collection_name=\"...\",\n    https=True,\n)\n</pre> knowledge_store = QdrantKnowledgeStore(     # qdrant credentials     api_key=\"...\",     host=\"...\",     collection_name=\"...\",     https=True, )"},{"location":"notebooks/integrations/qdrant/#using-qdrant-for-knowledge-storage","title":"Using Qdrant for Knowledge Storage\u00b6","text":""},{"location":"notebooks/integrations/qdrant/#introduction","title":"Introduction\u00b6","text":"<p>The <code>fed-rag</code> library supports a simple, in-memory knowledge store for rapid creation and development cycles of RAG systems. For larger scale fine-tuning jobs, you may need a more optimized knowledge store. FedRAG supports a seamless Qdrant integration in the form of the <code>QdrantKnowledgeStore</code>, allowing you to connect to any Qdrant service\u2014whether running locally or in a managed/cloud environment.</p> <p>In this notebook, we demonstrate how to launch a local Qdrant service and use it as the knowledge storage for your RAG system.</p>"},{"location":"notebooks/integrations/qdrant/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>The <code>QdrantKnowledgeStore</code> requires the installation of the <code>qdrant</code> extra. Note that we also will use a HuggingFace <code>SentenceTransformer</code> as the retriever/embedding model to encode our knowledge artifacts prior to loading them to our knowledge store.</p>"},{"location":"notebooks/integrations/qdrant/#launch-a-local-qdrant-service-with-docker","title":"Launch a Local Qdrant Service (with Docker)\u00b6","text":"<p>This step assumes that you have docker installed on your machine. If not installed, refer to the official Docker docs for installation found here.</p> <p>IMPORTANT NOTE: if you are running this within a Google Colab, you won't be able to run a docker image. Instead, you can run the rest of this notebook by using an in-memory instance of Qdrant.</p> <p>If using a Colab, set the <code>WITH_DOCKER</code> to <code>False</code></p>"},{"location":"notebooks/integrations/qdrant/#setup-the-retriever-and-qdrantknowledgestore","title":"Setup the Retriever and <code>QdrantKnowledgeStore</code>\u00b6","text":""},{"location":"notebooks/integrations/qdrant/#lets-add-some-knowledge","title":"Let's Add Some Knowledge\u00b6","text":""},{"location":"notebooks/integrations/qdrant/#retriever-from-the-knowledge-store","title":"Retriever From The Knowledge Store\u00b6","text":""},{"location":"notebooks/integrations/qdrant/#clean-up","title":"Clean up\u00b6","text":""},{"location":"notebooks/integrations/qdrant/#note-on-connecting-to-managed-qdrant-service","title":"Note on Connecting to Managed Qdrant Service\u00b6","text":"<p>If you have a managed Qdrant service, then connecting to is easy. Simply pass in the credentials (i.e., api_key), the host name, the collection name at instantiation.</p>"},{"location":"notebooks/integrations/unsloth/","title":"\ud83e\udda5 Using Unsloth FastModels as your RAG Generator Model","text":"<p>(NOTE: if running on Colab, you will need to supply a WandB API Key in addition to your HFToken. Also, you'll need to change the runtime to a T4.)</p> In\u00a0[\u00a0]: Copied! <pre>!uv pip install fed-rag[huggingface,unsloth] -q\n</pre> !uv pip install fed-rag[huggingface,unsloth] -q In\u00a0[\u00a0]: Copied! <pre>!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n!uv pip install --no-deps \"xformers&lt;0.0.27\" \"trl&lt;0.9.0\" peft accelerate bitsandbytes -q\n</pre> !uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q !uv pip install --no-deps \"xformers&lt;0.0.27\" \"trl&lt;0.9.0\" peft accelerate bitsandbytes -q In\u00a0[1]: Copied! <pre>import unsloth\n</pre> import unsloth <pre>\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n</pre> In\u00a0[2]: Copied! <pre>from fed_rag.generators import UnslothFastModelGenerator\n</pre> from fed_rag.generators import UnslothFastModelGenerator In\u00a0[3]: Copied! <pre>from transformers.generation.utils import GenerationConfig\n\ngeneration_cfg = GenerationConfig(\n    do_sample=True,\n    eos_token_id=[1, 106],\n    bos_token_id=2,\n    max_new_tokens=2048,\n    pad_token_id=0,\n    top_p=0.95,\n    top_k=64,\n    temperature=0.6,\n    cache_implementation=\"offloaded\",\n)\n</pre> from transformers.generation.utils import GenerationConfig  generation_cfg = GenerationConfig(     do_sample=True,     eos_token_id=[1, 106],     bos_token_id=2,     max_new_tokens=2048,     pad_token_id=0,     top_p=0.95,     top_k=64,     temperature=0.6,     cache_implementation=\"offloaded\", ) In\u00a0[4]: Copied! <pre>unsloth_load_kwargs = {\n    \"max_seq_length\": 2048,  # Choose any for long context!\n    \"load_in_4bit\": True,\n    \"load_in_8bit\": False,  # [NEW!] A bit more accurate, uses 2x memory\n    \"full_finetuning\": False,  # [NEW!] We have full finetuning now!\n}\ngenerator = UnslothFastModelGenerator(\n    model_name=\"unsloth/gemma-3-4b-it\",\n    load_model_kwargs=unsloth_load_kwargs,\n    generation_config=generation_cfg,\n)\n</pre> unsloth_load_kwargs = {     \"max_seq_length\": 2048,  # Choose any for long context!     \"load_in_4bit\": True,     \"load_in_8bit\": False,  # [NEW!] A bit more accurate, uses 2x memory     \"full_finetuning\": False,  # [NEW!] We have full finetuning now! } generator = UnslothFastModelGenerator(     model_name=\"unsloth/gemma-3-4b-it\",     load_model_kwargs=unsloth_load_kwargs,     generation_config=generation_cfg, ) <pre>==((====))==  Unsloth 2025.5.7: Fast Gemma3 patching. Transformers: 4.51.3.\n   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.448 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n</pre> <pre>Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n</pre> In\u00a0[5]: Copied! <pre>generator.model.dtype\n</pre> generator.model.dtype Out[5]: <pre>torch.bfloat16</pre> In\u00a0[6]: Copied! <pre>response = generator.generate(query=\"What is a Tulip?\", context=\"\")\nprint(response)\n</pre> response = generator.generate(query=\"What is a Tulip?\", context=\"\") print(response) <pre>The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n</pre> <pre>\nYou are a helpful assistant. Given the user's query, provide a succinct\nand accurate response. If context is provided, use it in your answer if it helps\nyou to create the most accurate response.\n\n&lt;query&gt;\nWhat is a Tulip?\n&lt;/query&gt;\n\n&lt;context&gt;\n\n&lt;/context&gt;\n\n&lt;response&gt;\n\nA tulip is a flowering plant in the genus *Tulipa*, native to Central Asia and Turkey. They are known for their cup-shaped flowers and are often associated with spring.\n&lt;/response&gt;\n\n</pre> <p>In Unsloth's Gemma 3 (4B) cookbook.ipynb), they demonstrate how to use <code>~transformers.TextStreamer</code> to stream generation output in real-time rather than waiting for completion. We can apply the same technique here.</p> In\u00a0[7]: Copied! <pre>from transformers import TextStreamer\n\ngenerator.generate(\n    query=\"What is a Porshe?\",\n    context=\"\",\n    streamer=TextStreamer(generator.tokenizer.unwrapped, skip_prompt=True),\n)\n</pre> from transformers import TextStreamer  generator.generate(     query=\"What is a Porshe?\",     context=\"\",     streamer=TextStreamer(generator.tokenizer.unwrapped, skip_prompt=True), ) <pre>Porsche is a German automobile manufacturer known for its high-performance sports cars, luxury vehicles, and SUVs. The company was founded in 1931 by Ferdinand Porsche.\n\n&lt;/response&gt;\n&lt;end_of_turn&gt;\n</pre> Out[7]: <pre>\"\\nYou are a helpful assistant. Given the user's query, provide a succinct\\nand accurate response. If context is provided, use it in your answer if it helps\\nyou to create the most accurate response.\\n\\n&lt;query&gt;\\nWhat is a Porshe?\\n&lt;/query&gt;\\n\\n&lt;context&gt;\\n\\n&lt;/context&gt;\\n\\n&lt;response&gt;\\n\\nPorsche is a German automobile manufacturer known for its high-performance sports cars, luxury vehicles, and SUVs. The company was founded in 1931 by Ferdinand Porsche.\\n\\n&lt;/response&gt;\\n\"</pre> In\u00a0[10]: Copied! <pre>import torch\n\nfrom fed_rag import RAGSystem, RAGConfig\nfrom fed_rag.retrievers.huggingface import (\n    HFSentenceTransformerRetriever,\n)\nfrom fed_rag.knowledge_stores import InMemoryKnowledgeStore\nfrom fed_rag.data_structures import KnowledgeNode, NodeType\n\n\nQUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\"\nCONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\"\nPRETRAINED_MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n\n# Retriever\nretriever = HFSentenceTransformerRetriever(\n    query_model_name=QUERY_ENCODER_NAME,\n    context_model_name=CONTEXT_ENCODER_NAME,\n    load_model_at_init=False,\n)\n\n# Knowledge store\nknowledge_store = InMemoryKnowledgeStore()\n</pre> import torch  from fed_rag import RAGSystem, RAGConfig from fed_rag.retrievers.huggingface import (     HFSentenceTransformerRetriever, ) from fed_rag.knowledge_stores import InMemoryKnowledgeStore from fed_rag.data_structures import KnowledgeNode, NodeType   QUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\" CONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\" PRETRAINED_MODEL_NAME = \"Qwen/Qwen3-0.6B\"  # Retriever retriever = HFSentenceTransformerRetriever(     query_model_name=QUERY_ENCODER_NAME,     context_model_name=CONTEXT_ENCODER_NAME,     load_model_at_init=False, )  # Knowledge store knowledge_store = InMemoryKnowledgeStore() In\u00a0[11]: Copied! <pre>text_chunks = [\n    \"Retrieval-Augmented Generation (RAG) combines retrieval with generation.\",\n    \"LLMs can hallucinate information when they lack context.\",\n]\nknowledge_nodes = [\n    KnowledgeNode(\n        node_type=\"text\",\n        embedding=retriever.encode_context(ct).tolist(),\n        text_content=ct,\n    )\n    for ct in text_chunks\n]\nknowledge_store.load_nodes(knowledge_nodes)\n</pre> text_chunks = [     \"Retrieval-Augmented Generation (RAG) combines retrieval with generation.\",     \"LLMs can hallucinate information when they lack context.\", ] knowledge_nodes = [     KnowledgeNode(         node_type=\"text\",         embedding=retriever.encode_context(ct).tolist(),         text_content=ct,     )     for ct in text_chunks ] knowledge_store.load_nodes(knowledge_nodes) In\u00a0[12]: Copied! <pre>knowledge_store.count\n</pre> knowledge_store.count Out[12]: <pre>2</pre> In\u00a0[13]: Copied! <pre># Create the RAG system\nrag_system = RAGSystem(\n    retriever=retriever,\n    generator=generator,\n    knowledge_store=knowledge_store,\n    rag_config=RAGConfig(top_k=1),\n)\n</pre> # Create the RAG system rag_system = RAGSystem(     retriever=retriever,     generator=generator,     knowledge_store=knowledge_store,     rag_config=RAGConfig(top_k=1), ) In\u00a0[14]: Copied! <pre>generator.to_peft(\n    finetune_vision_layers=False,  # Turn off for just text!\n    finetune_language_layers=True,  # Should leave on!\n    finetune_attention_modules=True,  # Attention good for GRPO\n    finetune_mlp_modules=True,  # SHould leave on always!\n    r=8,  # Larger = higher accuracy, but might overfit\n    lora_alpha=8,  # Recommended alpha == r at least\n    lora_dropout=0,\n    bias=\"none\",\n    random_state=3407,\n)\n</pre> generator.to_peft(     finetune_vision_layers=False,  # Turn off for just text!     finetune_language_layers=True,  # Should leave on!     finetune_attention_modules=True,  # Attention good for GRPO     finetune_mlp_modules=True,  # SHould leave on always!     r=8,  # Larger = higher accuracy, but might overfit     lora_alpha=8,  # Recommended alpha == r at least     lora_dropout=0,     bias=\"none\",     random_state=3407, ) <pre>Unsloth: Making `base_model.model.vision_tower.vision_model` require gradients\n</pre> Out[14]: <pre>UnslothFastModelGenerator(model_name='unsloth/gemma-3-4b-it', generation_config=GenerationConfig {\n  \"bos_token_id\": 2,\n  \"cache_implementation\": \"hybrid\",\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    1,\n    106\n  ],\n  \"max_new_tokens\": 2048,\n  \"pad_token_id\": 0,\n  \"temperature\": 0.6,\n  \"top_k\": 64,\n  \"top_p\": 0.95\n}\n, load_model_kwargs={'max_seq_length': 2048, 'load_in_4bit': True, 'load_in_8bit': False, 'full_finetuning': False})</pre> In\u00a0[15]: Copied! <pre>generator.model.dtype\n</pre> generator.model.dtype Out[15]: <pre>torch.bfloat16</pre> In\u00a0[16]: Copied! <pre>from peft import PeftModel\n\nisinstance(generator.model, PeftModel)\n</pre> from peft import PeftModel  isinstance(generator.model, PeftModel) Out[16]: <pre>True</pre> In\u00a0[17]: Copied! <pre>from datasets import Dataset\n\ntrain_dataset = Dataset.from_dict(\n    # examples from Commonsense QA\n    {\n        \"query\": [\n            \"The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\",\n            \"Sammy wanted to go to where the people were.  Where might he go?\",\n            \"To locate a choker not located in a jewelry box or boutique where would you go?\",\n            \"Google Maps and other highway and street GPS services have replaced what?\",\n            \"The fox walked from the city into the forest, what was it looking for?\",\n        ],\n        \"response\": [\n            \"ignore\",\n            \"populated areas\",\n            \"jewelry store\",\n            \"atlas\",\n            \"natural habitat\",\n        ],\n    }\n)\n</pre> from datasets import Dataset  train_dataset = Dataset.from_dict(     # examples from Commonsense QA     {         \"query\": [             \"The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\",             \"Sammy wanted to go to where the people were.  Where might he go?\",             \"To locate a choker not located in a jewelry box or boutique where would you go?\",             \"Google Maps and other highway and street GPS services have replaced what?\",             \"The fox walked from the city into the forest, what was it looking for?\",         ],         \"response\": [             \"ignore\",             \"populated areas\",             \"jewelry store\",             \"atlas\",             \"natural habitat\",         ],     } ) <p>Since, Unsloth essentially applies efficiencies to the training processes of <code>~transformer.PreTrainedModels</code> as well as <code>~peft.PeftModels</code>, we can make full use of our HuggingFace generator trainer classes.</p> In\u00a0[18]: Copied! <pre>from fed_rag.trainers.huggingface.ralt import HuggingFaceTrainerForRALT\n\n# the trainer object\ngenerator_trainer = HuggingFaceTrainerForRALT(\n    rag_system=rag_system,\n    train_dataset=train_dataset,\n    # training_arguments=...  # Optional ~transformers.TrainingArguments\n)\n</pre> from fed_rag.trainers.huggingface.ralt import HuggingFaceTrainerForRALT  # the trainer object generator_trainer = HuggingFaceTrainerForRALT(     rag_system=rag_system,     train_dataset=train_dataset,     # training_arguments=...  # Optional ~transformers.TrainingArguments ) In\u00a0[19]: Copied! <pre>result = generator_trainer.train()\n</pre> result = generator_trainer.train() <pre>==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 5 | Num Epochs = 3 | Total steps = 3\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n \"-____-\"     Trainable parameters = 16,394,240/4,000,000,000 (0.41% trained)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n</pre>        [3/3 00:01, Epoch 3/3]      Step Training Loss <p> </p> <pre>Unsloth: Will smartly offload gradients to save VRAM!\n</pre> In\u00a0[20]: Copied! <pre>result\n</pre> result Out[20]: <pre>TrainResult(loss=3.530837059020996)</pre>"},{"location":"notebooks/integrations/unsloth/#using-unsloth-fastmodels-as-your-rag-generator-model","title":"\ud83e\udda5 Using Unsloth FastModels as your RAG Generator Model\u00b6","text":""},{"location":"notebooks/integrations/unsloth/#introduction","title":"Introduction\u00b6","text":"<p>As of <code>v0.0.20</code>, the <code>fed-rag</code> library includes seamless integration with Unsloth.ai, a popular open-source library that dramatically accelerates fine-tuning workflows. This integration allows you to use <code>~unsloth.FastLanguageModel</code> instances as generator models in your RAG system while fully leveraging Unsloth's efficient fine-tuning capabilities.</p> <p>In this notebook, we demonstrate how to define a <code>UnslothFastModelGenerator</code>, integrate it into a RAG system, and fine-tune it using our <code>GeneratorTrainers</code>.</p> <p>NOTE: This notebook takes inspiration from Unsloth's cookbook.ipynb), for fine-tuning Gemma3 4B\u2014we'll use that exact same model as our generator in our RAG system. The key difference is that we're fine-tuning the model specifically for retrieval-augmented generation tasks using our <code>fed-rag</code> framework.</p>"},{"location":"notebooks/integrations/unsloth/#creating-an-unslothfastmodelgenerator","title":"Creating an <code>UnslothFastModelGenerator</code>\u00b6","text":""},{"location":"notebooks/integrations/unsloth/#give-it-a-spin","title":"Give it a spin\u00b6","text":""},{"location":"notebooks/integrations/unsloth/#lets-build-the-rest-of-our-rag-system","title":"Let's Build the Rest of our RAG System\u00b6","text":""},{"location":"notebooks/integrations/unsloth/#define-our-retriever-and-knowledge-store","title":"Define our Retriever and Knowledge Store\u00b6","text":""},{"location":"notebooks/integrations/unsloth/#add-some-knowledge-to-the-store","title":"Add some knowledge to the store\u00b6","text":""},{"location":"notebooks/integrations/unsloth/#assemble-the-rag-system","title":"Assemble the RAG system\u00b6","text":""},{"location":"notebooks/integrations/unsloth/#time-to-fine-tune","title":"Time to Fine-tune!\u00b6","text":"<p>Now, that we have our RAG system defined, let's proceed with fine-tuning the generator with the RALT method.</p>"},{"location":"notebooks/integrations/unsloth/#lets-first-add-our-lora-adapters","title":"Let's first add our LoRA adapters\u00b6","text":"<p>In order to do so, we use the <code>to_peft()</code> method, which under the hood will call the <code>FastModel.get_peft_model()</code> to build the <code>PeftModel</code>, and then set it as this generators model. In other words, the underlying model is currently a <code>PreTrainedModel</code>, but after executing the next cell, it will be a <code>PeftModel</code>.</p>"},{"location":"notebooks/integrations/unsloth/#the-train-dataset","title":"The Train Dataset\u00b6","text":""}]}