{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/decorators/","title":"Trainer and Tester Decorators","text":"<p>Trainer Decorators</p> <p>Tester Decorators</p>"},{"location":"api_reference/decorators/#src.fed_rag.decorators.trainer.TrainerDecorators","title":"<code>TrainerDecorators</code>","text":"Source code in <code>src/fed_rag/decorators/trainer.py</code> <pre><code>class TrainerDecorators:\n    def pytorch(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.pytorch import inspect_trainer_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_trainer_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_trainer_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n\n    def huggingface(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.huggingface import inspect_trainer_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_trainer_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_trainer_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n</code></pre>"},{"location":"api_reference/decorators/#src.fed_rag.decorators.tester.TesterDecorators","title":"<code>TesterDecorators</code>","text":"Source code in <code>src/fed_rag/decorators/tester.py</code> <pre><code>class TesterDecorators:\n    def pytorch(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.pytorch import inspect_tester_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_tester_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_tester_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n\n    def huggingface(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.huggingface import inspect_tester_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_tester_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_tester_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n</code></pre>"},{"location":"api_reference/fl_tasks/","title":"Base FL Task Classes","text":"<p>Base FL Task</p>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask","title":"<code>BaseFLTask</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>class BaseFLTask(BaseModel, ABC):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def training_loop(self) -&gt; Callable:\n        ...\n\n    @classmethod\n    @abstractmethod\n    def from_configs(\n        cls, trainer_cfg: BaseFLTaskConfig, tester_cfg: Any\n    ) -&gt; Self:\n        ...\n\n    @classmethod\n    @abstractmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        try:\n            trainer_cfg = getattr(trainer, \"__fl_task_trainer_config\")\n        except AttributeError:\n            msg = (\n                \"`__fl_task_trainer_config` has not been set on training loop. Make \"\n                \"sure to decorate your training loop with the appropriate \"\n                \"decorator.\"\n            )\n            raise MissingFLTaskConfig(msg)\n\n        try:\n            tester_cfg = getattr(tester, \"__fl_task_tester_config\")\n        except AttributeError:\n            msg = (\n                \"`__fl_task_tester_config` has not been set on tester callable. Make \"\n                \"sure to decorate your tester with the appropriate decorator.\"\n            )\n            raise MissingFLTaskConfig(msg)\n        return cls.from_configs(trainer_cfg, tester_cfg)\n\n    @abstractmethod\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        \"\"\"Simulate the FL task.\n\n        Either use flwr's simulation tools, or create our own here.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def server(self, **kwargs: Any) -&gt; Server:\n        \"\"\"Create a flwr.Server object.\"\"\"\n        ...\n\n    @abstractmethod\n    def client(self, **kwargs: Any) -&gt; Client:\n        \"\"\"Create a flwr.Client object.\"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.simulate","title":"<code>simulate(num_clients, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Simulate the FL task.</p> <p>Either use flwr's simulation tools, or create our own here.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n    \"\"\"Simulate the FL task.\n\n    Either use flwr's simulation tools, or create our own here.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.server","title":"<code>server(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create a flwr.Server object.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef server(self, **kwargs: Any) -&gt; Server:\n    \"\"\"Create a flwr.Server object.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.client","title":"<code>client(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create a flwr.Client object.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef client(self, **kwargs: Any) -&gt; Client:\n    \"\"\"Create a flwr.Client object.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTaskConfig","title":"<code>BaseFLTaskConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>class BaseFLTaskConfig(BaseModel):\n    pass\n</code></pre>"},{"location":"api_reference/fl_tasks/huggingface/","title":"Huggingface","text":"<p>HuggingFace FL Task.</p> <p>NOTE: Using this module requires the <code>huggingface</code> extra to be installed.</p>"},{"location":"api_reference/fl_tasks/huggingface/#src.fed_rag.fl_tasks.huggingface.HuggingFaceFlowerClient","title":"<code>HuggingFaceFlowerClient</code>","text":"<p>               Bases: <code>NumPyClient</code></p> Source code in <code>src/fed_rag/fl_tasks/huggingface.py</code> <pre><code>class HuggingFaceFlowerClient(NumPyClient):\n    def __init__(\n        self,\n        task_bundle: BaseFLTaskBundle,\n    ) -&gt; None:\n        super().__init__()\n        self.task_bundle = task_bundle\n\n    def __getattr__(self, name: str) -&gt; Any:\n        if name in self.task_bundle.model_fields:\n            return getattr(self.task_bundle, name)\n        else:\n            return super().__getattr__(name)\n\n    def get_weights(self) -&gt; NDArrays:\n        return _get_weights(self.net)\n\n    def set_weights(self, parameters: NDArrays) -&gt; None:\n        if isinstance(self.net, PeftModel):\n            # get state dict\n            state_dict = get_peft_model_state_dict(self.net)\n            state_dict = cast(Dict[str, Any], state_dict)\n\n            # update state dict with supplied parameters\n            params_dict = zip(state_dict.keys(), parameters)\n            state_dict = OrderedDict(\n                {k: torch.tensor(v) for k, v in params_dict}\n            )\n            set_peft_model_state_dict(self.net, state_dict)\n        else:  # SentenceTransformer | PreTrainedModel\n            # get state dict\n            state_dict = self.net.state_dict()\n\n            # update state dict with supplied parameters\n            params_dict = zip(state_dict.keys(), parameters)\n            state_dict = OrderedDict(\n                {k: torch.tensor(v) for k, v in params_dict}\n            )\n            self.net.load_state_dict(state_dict, strict=True)\n\n    def fit(\n        self, parameters: NDArrays, config: dict[str, Scalar]\n    ) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n        self.set_weights(parameters)\n\n        result: TrainResult = self.trainer(\n            self.net,\n            self.train_dataset,\n            self.val_dataset,\n            **self.task_bundle.extra_train_kwargs,\n        )\n        return (\n            self.get_weights(),\n            len(self.train_dataset),\n            {\"loss\": result.loss},\n        )\n\n    def evaluate(\n        self, parameters: NDArrays, config: dict[str, Scalar]\n    ) -&gt; tuple[float, int, dict[str, Scalar]]:\n        self.set_weights(parameters)\n        result: TestResult = self.tester(\n            self.net, self.val_dataset, **self.extra_test_kwargs\n        )\n        return result.loss, len(self.val_dataset), result.metrics\n</code></pre>"},{"location":"api_reference/fl_tasks/huggingface/#src.fed_rag.fl_tasks.huggingface.HuggingFaceFLTask","title":"<code>HuggingFaceFLTask</code>","text":"<p>               Bases: <code>BaseFLTask</code></p> Source code in <code>src/fed_rag/fl_tasks/huggingface.py</code> <pre><code>class HuggingFaceFLTask(BaseFLTask):\n    _client: NumPyClient = PrivateAttr()\n    _trainer: Callable = PrivateAttr()\n    _trainer_spec: TrainerSignatureSpec = PrivateAttr()\n    _tester: Callable = PrivateAttr()\n    _tester_spec: TesterSignatureSpec = PrivateAttr()\n\n    def __init__(\n        self,\n        trainer: Callable,\n        trainer_spec: TrainerSignatureSpec,\n        tester: Callable,\n        tester_spec: TesterSignatureSpec,\n        **kwargs: Any,\n    ) -&gt; None:\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise ValueError(msg)\n\n        if (\n            trainer_spec.net_parameter_class_name\n            != tester_spec.net_parameter_class_name\n        ):\n            msg = (\n                \"`trainer`'s model class is not the same as that for `tester`.\"\n            )\n            raise NetTypeMismatch(msg)\n\n        if trainer_spec.net_parameter != tester_spec.net_parameter:\n            msg = (\n                \"`trainer`'s model parameter name is not the same as that for `tester`. \"\n                \"Will use the name supplied in `trainer`.\"\n            )\n            warnings.warn(msg, UnequalNetParamWarning)\n\n        super().__init__(**kwargs)\n        self._trainer = trainer\n        self._trainer_spec = trainer_spec\n        self._tester = tester\n        self._tester_spec = tester_spec\n\n    @property\n    def training_loop(self) -&gt; Callable:\n        return self._trainer\n\n    @classmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        # extract trainer spec\n        try:\n            trainer_spec: TrainerSignatureSpec = getattr(\n                trainer, \"__fl_task_trainer_config\"\n            )\n        except AttributeError:\n            msg = \"Cannot extract `TrainerSignatureSpec` from supplied `trainer`.\"\n            raise MissingTrainerSpec(msg)\n\n        # extract tester spec\n        try:\n            tester_spec: TesterSignatureSpec = getattr(\n                tester, \"__fl_task_tester_config\"\n            )\n        except AttributeError:\n            msg = (\n                \"Cannot extract `TesterSignatureSpec` from supplied `tester`.\"\n            )\n            raise MissingTesterSpec(msg)\n\n        return cls(\n            trainer=trainer,\n            trainer_spec=trainer_spec,\n            tester=tester,\n            tester_spec=tester_spec,\n        )\n\n    def server(\n        self,\n        strategy: Strategy | None = None,\n        client_manager: ClientManager | None = None,\n        **kwargs: Any,\n    ) -&gt; HuggingFaceFlowerServer | None:\n        if strategy is None:\n            if self._trainer_spec.net_parameter not in kwargs:\n                msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n                raise MissingRequiredNetParam(msg)\n\n            model = kwargs.pop(self._trainer_spec.net_parameter)\n\n            ndarrays = _get_weights(model)\n            parameters = ndarrays_to_parameters(ndarrays)\n            strategy = FedAvg(\n                fraction_evaluate=1.0,\n                initial_parameters=parameters,\n            )\n\n        if client_manager is None:\n            client_manager = SimpleClientManager()\n\n        return HuggingFaceFlowerServer(\n            client_manager=client_manager, strategy=strategy\n        )\n\n    def client(self, **kwargs: Any) -&gt; Client | None:\n        # validate kwargs\n        if self._trainer_spec.net_parameter not in kwargs:\n            msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n            raise MissingRequiredNetParam(msg)\n        # build bundle\n        net = kwargs.pop(self._trainer_spec.net_parameter)\n        train_dataset = kwargs.pop(self._trainer_spec.train_data_param)\n        val_dataset = kwargs.pop(self._trainer_spec.val_data_param)\n\n        bundle = BaseFLTaskBundle(\n            net=net,\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            extra_train_kwargs=kwargs,\n            extra_test_kwargs={},  # TODO make this functional or get rid of it\n            trainer=self._trainer,\n            tester=self._tester,\n        )\n        return HuggingFaceFlowerClient(task_bundle=bundle)\n\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        raise NotImplementedError\n\n    @classmethod\n    def from_configs(cls, trainer_cfg: Any, tester_cfg: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/fl_tasks/pytorch/","title":"Pytorch","text":"<p>PyTorch FL Task</p>"},{"location":"api_reference/fl_tasks/pytorch/#src.fed_rag.fl_tasks.pytorch.PyTorchFLTask","title":"<code>PyTorchFLTask</code>","text":"<p>               Bases: <code>BaseFLTask</code></p> Source code in <code>src/fed_rag/fl_tasks/pytorch.py</code> <pre><code>class PyTorchFLTask(BaseFLTask):\n    _client: NumPyClient = PrivateAttr()\n    _trainer: Callable = PrivateAttr()\n    _trainer_spec: TrainerSignatureSpec = PrivateAttr()\n    _tester: Callable = PrivateAttr()\n    _tester_spec: TesterSignatureSpec = PrivateAttr()\n\n    def __init__(\n        self,\n        trainer: Callable,\n        trainer_spec: TrainerSignatureSpec,\n        tester: Callable,\n        tester_spec: TesterSignatureSpec,\n        **kwargs: Any,\n    ) -&gt; None:\n        if trainer_spec.net_parameter != tester_spec.net_parameter:\n            msg = (\n                \"`trainer`'s model parameter name is not the same as that for `tester`. \"\n                \"Will use the name supplied in `trainer`.\"\n            )\n            warnings.warn(msg, UnequalNetParamWarning)\n\n        super().__init__(**kwargs)\n        self._trainer = trainer\n        self._trainer_spec = trainer_spec\n        self._tester = tester\n        self._tester_spec = tester_spec\n\n    @property\n    def training_loop(self) -&gt; Callable:\n        return self._trainer\n\n    @classmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        # extract trainer spec\n        try:\n            trainer_spec: TrainerSignatureSpec = getattr(\n                trainer, \"__fl_task_trainer_config\"\n            )\n        except AttributeError:\n            msg = \"Cannot extract `TrainerSignatureSpec` from supplied `trainer`.\"\n            raise MissingTrainerSpec(msg)\n\n        # extract tester spec\n        try:\n            tester_spec: TesterSignatureSpec = getattr(\n                tester, \"__fl_task_tester_config\"\n            )\n        except AttributeError:\n            msg = (\n                \"Cannot extract `TesterSignatureSpec` from supplied `tester`.\"\n            )\n            raise MissingTesterSpec(msg)\n\n        return cls(\n            trainer=trainer,\n            trainer_spec=trainer_spec,\n            tester=tester,\n            tester_spec=tester_spec,\n        )\n\n    @classmethod\n    def from_configs(cls, trainer_cfg: Any, tester_cfg: Any) -&gt; Any:\n        return super().from_configs(trainer_cfg, tester_cfg)\n\n    def server(\n        self,\n        strategy: Strategy | None = None,\n        client_manager: ClientManager | None = None,\n        **kwargs: Any,\n    ) -&gt; PyTorchFlowerServer | None:\n        if strategy is None:\n            if self._trainer_spec.net_parameter not in kwargs:\n                msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n                raise MissingRequiredNetParam(msg)\n\n            model = kwargs.pop(self._trainer_spec.net_parameter)\n\n            ndarrays = _get_weights(model)\n            parameters = ndarrays_to_parameters(ndarrays)\n            strategy = FedAvg(\n                fraction_evaluate=1.0,\n                initial_parameters=parameters,\n            )\n\n        if client_manager is None:\n            client_manager = SimpleClientManager()\n\n        return PyTorchFlowerServer(\n            client_manager=client_manager, strategy=strategy\n        )\n\n    def client(self, **kwargs: Any) -&gt; Client | None:\n        # validate kwargs\n        if self._trainer_spec.net_parameter not in kwargs:\n            msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n            raise MissingRequiredNetParam(msg)\n        # build bundle\n        net = kwargs.pop(self._trainer_spec.net_parameter)\n        trainloader = kwargs.pop(self._trainer_spec.train_data_param)\n        valloader = kwargs.pop(self._trainer_spec.val_data_param)\n\n        bundle = BaseFLTaskBundle(\n            net=net,\n            trainloader=trainloader,\n            valloader=valloader,\n            extra_train_kwargs=kwargs,\n            extra_test_kwargs={},  # TODO make this functional or get rid of it\n            trainer=self._trainer,\n            tester=self._tester,\n        )\n        return PyTorchFlowerClient(task_bundle=bundle)\n\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/inspectors/","title":"Inspectors","text":"<p>Common abstractions for inspectors</p>"},{"location":"api_reference/inspectors/#src.fed_rag.inspectors.common.TesterSignatureSpec","title":"<code>TesterSignatureSpec</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/inspectors/common.py</code> <pre><code>class TesterSignatureSpec(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    net_parameter: str\n    test_data_param: str\n    extra_test_kwargs: list[str] = []\n    net_parameter_class_name: str\n</code></pre>"},{"location":"api_reference/inspectors/#src.fed_rag.inspectors.common.TesterSignatureSpec","title":"<code>TesterSignatureSpec</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/inspectors/common.py</code> <pre><code>class TesterSignatureSpec(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    net_parameter: str\n    test_data_param: str\n    extra_test_kwargs: list[str] = []\n    net_parameter_class_name: str\n</code></pre>"},{"location":"api_reference/inspectors/huggingface/","title":"Huggingface","text":""},{"location":"api_reference/inspectors/huggingface/#src.fed_rag.inspectors.huggingface.inspect_trainer_signature","title":"<code>inspect_trainer_signature(fn)</code>","text":"Source code in <code>src/fed_rag/inspectors/huggingface/trainer.py</code> <pre><code>def inspect_trainer_signature(fn: Callable) -&gt; TrainerSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TrainResult):\n        msg = \"Trainer should return a fed_rag.types.TrainResult or a subclass of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_train_kwargs = []\n    net_param = None\n    train_data_param = None\n    val_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if (\n                type_name\n                in [\n                    \"SentenceTransformer\",\n                    \"PreTrainedModel\",\n                    \"PeftModel\",\n                ]  # TODO: should accept union types involving these two\n                and net_param is None\n            ):\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"Dataset\" and train_data_param is None:\n                train_data_param = name\n                continue\n\n            if type_name == \"Dataset\" and val_data_param is None:\n                val_data_param = name\n                continue\n\n        extra_train_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For HuggingFace this param must have type `PreTrainedModel` or `SentenceTransformers`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if train_data_param is None:\n        msg = (\n            \"Inspection failed to find two data params for train and val datasets.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingMultipleDataParams(msg)\n\n    if val_data_param is None:\n        msg = (\n            \"Inspection found one data param but failed to find another. \"\n            \"Two data params are required for train and val datasets.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TrainerSignatureSpec(\n        net_parameter=net_param,\n        train_data_param=train_data_param,\n        val_data_param=val_data_param,\n        extra_train_kwargs=extra_train_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/huggingface/#src.fed_rag.inspectors.huggingface.inspect_tester_signature","title":"<code>inspect_tester_signature(fn)</code>","text":"Source code in <code>src/fed_rag/inspectors/huggingface/tester.py</code> <pre><code>def inspect_tester_signature(fn: Callable) -&gt; TesterSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TestResult):\n        msg = \"Tester should return a fed_rag.types.TestResult or a subclass of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_tester_kwargs = []\n    net_param = None\n    test_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if (\n                type_name\n                in [\"PreTrainedModel\", \"SentenceTransformer\", \"PeftModel\"]\n                and net_param is None\n            ):\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"Dataset\" and test_data_param is None:\n                test_data_param = name\n                continue\n\n        extra_tester_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For HuggingFace this param must have type `PreTrainedModel` or `SentenceTransformers`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if test_data_param is None:\n        msg = (\n            \"Inspection failed to find a data param for a test dataset.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TesterSignatureSpec(\n        net_parameter=net_param,\n        test_data_param=test_data_param,\n        extra_test_kwargs=extra_tester_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/pytorch/","title":"Pytorch","text":""},{"location":"api_reference/inspectors/pytorch/#src.fed_rag.inspectors.pytorch.inspect_trainer_signature","title":"<code>inspect_trainer_signature(fn)</code>","text":"Source code in <code>src/fed_rag/inspectors/pytorch/trainer.py</code> <pre><code>def inspect_trainer_signature(fn: Callable) -&gt; TrainerSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TrainResult):\n        msg = \"Trainer should return a fed_rag.types.TrainResult or a subclsas of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_train_kwargs = []\n    net_param = None\n    train_data_param = None\n    val_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if type_name == \"Module\" and net_param is None:\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"DataLoader\" and train_data_param is None:\n                train_data_param = name\n                continue\n\n            if type_name == \"DataLoader\" and val_data_param is None:\n                val_data_param = name\n                continue\n\n        extra_train_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For PyTorch this param must have type `nn.Module`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if train_data_param is None:\n        msg = (\n            \"Inspection failed to find two data params for train and val datasets.\"\n            \"For PyTorch these params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingMultipleDataParams(msg)\n\n    if val_data_param is None:\n        msg = (\n            \"Inspection found one data param but failed to find another. \"\n            \"Two data params are required for train and val datasets.\"\n            \"For PyTorch these params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TrainerSignatureSpec(\n        net_parameter=net_param,\n        train_data_param=train_data_param,\n        val_data_param=val_data_param,\n        extra_train_kwargs=extra_train_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/pytorch/#src.fed_rag.inspectors.pytorch.inspect_tester_signature","title":"<code>inspect_tester_signature(fn)</code>","text":"Source code in <code>src/fed_rag/inspectors/pytorch/tester.py</code> <pre><code>def inspect_tester_signature(fn: Callable) -&gt; TesterSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TestResult):\n        msg = \"Tester should return a fed_rag.types.TestResult or a subclsas of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_tester_kwargs = []\n    net_param = None\n    test_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if type_name == \"Module\" and net_param is None:\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"DataLoader\" and test_data_param is None:\n                test_data_param = name\n                continue\n\n        extra_tester_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For PyTorch this param must have type `nn.Module`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if test_data_param is None:\n        msg = (\n            \"Inspection failed to find a data param for a test dataset.\"\n            \"For PyTorch this params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TesterSignatureSpec(\n        net_parameter=net_param,\n        test_data_param=test_data_param,\n        extra_test_kwargs=extra_tester_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/knowledge_stores/","title":"Base KnowledgeStore","text":"<p>Base Knowledge Store</p>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore","title":"<code>BaseKnowledgeStore</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Knowledge Store Class.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>class BaseKnowledgeStore(BaseModel, ABC):\n    \"\"\"Base Knowledge Store Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def load_node(self, node: KnowledgeNode) -&gt; None:\n        \"\"\"Load a KnowledgeNode into the KnowledgeStore.\"\"\"\n\n    @abstractmethod\n    def load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n        \"\"\"Load multiple KnowledgeNodes in batch.\"\"\"\n\n    @abstractmethod\n    def retrieve(\n        self, query_emb: list[float], top_k: int\n    ) -&gt; list[tuple[float, KnowledgeNode]]:\n        \"\"\"Retrieve top-k nodes from KnowledgeStore against a provided user query.\n\n        Returns:\n            A list of tuples where the first element represents the similarity score\n            of the node to the query, and the second element is the node itself.\n        \"\"\"\n\n    @abstractmethod\n    def delete_node(self, node_id: str) -&gt; bool:\n        \"\"\"Remove a node from the KnowledgeStore by ID, returning success status.\"\"\"\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Clear all nodes from the KnowledgeStore.\"\"\"\n\n    @property\n    @abstractmethod\n    def count(self) -&gt; int:\n        \"\"\"Return the number of nodes in the store.\"\"\"\n\n    @abstractmethod\n    def persist(self) -&gt; None:\n        \"\"\"Save the KnowledgeStore nodes to a permanent storage.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def load(cls, ks_id: str) -&gt; Self:\n        \"\"\"\n        Load the KnowledgeStore nodes from a permanent storage given an id.\n\n        Args:\n            ks_id: The id of the knowledge store to load.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.count","title":"<code>count</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the number of nodes in the store.</p>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load_node","title":"<code>load_node(node)</code>  <code>abstractmethod</code>","text":"<p>Load a KnowledgeNode into the KnowledgeStore.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef load_node(self, node: KnowledgeNode) -&gt; None:\n    \"\"\"Load a KnowledgeNode into the KnowledgeStore.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load_nodes","title":"<code>load_nodes(nodes)</code>  <code>abstractmethod</code>","text":"<p>Load multiple KnowledgeNodes in batch.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n    \"\"\"Load multiple KnowledgeNodes in batch.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.retrieve","title":"<code>retrieve(query_emb, top_k)</code>  <code>abstractmethod</code>","text":"<p>Retrieve top-k nodes from KnowledgeStore against a provided user query.</p> <p>Returns:</p> Type Description <code>list[tuple[float, KnowledgeNode]]</code> <p>A list of tuples where the first element represents the similarity score</p> <code>list[tuple[float, KnowledgeNode]]</code> <p>of the node to the query, and the second element is the node itself.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef retrieve(\n    self, query_emb: list[float], top_k: int\n) -&gt; list[tuple[float, KnowledgeNode]]:\n    \"\"\"Retrieve top-k nodes from KnowledgeStore against a provided user query.\n\n    Returns:\n        A list of tuples where the first element represents the similarity score\n        of the node to the query, and the second element is the node itself.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.delete_node","title":"<code>delete_node(node_id)</code>  <code>abstractmethod</code>","text":"<p>Remove a node from the KnowledgeStore by ID, returning success status.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef delete_node(self, node_id: str) -&gt; bool:\n    \"\"\"Remove a node from the KnowledgeStore by ID, returning success status.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Clear all nodes from the KnowledgeStore.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Clear all nodes from the KnowledgeStore.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.persist","title":"<code>persist()</code>  <code>abstractmethod</code>","text":"<p>Save the KnowledgeStore nodes to a permanent storage.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef persist(self) -&gt; None:\n    \"\"\"Save the KnowledgeStore nodes to a permanent storage.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load","title":"<code>load(ks_id)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Load the KnowledgeStore nodes from a permanent storage given an id.</p> <p>Parameters:</p> Name Type Description Default <code>ks_id</code> <code>str</code> <p>The id of the knowledge store to load.</p> required Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@classmethod\n@abstractmethod\ndef load(cls, ks_id: str) -&gt; Self:\n    \"\"\"\n    Load the KnowledgeStore nodes from a permanent storage given an id.\n\n    Args:\n        ks_id: The id of the knowledge store to load.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/in_memory/","title":"In memory","text":"<p>In Memory Knowledge Store</p>"},{"location":"api_reference/knowledge_stores/in_memory/#src.fed_rag.knowledge_stores.in_memory.InMemoryKnowledgeStore","title":"<code>InMemoryKnowledgeStore</code>","text":"<p>               Bases: <code>BaseKnowledgeStore</code></p> <p>InMemoryKnowledgeStore Class.</p> Source code in <code>src/fed_rag/knowledge_stores/in_memory.py</code> <pre><code>class InMemoryKnowledgeStore(BaseKnowledgeStore):\n    \"\"\"InMemoryKnowledgeStore Class.\"\"\"\n\n    default_save_path: ClassVar[str] = \".fed_rag/data_cache/{0}.parquet\"\n\n    ks_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    _data: dict[str, KnowledgeNode] = PrivateAttr(default_factory=dict)\n\n    @classmethod\n    def from_nodes(cls, nodes: list[KnowledgeNode]) -&gt; Self:\n        instance = cls()\n        instance.load_nodes(nodes)\n        return instance\n\n    def load_node(self, node: KnowledgeNode) -&gt; None:\n        if node.node_id not in self._data:\n            self._data[node.node_id] = node\n\n    def load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n        for node in nodes:\n            self.load_node(node)\n\n    def retrieve(\n        self, query_emb: list[float], top_k: int\n    ) -&gt; list[tuple[float, KnowledgeNode]]:\n        all_nodes = list(self._data.values())\n        node_ids_and_scores = _get_top_k_nodes(\n            nodes=all_nodes, query_emb=query_emb, top_k=top_k\n        )\n        return [(el[1], self._data[el[0]]) for el in node_ids_and_scores]\n\n    def delete_node(self, node_id: str) -&gt; bool:\n        if node_id in self._data:\n            del self._data[node_id]\n            return True\n        else:\n            return False\n\n    def clear(self) -&gt; None:\n        self._data = {}\n\n    @property\n    def count(self) -&gt; int:\n        return len(self._data)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(self, handler: Any) -&gt; Dict[str, Any]:\n        data = handler(self)\n        data = cast(Dict[str, Any], data)\n        # include _data in serialization\n        if self._data:\n            data[\"_data\"] = self._data\n        return data  # type: ignore[no-any-return]\n\n    def persist(self) -&gt; None:\n        serialized_model = self.model_dump()\n        data_values = list(serialized_model[\"_data\"].values())\n\n        parquet_table = pa.Table.from_pylist(data_values)\n\n        filename = self.__class__.default_save_path.format(self.ks_id)\n        Path(filename).parent.mkdir(parents=True, exist_ok=True)\n        pq.write_table(parquet_table, filename)\n\n    @classmethod\n    def load(cls, ks_id: str) -&gt; Self:\n        filename = cls.default_save_path.format(ks_id)\n        parquet_data = pq.read_table(filename).to_pylist()\n\n        knowledge_store = cls.from_nodes(\n            [KnowledgeNode(**data) for data in parquet_data]\n        )\n        knowledge_store.ks_id = ks_id\n        return knowledge_store\n</code></pre>"}]}