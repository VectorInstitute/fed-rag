{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#fedrag","title":"FedRAG","text":""},{"location":"#simplified-rag-fine-tuning-across-centralized-or-federated-architectures","title":"Simplified RAG fine-tuning across centralized or federated architectures Advanced RAG fine-tuning Work with your tools Lightweight abstractions","text":"<ul> <li> <p>Comprehensive support for state-of-the-art RAG fine-tuning methods that can be federated with ease.</p> <p> Getting started</p> </li> <li> <p></p> <p>Seamlessly integrates with popular frameworks including HuggingFace, and LlamaIndex \u2014 use the tools you already know.</p> <p> Examples</p> </li> <li> <p></p> <p>Clean, intuitive abstractions that simplify RAG fine-tuning while maintaining full flexibility and control.</p> <p> API Reference</p> </li> </ul>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/data_collators/","title":"Base Data Collator","text":"<p>Base Data Collator</p>"},{"location":"api_reference/data_collators/#src.fed_rag.base.data_collator.BaseDataCollator","title":"BaseDataCollator","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Data Collator.</p> Source code in <code>src/fed_rag/base/data_collator.py</code> <pre><code>class BaseDataCollator(BaseModel, ABC):\n    \"\"\"Base Data Collator.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    rag_system: RAGSystem\n\n    @abstractmethod\n    def __call__(self, features: list[dict[str, Any]], **kwargs: Any) -&gt; Any:\n        \"\"\"Collate examples into a batch.\"\"\"\n</code></pre>"},{"location":"api_reference/data_collators/#src.fed_rag.base.data_collator.BaseDataCollator.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(features, **kwargs)\n</code></pre> <p>Collate examples into a batch.</p> Source code in <code>src/fed_rag/base/data_collator.py</code> <pre><code>@abstractmethod\ndef __call__(self, features: list[dict[str, Any]], **kwargs: Any) -&gt; Any:\n    \"\"\"Collate examples into a batch.\"\"\"\n</code></pre>"},{"location":"api_reference/data_collators/huggingface/","title":"Huggingface","text":"<p>HuggingFace Data Collator For LM-Supervised Retriever Training</p>"},{"location":"api_reference/data_collators/huggingface/#src.fed_rag.data_collators.huggingface.lsr.DataCollatorForLSR","title":"DataCollatorForLSR","text":"<p>               Bases: <code>SentenceTransformerDataCollator</code>, <code>BaseDataCollator</code></p> <p>A HuggingFace DataCollator for LM-Supervised Retrieval.</p> Source code in <code>src/fed_rag/data_collators/huggingface/lsr.py</code> <pre><code>class DataCollatorForLSR(SentenceTransformerDataCollator, BaseDataCollator):\n    \"\"\"A HuggingFace DataCollator for LM-Supervised Retrieval.\"\"\"\n\n    prompt_template: str = Field(default=DEFAULT_PROMPT_TEMPLATE)\n    target_template: str = Field(default=DEFAULT_TARGET_TEMPLATE)\n    default_return_tensors: str = Field(default=\"pt\")\n\n    # Add these fields to make Pydantic aware of them\n    tokenize_fn: Callable = Field(\n        default_factory=lambda: (lambda *args, **kwargs: {})\n    )\n    valid_label_columns: list[str] = Field(\n        default_factory=lambda: [\"label\", \"score\"]\n    )\n    _warned_columns: set = PrivateAttr(\n        default_factory=set\n    )  # exclude=True to match dataclass repr=False\n\n    def __init__(\n        self,\n        rag_system: RAGSystem,\n        prompt_template: str | None = None,\n        target_template: str | None = None,\n        default_return_tensors: str = \"pt\",\n        **kwargs: Any,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        _validate_rag_system(rag_system)\n\n        prompt_template = prompt_template or DEFAULT_PROMPT_TEMPLATE\n        target_template = target_template or DEFAULT_TARGET_TEMPLATE\n\n        # init pydantic base model\n        BaseDataCollator.__init__(\n            self,\n            rag_system=rag_system,\n            prompt_template=prompt_template,\n            target_template=target_template,\n            default_return_tensors=default_return_tensors,\n            tokenize_fn=lambda *args, **kwargs: {},  # Pass this to Pydantic\n            **kwargs,\n        )\n\n    def __call__(\n        self, features: list[dict[str, Any]], return_tensors: str | None = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"Use the features of the dataset in order to get the retrieval and lm-scores.\n\n\n        Args:\n            features (list[Any]): Should contain a 'query' and 'reponse' field.\n            return_tensors (_type_, optional): supports right now only 'pt'\n\n        Returns:\n            dict[str, Any]: a dictionary of ~torch.Tensors with keys 'retrieval_scores'\n                and 'lm_scores'\n            Note that each ('query', 'response') pair generates one fine-tuning instance for LSR.\n        \"\"\"\n        return_tensors = (\n            return_tensors if return_tensors else self.default_return_tensors\n        )\n        if return_tensors != \"pt\":\n            raise FedRAGError(f\"Framework '{return_tensors}' not recognized!\")\n\n        # use rag system to get scores\n        batch_retriever_scores = []\n        batch_lm_scores = []\n        for example in features:\n            query = example.get(\"query\")\n            response = example.get(\"response\")\n\n            # retriever scores - this should participate in gradient computation\n            source_nodes = self.rag_system.retrieve(query)\n            retriever_scores = torch.tensor(\n                [n.score for n in source_nodes], requires_grad=True\n            )\n\n            # lm supervised scores - we don't want these to participate in gradient computation\n            lm_scores = []\n            with torch.no_grad():\n                for chunk in source_nodes:\n                    prompt = self.prompt_template.format(\n                        query=query,\n                        context=chunk.node.get_content()[\"text_content\"],\n                    )\n                    target = self.target_template.format(response=response)\n                    lm_score = self.rag_system.generator.compute_target_sequence_proba(\n                        prompt=prompt, target=target\n                    )\n                    lm_scores.append(lm_score)\n                lm_scores = torch.stack(lm_scores, dim=0)\n\n            # append to batch\n            batch_retriever_scores.append(retriever_scores)\n            batch_lm_scores.append(lm_scores)\n\n        # create torch.Tensors\n        retrieval_scores = torch.stack(batch_retriever_scores, dim=0)\n        lm_scores = torch.stack(batch_lm_scores, dim=0)\n\n        return {\"retrieval_scores\": retrieval_scores, \"lm_scores\": lm_scores}\n</code></pre>"},{"location":"api_reference/data_collators/huggingface/#src.fed_rag.data_collators.huggingface.lsr.DataCollatorForLSR.__call__","title":"__call__","text":"<pre><code>__call__(features, return_tensors=None)\n</code></pre> <p>Use the features of the dataset in order to get the retrieval and lm-scores.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[Any]</code> <p>Should contain a 'query' and 'reponse' field.</p> required <code>return_tensors</code> <code>_type_</code> <p>supports right now only 'pt'</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: a dictionary of ~torch.Tensors with keys 'retrieval_scores' and 'lm_scores'</p> <code>dict[str, Any]</code> <p>Note that each ('query', 'response') pair generates one fine-tuning instance for LSR.</p> Source code in <code>src/fed_rag/data_collators/huggingface/lsr.py</code> <pre><code>def __call__(\n    self, features: list[dict[str, Any]], return_tensors: str | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"Use the features of the dataset in order to get the retrieval and lm-scores.\n\n\n    Args:\n        features (list[Any]): Should contain a 'query' and 'reponse' field.\n        return_tensors (_type_, optional): supports right now only 'pt'\n\n    Returns:\n        dict[str, Any]: a dictionary of ~torch.Tensors with keys 'retrieval_scores'\n            and 'lm_scores'\n        Note that each ('query', 'response') pair generates one fine-tuning instance for LSR.\n    \"\"\"\n    return_tensors = (\n        return_tensors if return_tensors else self.default_return_tensors\n    )\n    if return_tensors != \"pt\":\n        raise FedRAGError(f\"Framework '{return_tensors}' not recognized!\")\n\n    # use rag system to get scores\n    batch_retriever_scores = []\n    batch_lm_scores = []\n    for example in features:\n        query = example.get(\"query\")\n        response = example.get(\"response\")\n\n        # retriever scores - this should participate in gradient computation\n        source_nodes = self.rag_system.retrieve(query)\n        retriever_scores = torch.tensor(\n            [n.score for n in source_nodes], requires_grad=True\n        )\n\n        # lm supervised scores - we don't want these to participate in gradient computation\n        lm_scores = []\n        with torch.no_grad():\n            for chunk in source_nodes:\n                prompt = self.prompt_template.format(\n                    query=query,\n                    context=chunk.node.get_content()[\"text_content\"],\n                )\n                target = self.target_template.format(response=response)\n                lm_score = self.rag_system.generator.compute_target_sequence_proba(\n                    prompt=prompt, target=target\n                )\n                lm_scores.append(lm_score)\n            lm_scores = torch.stack(lm_scores, dim=0)\n\n        # append to batch\n        batch_retriever_scores.append(retriever_scores)\n        batch_lm_scores.append(lm_scores)\n\n    # create torch.Tensors\n    retrieval_scores = torch.stack(batch_retriever_scores, dim=0)\n    lm_scores = torch.stack(batch_lm_scores, dim=0)\n\n    return {\"retrieval_scores\": retrieval_scores, \"lm_scores\": lm_scores}\n</code></pre>"},{"location":"api_reference/decorators/","title":"Trainer and Tester Decorators","text":"<p>Trainer Decorators</p> <p>Tester Decorators</p>"},{"location":"api_reference/decorators/#src.fed_rag.decorators.trainer.TrainerDecorators","title":"TrainerDecorators","text":"Source code in <code>src/fed_rag/decorators/trainer.py</code> <pre><code>class TrainerDecorators:\n    def pytorch(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.pytorch import inspect_trainer_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_trainer_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_trainer_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n\n    def huggingface(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.huggingface import inspect_trainer_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_trainer_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_trainer_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n</code></pre>"},{"location":"api_reference/decorators/#src.fed_rag.decorators.tester.TesterDecorators","title":"TesterDecorators","text":"Source code in <code>src/fed_rag/decorators/tester.py</code> <pre><code>class TesterDecorators:\n    def pytorch(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.pytorch import inspect_tester_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_tester_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_tester_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n\n    def huggingface(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.huggingface import inspect_tester_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_tester_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_tester_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n</code></pre>"},{"location":"api_reference/exceptions/","title":"FedRAG Exceptions","text":"<p>Base Error Class for FedRAG.</p>"},{"location":"api_reference/exceptions/#src.fed_rag.exceptions.core.FedRAGError","title":"FedRAGError","text":"<p>               Bases: <code>Exception</code></p> <p>Base error for all fed-rag exceptions.</p> Source code in <code>src/fed_rag/exceptions/core.py</code> <pre><code>class FedRAGError(Exception):\n    \"\"\"Base error for all fed-rag exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/#src.fed_rag.exceptions.core.FedRAGWarning","title":"FedRAGWarning","text":"<p>               Bases: <code>Warning</code></p> <p>Base warning for all fed-rag warnings.</p> Source code in <code>src/fed_rag/exceptions/core.py</code> <pre><code>class FedRAGWarning(Warning):\n    \"\"\"Base warning for all fed-rag warnings.\"\"\"\n</code></pre>"},{"location":"api_reference/exceptions/fl_tasks/","title":"FL Tasks","text":"<p>Exceptions for FL Tasks.</p>"},{"location":"api_reference/exceptions/fl_tasks/#src.fed_rag.exceptions.fl_tasks.FLTaskError","title":"FLTaskError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base fl task error for all fl-task-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/fl_tasks.py</code> <pre><code>class FLTaskError(FedRAGError):\n    \"\"\"Base fl task error for all fl-task-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/fl_tasks/#src.fed_rag.exceptions.fl_tasks.MissingRequiredNetParam","title":"MissingRequiredNetParam","text":"<p>               Bases: <code>FLTaskError</code></p> <p>Raised when invoking fl_task.server without passing the specified model/net param.</p> Source code in <code>src/fed_rag/exceptions/fl_tasks.py</code> <pre><code>class MissingRequiredNetParam(FLTaskError):\n    \"\"\"Raised when invoking fl_task.server without passing the specified model/net param.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/fl_tasks/#src.fed_rag.exceptions.fl_tasks.NetTypeMismatch","title":"NetTypeMismatch","text":"<p>               Bases: <code>FLTaskError</code></p> <p>Raised when a <code>trainer</code> and <code>tester</code> spec have differing <code>net_parameter_class_name</code>.</p> <p>This indicates that the these methods have different types for the <code>net_parameter</code>.</p> Source code in <code>src/fed_rag/exceptions/fl_tasks.py</code> <pre><code>class NetTypeMismatch(FLTaskError):\n    \"\"\"Raised when a `trainer` and `tester` spec have differing `net_parameter_class_name`.\n\n    This indicates that the these methods have different types for the `net_parameter`.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/inspectors/","title":"Inspectors","text":"<p>Exceptions for inspectors.</p>"},{"location":"api_reference/exceptions/inspectors/#src.fed_rag.exceptions.inspectors.InspectorError","title":"InspectorError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base inspector error for all inspector-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/inspectors.py</code> <pre><code>class InspectorError(FedRAGError):\n    \"\"\"Base inspector error for all inspector-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/inspectors/#src.fed_rag.exceptions.inspectors.InspectorWarning","title":"InspectorWarning","text":"<p>               Bases: <code>FedRAGWarning</code></p> <p>Base inspector warning for all inspector-related warnings.</p> Source code in <code>src/fed_rag/exceptions/inspectors.py</code> <pre><code>class InspectorWarning(FedRAGWarning):\n    \"\"\"Base inspector warning for all inspector-related warnings.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/knowledge_stores/","title":"Knowledge Stores","text":"<p>Exceptions for Knowledge Stores.</p>"},{"location":"api_reference/exceptions/knowledge_stores/#src.fed_rag.exceptions.knowledge_stores.KnowledgeStoreError","title":"KnowledgeStoreError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base knowledge store error for all knowledge-store-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/knowledge_stores.py</code> <pre><code>class KnowledgeStoreError(FedRAGError):\n    \"\"\"Base knowledge store error for all knowledge-store-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/loss/","title":"Loss","text":"<p>Exceptions for loss.</p>"},{"location":"api_reference/exceptions/loss/#src.fed_rag.exceptions.loss.LossError","title":"LossError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base loss errors for all loss-related exceptions.</p> Source code in <code>src/fed_rag/exceptions/loss.py</code> <pre><code>class LossError(FedRAGError):\n    \"\"\"Base loss errors for all loss-related exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/exceptions/rag_trainer/","title":"RAG Trainer","text":""},{"location":"api_reference/exceptions/rag_trainer/#src.fed_rag.exceptions.trainer_manager.RAGTrainerManagerError","title":"RAGTrainerManagerError","text":"<p>               Bases: <code>FedRAGError</code></p> <p>Base errors for all rag trainer manager relevant exceptions.</p> Source code in <code>src/fed_rag/exceptions/trainer_manager.py</code> <pre><code>class RAGTrainerManagerError(FedRAGError):\n    \"\"\"Base errors for all rag trainer manager relevant exceptions.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/finetuning_datasets/","title":"Finetuning Datasets","text":"<p>Data utils</p>"},{"location":"api_reference/finetuning_datasets/#src.fed_rag.utils.data._functions.ReturnType","title":"ReturnType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/fed_rag/utils/data/_functions.py</code> <pre><code>class ReturnType(str, Enum):\n    PYTORCH = \"pt\"\n    HUGGINGFACE = \"hf\"\n    TEXT = \"txt\"\n</code></pre>"},{"location":"api_reference/finetuning_datasets/#src.fed_rag.utils.data._functions.build_finetune_dataset","title":"build_finetune_dataset","text":"<pre><code>build_finetune_dataset(\n    rag_system,\n    examples,\n    eos_token_id,\n    finetune_example_template=DEFAULT_FINETUNE_EXAMPLE_TEMPLATE,\n    query_key=\"query\",\n    answer_key=\"answer\",\n    return_dataset=ReturnType.PYTORCH,\n)\n</code></pre> <p>Generates the finetuning dataset using the supplied rag_system and examples.</p> Source code in <code>src/fed_rag/utils/data/_functions.py</code> <pre><code>def build_finetune_dataset(\n    rag_system: RAGSystem,\n    examples: Sequence[dict],\n    eos_token_id: int,\n    finetune_example_template: str = DEFAULT_FINETUNE_EXAMPLE_TEMPLATE,\n    query_key: str = \"query\",\n    answer_key: str = \"answer\",\n    return_dataset: ReturnType = ReturnType.PYTORCH,\n) -&gt; Any:\n    \"\"\"Generates the finetuning dataset using the supplied rag_system and examples.\"\"\"\n\n    if (\n        isinstance(return_dataset, str)\n        and return_dataset not in ReturnType._value2member_map_.keys()\n    ):\n        raise ValueError(\n            \"Invalid `return_type` specified.\"\n        )  # TODO: give a proper exception to this\n\n    inputs_list = []\n    targets_list = []\n    attention_mask_list = []\n    finetuning_instances = []\n    for example in examples:\n        # retrieve\n        source_nodes = rag_system.retrieve(query=example[query_key])\n        total_sum_scores = sum(s.score for s in source_nodes)\n\n        # parallel in-context retrieval-augmentation creates\n        # top_k separated finetuning instances\n        for source in source_nodes:\n            finetune_instance_text = finetune_example_template.format(\n                query=example[query_key],\n                answer=example[answer_key],\n                context=source.node.get_content()[\"text_content\"],\n            )\n            finetuning_instances.append(finetune_instance_text)\n            _weight = source.score / total_sum_scores\n\n            # tokenize to get input_ids and target_ids\n            tokenizer = rag_system.generator.tokenizer\n            encode_result = tokenizer.encode(finetune_instance_text)\n            input_ids = encode_result[\"input_ids\"]\n            attention_mask = encode_result[\"attention_mask\"]\n            target_ids = input_ids[1:] + [eos_token_id]\n\n            inputs_list.append(input_ids)\n            targets_list.append(target_ids)\n            attention_mask_list.append(attention_mask)\n\n    if return_dataset == ReturnType.TEXT:\n        return finetuning_instances\n    elif return_dataset == ReturnType.PYTORCH:\n        return PyTorchRAGFinetuningDataset(\n            input_ids=[torch.Tensor(el) for el in inputs_list],\n            target_ids=[torch.Tensor(el) for el in targets_list],\n        )\n    elif return_dataset == ReturnType.HUGGINGFACE:\n        # needs `fed-rag[huggingface]` extra to be installed\n        # this import will fail if not installed\n        from fed_rag.utils.data.finetuning_datasets.huggingface import (\n            HuggingFaceRAGFinetuningDataset,\n        )\n\n        return HuggingFaceRAGFinetuningDataset.from_inputs(\n            input_ids=inputs_list,\n            target_ids=targets_list,\n            attention_mask=attention_mask_list,\n        )\n    else:\n        assert_never(return_dataset)  # pragma: no cover\n</code></pre>"},{"location":"api_reference/finetuning_datasets/huggingface/","title":"Huggingface","text":"<p>HuggingFace RAG Finetuning Dataset</p>"},{"location":"api_reference/finetuning_datasets/huggingface/#src.fed_rag.utils.data.finetuning_datasets.huggingface.HuggingFaceRAGFinetuningDataset","title":"HuggingFaceRAGFinetuningDataset","text":"<p>               Bases: <code>Dataset</code></p> <p>Thin wrapper over ~datasets.Dataset.</p> Source code in <code>src/fed_rag/utils/data/finetuning_datasets/huggingface.py</code> <pre><code>class HuggingFaceRAGFinetuningDataset(Dataset):\n    \"\"\"Thin wrapper over ~datasets.Dataset.\"\"\"\n\n    @classmethod\n    def from_inputs(\n        cls,\n        input_ids: list[list[int]],\n        target_ids: list[list[int]],\n        attention_mask: list[list[int]],\n    ) -&gt; Self:\n        return cls.from_dict(  # type: ignore[no-any-return]\n            {\n                \"input_ids\": input_ids,\n                \"target_ids\": target_ids,\n                \"attention_mask\": attention_mask,\n            }\n        )\n</code></pre>"},{"location":"api_reference/finetuning_datasets/pytorch/","title":"Pytorch","text":"<p>PyTorch RAG Finetuning Dataset</p>"},{"location":"api_reference/finetuning_datasets/pytorch/#src.fed_rag.utils.data.finetuning_datasets.pytorch.PyTorchRAGFinetuningDataset","title":"PyTorchRAGFinetuningDataset","text":"<p>               Bases: <code>Dataset</code></p> <p>PyTorch RAG Fine-Tuning Dataset Class.</p> <p>Parameters:</p> Name Type Description Default <code>Dataset</code> <code>_type_</code> <p>description</p> required Source code in <code>src/fed_rag/utils/data/finetuning_datasets/pytorch.py</code> <pre><code>class PyTorchRAGFinetuningDataset(Dataset):\n    \"\"\"PyTorch RAG Fine-Tuning Dataset Class.\n\n    Args:\n        Dataset (_type_): _description_\n    \"\"\"\n\n    def __init__(\n        self, input_ids: list[torch.Tensor], target_ids: list[torch.Tensor]\n    ):\n        self.input_ids = input_ids\n        self.target_ids = target_ids\n\n    def __len__(self) -&gt; int:\n        return len(self.input_ids)\n\n    def __getitem__(self, idx: int) -&gt; Any:\n        return self.input_ids[idx], self.target_ids[idx]\n</code></pre>"},{"location":"api_reference/fl_tasks/","title":"Base FL Task Classes","text":"<p>Base FL Task</p>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask","title":"BaseFLTask","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>class BaseFLTask(BaseModel, ABC):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def training_loop(self) -&gt; Callable:\n        ...\n\n    @classmethod\n    @abstractmethod\n    def from_configs(\n        cls, trainer_cfg: BaseFLTaskConfig, tester_cfg: Any\n    ) -&gt; Self:\n        ...\n\n    @classmethod\n    @abstractmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        try:\n            trainer_cfg = getattr(trainer, \"__fl_task_trainer_config\")\n        except AttributeError:\n            msg = (\n                \"`__fl_task_trainer_config` has not been set on training loop. Make \"\n                \"sure to decorate your training loop with the appropriate \"\n                \"decorator.\"\n            )\n            raise MissingFLTaskConfig(msg)\n\n        try:\n            tester_cfg = getattr(tester, \"__fl_task_tester_config\")\n        except AttributeError:\n            msg = (\n                \"`__fl_task_tester_config` has not been set on tester callable. Make \"\n                \"sure to decorate your tester with the appropriate decorator.\"\n            )\n            raise MissingFLTaskConfig(msg)\n        return cls.from_configs(trainer_cfg, tester_cfg)\n\n    @abstractmethod\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        \"\"\"Simulate the FL task.\n\n        Either use flwr's simulation tools, or create our own here.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def server(self, **kwargs: Any) -&gt; Server:\n        \"\"\"Create a flwr.Server object.\"\"\"\n        ...\n\n    @abstractmethod\n    def client(self, **kwargs: Any) -&gt; Client:\n        \"\"\"Create a flwr.Client object.\"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.simulate","title":"simulate  <code>abstractmethod</code>","text":"<pre><code>simulate(num_clients, **kwargs)\n</code></pre> <p>Simulate the FL task.</p> <p>Either use flwr's simulation tools, or create our own here.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n    \"\"\"Simulate the FL task.\n\n    Either use flwr's simulation tools, or create our own here.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.server","title":"server  <code>abstractmethod</code>","text":"<pre><code>server(**kwargs)\n</code></pre> <p>Create a flwr.Server object.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef server(self, **kwargs: Any) -&gt; Server:\n    \"\"\"Create a flwr.Server object.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.client","title":"client  <code>abstractmethod</code>","text":"<pre><code>client(**kwargs)\n</code></pre> <p>Create a flwr.Client object.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef client(self, **kwargs: Any) -&gt; Client:\n    \"\"\"Create a flwr.Client object.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTaskConfig","title":"BaseFLTaskConfig","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>class BaseFLTaskConfig(BaseModel):\n    pass\n</code></pre>"},{"location":"api_reference/fl_tasks/huggingface/","title":"Huggingface","text":"<p>HuggingFace FL Task.</p> <p>NOTE: Using this module requires the <code>huggingface</code> extra to be installed.</p>"},{"location":"api_reference/fl_tasks/huggingface/#src.fed_rag.fl_tasks.huggingface.HuggingFaceFlowerClient","title":"HuggingFaceFlowerClient","text":"<p>               Bases: <code>NumPyClient</code></p> Source code in <code>src/fed_rag/fl_tasks/huggingface.py</code> <pre><code>class HuggingFaceFlowerClient(NumPyClient):\n    def __init__(\n        self,\n        task_bundle: BaseFLTaskBundle,\n    ) -&gt; None:\n        super().__init__()\n        self.task_bundle = task_bundle\n\n    def __getattr__(self, name: str) -&gt; Any:\n        if name in self.task_bundle.model_fields:\n            return getattr(self.task_bundle, name)\n        else:\n            return super().__getattr__(name)\n\n    def get_weights(self) -&gt; NDArrays:\n        return _get_weights(self.net)\n\n    def set_weights(self, parameters: NDArrays) -&gt; None:\n        if isinstance(self.net, PeftModel):\n            # get state dict\n            state_dict = get_peft_model_state_dict(self.net)\n            state_dict = cast(Dict[str, Any], state_dict)\n\n            # update state dict with supplied parameters\n            params_dict = zip(state_dict.keys(), parameters)\n            state_dict = OrderedDict(\n                {k: torch.tensor(v) for k, v in params_dict}\n            )\n            set_peft_model_state_dict(self.net, state_dict)\n        else:  # SentenceTransformer | PreTrainedModel\n            # get state dict\n            state_dict = self.net.state_dict()\n\n            # update state dict with supplied parameters\n            params_dict = zip(state_dict.keys(), parameters)\n            state_dict = OrderedDict(\n                {k: torch.tensor(v) for k, v in params_dict}\n            )\n            self.net.load_state_dict(state_dict, strict=True)\n\n    def fit(\n        self, parameters: NDArrays, config: dict[str, Scalar]\n    ) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n        self.set_weights(parameters)\n\n        result: TrainResult = self.trainer(\n            self.net,\n            self.train_dataset,\n            self.val_dataset,\n            **self.task_bundle.extra_train_kwargs,\n        )\n        return (\n            self.get_weights(),\n            len(self.train_dataset),\n            {\"loss\": result.loss},\n        )\n\n    def evaluate(\n        self, parameters: NDArrays, config: dict[str, Scalar]\n    ) -&gt; tuple[float, int, dict[str, Scalar]]:\n        self.set_weights(parameters)\n        result: TestResult = self.tester(\n            self.net, self.val_dataset, **self.extra_test_kwargs\n        )\n        return result.loss, len(self.val_dataset), result.metrics\n</code></pre>"},{"location":"api_reference/fl_tasks/huggingface/#src.fed_rag.fl_tasks.huggingface.HuggingFaceFLTask","title":"HuggingFaceFLTask","text":"<p>               Bases: <code>BaseFLTask</code></p> Source code in <code>src/fed_rag/fl_tasks/huggingface.py</code> <pre><code>class HuggingFaceFLTask(BaseFLTask):\n    _client: NumPyClient = PrivateAttr()\n    _trainer: Callable = PrivateAttr()\n    _trainer_spec: TrainerSignatureSpec = PrivateAttr()\n    _tester: Callable = PrivateAttr()\n    _tester_spec: TesterSignatureSpec = PrivateAttr()\n\n    def __init__(\n        self,\n        trainer: Callable,\n        trainer_spec: TrainerSignatureSpec,\n        tester: Callable,\n        tester_spec: TesterSignatureSpec,\n        **kwargs: Any,\n    ) -&gt; None:\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        if (\n            trainer_spec.net_parameter_class_name\n            != tester_spec.net_parameter_class_name\n        ):\n            msg = (\n                \"`trainer`'s model class is not the same as that for `tester`.\"\n            )\n            raise NetTypeMismatch(msg)\n\n        if trainer_spec.net_parameter != tester_spec.net_parameter:\n            msg = (\n                \"`trainer`'s model parameter name is not the same as that for `tester`. \"\n                \"Will use the name supplied in `trainer`.\"\n            )\n            warnings.warn(msg, UnequalNetParamWarning)\n\n        super().__init__(**kwargs)\n        self._trainer = trainer\n        self._trainer_spec = trainer_spec\n        self._tester = tester\n        self._tester_spec = tester_spec\n\n    @property\n    def training_loop(self) -&gt; Callable:\n        return self._trainer\n\n    @classmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        # extract trainer spec\n        try:\n            trainer_spec: TrainerSignatureSpec = getattr(\n                trainer, \"__fl_task_trainer_config\"\n            )\n        except AttributeError:\n            msg = \"Cannot extract `TrainerSignatureSpec` from supplied `trainer`.\"\n            raise MissingTrainerSpec(msg)\n\n        # extract tester spec\n        try:\n            tester_spec: TesterSignatureSpec = getattr(\n                tester, \"__fl_task_tester_config\"\n            )\n        except AttributeError:\n            msg = (\n                \"Cannot extract `TesterSignatureSpec` from supplied `tester`.\"\n            )\n            raise MissingTesterSpec(msg)\n\n        return cls(\n            trainer=trainer,\n            trainer_spec=trainer_spec,\n            tester=tester,\n            tester_spec=tester_spec,\n        )\n\n    def server(\n        self,\n        strategy: Strategy | None = None,\n        client_manager: ClientManager | None = None,\n        **kwargs: Any,\n    ) -&gt; HuggingFaceFlowerServer | None:\n        if strategy is None:\n            if self._trainer_spec.net_parameter not in kwargs:\n                msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n                raise MissingRequiredNetParam(msg)\n\n            model = kwargs.pop(self._trainer_spec.net_parameter)\n\n            ndarrays = _get_weights(model)\n            parameters = ndarrays_to_parameters(ndarrays)\n            strategy = FedAvg(\n                fraction_evaluate=1.0,\n                initial_parameters=parameters,\n            )\n\n        if client_manager is None:\n            client_manager = SimpleClientManager()\n\n        return HuggingFaceFlowerServer(\n            client_manager=client_manager, strategy=strategy\n        )\n\n    def client(self, **kwargs: Any) -&gt; Client | None:\n        # validate kwargs\n        if self._trainer_spec.net_parameter not in kwargs:\n            msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n            raise MissingRequiredNetParam(msg)\n        # build bundle\n        net = kwargs.pop(self._trainer_spec.net_parameter)\n        train_dataset = kwargs.pop(self._trainer_spec.train_data_param)\n        val_dataset = kwargs.pop(self._trainer_spec.val_data_param)\n\n        bundle = BaseFLTaskBundle(\n            net=net,\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            extra_train_kwargs=kwargs,\n            extra_test_kwargs={},  # TODO make this functional or get rid of it\n            trainer=self._trainer,\n            tester=self._tester,\n        )\n        return HuggingFaceFlowerClient(task_bundle=bundle)\n\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        raise NotImplementedError\n\n    @classmethod\n    def from_configs(cls, trainer_cfg: Any, tester_cfg: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/fl_tasks/pytorch/","title":"Pytorch","text":"<p>PyTorch FL Task</p>"},{"location":"api_reference/fl_tasks/pytorch/#src.fed_rag.fl_tasks.pytorch.PyTorchFLTask","title":"PyTorchFLTask","text":"<p>               Bases: <code>BaseFLTask</code></p> Source code in <code>src/fed_rag/fl_tasks/pytorch.py</code> <pre><code>class PyTorchFLTask(BaseFLTask):\n    _client: NumPyClient = PrivateAttr()\n    _trainer: Callable = PrivateAttr()\n    _trainer_spec: TrainerSignatureSpec = PrivateAttr()\n    _tester: Callable = PrivateAttr()\n    _tester_spec: TesterSignatureSpec = PrivateAttr()\n\n    def __init__(\n        self,\n        trainer: Callable,\n        trainer_spec: TrainerSignatureSpec,\n        tester: Callable,\n        tester_spec: TesterSignatureSpec,\n        **kwargs: Any,\n    ) -&gt; None:\n        if trainer_spec.net_parameter != tester_spec.net_parameter:\n            msg = (\n                \"`trainer`'s model parameter name is not the same as that for `tester`. \"\n                \"Will use the name supplied in `trainer`.\"\n            )\n            warnings.warn(msg, UnequalNetParamWarning)\n\n        super().__init__(**kwargs)\n        self._trainer = trainer\n        self._trainer_spec = trainer_spec\n        self._tester = tester\n        self._tester_spec = tester_spec\n\n    @property\n    def training_loop(self) -&gt; Callable:\n        return self._trainer\n\n    @classmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        # extract trainer spec\n        try:\n            trainer_spec: TrainerSignatureSpec = getattr(\n                trainer, \"__fl_task_trainer_config\"\n            )\n        except AttributeError:\n            msg = \"Cannot extract `TrainerSignatureSpec` from supplied `trainer`.\"\n            raise MissingTrainerSpec(msg)\n\n        # extract tester spec\n        try:\n            tester_spec: TesterSignatureSpec = getattr(\n                tester, \"__fl_task_tester_config\"\n            )\n        except AttributeError:\n            msg = (\n                \"Cannot extract `TesterSignatureSpec` from supplied `tester`.\"\n            )\n            raise MissingTesterSpec(msg)\n\n        return cls(\n            trainer=trainer,\n            trainer_spec=trainer_spec,\n            tester=tester,\n            tester_spec=tester_spec,\n        )\n\n    @classmethod\n    def from_configs(cls, trainer_cfg: Any, tester_cfg: Any) -&gt; Any:\n        return super().from_configs(trainer_cfg, tester_cfg)\n\n    def server(\n        self,\n        strategy: Strategy | None = None,\n        client_manager: ClientManager | None = None,\n        **kwargs: Any,\n    ) -&gt; PyTorchFlowerServer | None:\n        if strategy is None:\n            if self._trainer_spec.net_parameter not in kwargs:\n                msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n                raise MissingRequiredNetParam(msg)\n\n            model = kwargs.pop(self._trainer_spec.net_parameter)\n\n            ndarrays = _get_weights(model)\n            parameters = ndarrays_to_parameters(ndarrays)\n            strategy = FedAvg(\n                fraction_evaluate=1.0,\n                initial_parameters=parameters,\n            )\n\n        if client_manager is None:\n            client_manager = SimpleClientManager()\n\n        return PyTorchFlowerServer(\n            client_manager=client_manager, strategy=strategy\n        )\n\n    def client(self, **kwargs: Any) -&gt; Client | None:\n        # validate kwargs\n        if self._trainer_spec.net_parameter not in kwargs:\n            msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n            raise MissingRequiredNetParam(msg)\n        # build bundle\n        net = kwargs.pop(self._trainer_spec.net_parameter)\n        trainloader = kwargs.pop(self._trainer_spec.train_data_param)\n        valloader = kwargs.pop(self._trainer_spec.val_data_param)\n\n        bundle = BaseFLTaskBundle(\n            net=net,\n            trainloader=trainloader,\n            valloader=valloader,\n            extra_train_kwargs=kwargs,\n            extra_test_kwargs={},  # TODO make this functional or get rid of it\n            trainer=self._trainer,\n            tester=self._tester,\n        )\n        return PyTorchFlowerClient(task_bundle=bundle)\n\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/generators/","title":"Generators","text":"<p>Base Generator</p>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator","title":"BaseGenerator","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Generator Class.</p> Source code in <code>src/fed_rag/base/generator.py</code> <pre><code>class BaseGenerator(BaseModel, ABC):\n    \"\"\"Base Generator Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def generate(self, query: str, context: str, **kwargs: dict) -&gt; str:\n        \"\"\"Generate an output from a given query and context.\"\"\"\n\n    @property\n    @abstractmethod\n    def model(self) -&gt; torch.nn.Module:\n        \"\"\"Model associated with this generator.\"\"\"\n\n    @property\n    @abstractmethod\n    def tokenizer(self) -&gt; BaseTokenizer:\n        \"\"\"Tokenizer associated with this generator.\"\"\"\n\n    @abstractmethod\n    def compute_target_sequence_proba(\n        self, prompt: str, target: str\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute P(target | prompt).\n\n        NOTE: this is used in LM Supervised Retriever fine-tuning.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.model","title":"model  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>model\n</code></pre> <p>Model associated with this generator.</p>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.tokenizer","title":"tokenizer  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>tokenizer\n</code></pre> <p>Tokenizer associated with this generator.</p>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(query, context, **kwargs)\n</code></pre> <p>Generate an output from a given query and context.</p> Source code in <code>src/fed_rag/base/generator.py</code> <pre><code>@abstractmethod\ndef generate(self, query: str, context: str, **kwargs: dict) -&gt; str:\n    \"\"\"Generate an output from a given query and context.\"\"\"\n</code></pre>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.compute_target_sequence_proba","title":"compute_target_sequence_proba  <code>abstractmethod</code>","text":"<pre><code>compute_target_sequence_proba(prompt, target)\n</code></pre> <p>Compute P(target | prompt).</p> <p>NOTE: this is used in LM Supervised Retriever fine-tuning.</p> Source code in <code>src/fed_rag/base/generator.py</code> <pre><code>@abstractmethod\ndef compute_target_sequence_proba(\n    self, prompt: str, target: str\n) -&gt; torch.Tensor:\n    \"\"\"Compute P(target | prompt).\n\n    NOTE: this is used in LM Supervised Retriever fine-tuning.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/generators/huggingface/","title":"Huggingface","text":"<p>HuggingFace PeftModel Generator</p> <p>HuggingFace PretrainedModel Generator</p>"},{"location":"api_reference/generators/huggingface/#src.fed_rag.generators.huggingface.hf_peft_model.HFPeftModelGenerator","title":"HFPeftModelGenerator","text":"<p>               Bases: <code>HuggingFaceGeneratorMixin</code>, <code>BaseGenerator</code></p> <p>HFPeftModelGenerator Class.</p> <p>NOTE: this class supports loading PeftModel's from HF Hub or from local. TODO: support loading custom models via a <code>~peft.Config</code> and <code>~peft.get_peft_model</code></p> Source code in <code>src/fed_rag/generators/huggingface/hf_peft_model.py</code> <pre><code>class HFPeftModelGenerator(HuggingFaceGeneratorMixin, BaseGenerator):\n    \"\"\"HFPeftModelGenerator Class.\n\n    NOTE: this class supports loading PeftModel's from HF Hub or from local.\n    TODO: support loading custom models via a `~peft.Config` and `~peft.get_peft_model`\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str = Field(\n        description=\"Name of Peft model. Used for loading model from HF hub or local.\"\n    )\n    base_model_name: str = Field(\n        description=\"Name of the frozen HuggingFace base model. Used for loading the model from HF hub or local.\"\n    )\n    generation_config: \"GenerationConfig\" = Field(\n        description=\"The generation config used for generating with the PreTrainedModel.\"\n    )\n    load_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading peft model from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    load_base_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading base model from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    prompt_template: str = Field(description=\"Prompt template for RAG.\")\n    _model: Optional[\"PeftModel\"] = PrivateAttr(default=None)\n    _tokenizer: HFPretrainedTokenizer | None = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str,\n        base_model_name: str,\n        generation_config: Optional[\"GenerationConfig\"] = None,\n        prompt_template: str | None = None,\n        load_model_kwargs: dict | None = None,\n        load_base_model_kwargs: dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        generation_config = (\n            generation_config if generation_config else GenerationConfig()\n        )\n        prompt_template = (\n            prompt_template if prompt_template else DEFAULT_PROMPT_TEMPLATE\n        )\n        super().__init__(\n            model_name=model_name,\n            base_model_name=base_model_name,\n            generation_config=generation_config,\n            prompt_template=prompt_template,\n            load_model_kwargs=load_model_kwargs if load_model_kwargs else {},\n            load_base_model_kwargs=(\n                load_base_model_kwargs if load_base_model_kwargs else {}\n            ),\n        )\n        self._tokenizer = HFPretrainedTokenizer(\n            model_name=base_model_name, load_model_at_init=load_model_at_init\n        )\n        if load_model_at_init:\n            self._model = self._load_model_from_hf()\n\n    def _load_model_from_hf(self, **kwargs: Any) -&gt; \"PeftModel\":\n        load_base_kwargs = self.load_base_model_kwargs\n        load_kwargs = self.load_model_kwargs\n        load_kwargs.update(kwargs)\n        self.load_model_kwargs = load_kwargs  # update load_model_kwargs\n        base_model = AutoModelForCausalLM.from_pretrained(\n            self.base_model_name, **load_base_kwargs\n        )\n\n        if \"quantization_config\" in load_base_kwargs:\n            # preprocess model for kbit fine-tuning\n            # https://huggingface.co/docs/peft/developer_guides/quantization\n            base_model = prepare_model_for_kbit_training(base_model)\n\n        return PeftModel.from_pretrained(\n            base_model, self.model_name, **load_kwargs\n        )\n\n    @property\n    def model(self) -&gt; \"PeftModel\":\n        if self._model is None:\n            # load HF PeftModel\n            self._model = self._load_model_from_hf()\n        return self._model\n\n    @model.setter\n    def model(self, value: \"PeftModel\") -&gt; None:\n        self._model = value\n\n    @property\n    def tokenizer(self) -&gt; HFPretrainedTokenizer:\n        return self._tokenizer\n\n    @tokenizer.setter\n    def tokenizer(self, value: HFPretrainedTokenizer) -&gt; None:\n        self._tokenizer = value\n</code></pre>"},{"location":"api_reference/generators/huggingface/#src.fed_rag.generators.huggingface.hf_pretrained_model.HFPretrainedModelGenerator","title":"HFPretrainedModelGenerator","text":"<p>               Bases: <code>HuggingFaceGeneratorMixin</code>, <code>BaseGenerator</code></p> Source code in <code>src/fed_rag/generators/huggingface/hf_pretrained_model.py</code> <pre><code>class HFPretrainedModelGenerator(HuggingFaceGeneratorMixin, BaseGenerator):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str = Field(\n        description=\"Name of HuggingFace model. Used for loading the model from HF hub or local.\"\n    )\n    generation_config: \"GenerationConfig\" = Field(\n        description=\"The generation config used for generating with the PreTrainedModel.\"\n    )\n    load_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading models from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    prompt_template: str = Field(description=\"Prompt template for RAG.\")\n    _model: Optional[\"PreTrainedModel\"] = PrivateAttr(default=None)\n    _tokenizer: HFPretrainedTokenizer | None = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str,\n        generation_config: Optional[\"GenerationConfig\"] = None,\n        prompt_template: str | None = None,\n        load_model_kwargs: dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        generation_config = (\n            generation_config if generation_config else GenerationConfig()\n        )\n        prompt_template = (\n            prompt_template if prompt_template else DEFAULT_PROMPT_TEMPLATE\n        )\n        super().__init__(\n            model_name=model_name,\n            generation_config=generation_config,\n            prompt_template=prompt_template,\n            load_model_kwargs=load_model_kwargs if load_model_kwargs else {},\n        )\n        self._tokenizer = HFPretrainedTokenizer(\n            model_name=model_name, load_model_at_init=load_model_at_init\n        )\n        if load_model_at_init:\n            self._model = self._load_model_from_hf()\n\n    def _load_model_from_hf(self, **kwargs: Any) -&gt; \"PreTrainedModel\":\n        load_kwargs = self.load_model_kwargs\n        load_kwargs.update(kwargs)\n        self.load_model_kwargs = load_kwargs\n        model = AutoModelForCausalLM.from_pretrained(\n            self.model_name, **load_kwargs\n        )\n        return model\n\n    @property\n    def model(self) -&gt; \"PreTrainedModel\":\n        if self._model is None:\n            # load HF Pretrained Model\n            model = self._load_model_from_hf()\n            self._model = model\n        return self._model\n\n    @model.setter\n    def model(self, value: \"PreTrainedModel\") -&gt; None:\n        self._model = value\n\n    @property\n    def tokenizer(self) -&gt; HFPretrainedTokenizer:\n        return self._tokenizer\n\n    @tokenizer.setter\n    def tokenizer(self, value: HFPretrainedTokenizer) -&gt; None:\n        self._tokenizer = value\n</code></pre>"},{"location":"api_reference/inspectors/","title":"Inspectors","text":"<p>Common abstractions for inspectors</p> <p>TesterResult</p>"},{"location":"api_reference/inspectors/#src.fed_rag.inspectors.common.TesterSignatureSpec","title":"TesterSignatureSpec","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/inspectors/common.py</code> <pre><code>class TesterSignatureSpec(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    net_parameter: str\n    test_data_param: str\n    extra_test_kwargs: list[str] = []\n    net_parameter_class_name: str\n</code></pre>"},{"location":"api_reference/inspectors/#src.fed_rag.inspectors.common.TesterSignatureSpec","title":"TesterSignatureSpec","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/inspectors/common.py</code> <pre><code>class TesterSignatureSpec(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    net_parameter: str\n    test_data_param: str\n    extra_test_kwargs: list[str] = []\n    net_parameter_class_name: str\n</code></pre>"},{"location":"api_reference/inspectors/#src.fed_rag.types.results.TrainResult","title":"TrainResult","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/types/results.py</code> <pre><code>class TrainResult(BaseModel):\n    loss: float\n</code></pre>"},{"location":"api_reference/inspectors/#src.fed_rag.types.results.TestResult","title":"TestResult","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/types/results.py</code> <pre><code>class TestResult(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    loss: float\n    metrics: dict[str, Any] = Field(\n        description=\"Additional metrics computed on test set.\",\n        default_factory=dict,\n    )\n</code></pre>"},{"location":"api_reference/inspectors/huggingface/","title":"Huggingface","text":""},{"location":"api_reference/inspectors/huggingface/#src.fed_rag.inspectors.huggingface.inspect_trainer_signature","title":"inspect_trainer_signature","text":"<pre><code>inspect_trainer_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/huggingface/trainer.py</code> <pre><code>def inspect_trainer_signature(fn: Callable) -&gt; TrainerSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TrainResult):\n        msg = \"Trainer should return a fed_rag.types.TrainResult or a subclass of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_train_kwargs = []\n    net_param = None\n    train_data_param = None\n    val_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := get_type_name(t):\n            if (\n                type_name\n                in [\n                    \"SentenceTransformer\",\n                    \"PreTrainedModel\",\n                    \"PeftModel\",\n                    \"HFModelType\",\n                ]  # TODO: should accept union types involving these two\n                and net_param is None\n            ):\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"Dataset\" and train_data_param is None:\n                train_data_param = name\n                continue\n\n            if type_name == \"Dataset\" and val_data_param is None:\n                val_data_param = name\n                continue\n\n        extra_train_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For HuggingFace this param must have type `PreTrainedModel` or `SentenceTransformers`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if train_data_param is None:\n        msg = (\n            \"Inspection failed to find two data params for train and val datasets.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingMultipleDataParams(msg)\n\n    if val_data_param is None:\n        msg = (\n            \"Inspection found one data param but failed to find another. \"\n            \"Two data params are required for train and val datasets.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TrainerSignatureSpec(\n        net_parameter=net_param,\n        train_data_param=train_data_param,\n        val_data_param=val_data_param,\n        extra_train_kwargs=extra_train_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/huggingface/#src.fed_rag.inspectors.huggingface.inspect_tester_signature","title":"inspect_tester_signature","text":"<pre><code>inspect_tester_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/huggingface/tester.py</code> <pre><code>def inspect_tester_signature(fn: Callable) -&gt; TesterSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TestResult):\n        msg = \"Tester should return a fed_rag.types.TestResult or a subclass of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_tester_kwargs = []\n    net_param = None\n    test_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := get_type_name(t):\n            if (\n                type_name\n                in [\n                    \"PreTrainedModel\",\n                    \"SentenceTransformer\",\n                    \"PeftModel\",\n                    \"HFModelType\",\n                ]\n                and net_param is None\n            ):\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"Dataset\" and test_data_param is None:\n                test_data_param = name\n                continue\n\n        extra_tester_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For HuggingFace this param must have type `PreTrainedModel` or `SentenceTransformers`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if test_data_param is None:\n        msg = (\n            \"Inspection failed to find a data param for a test dataset.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TesterSignatureSpec(\n        net_parameter=net_param,\n        test_data_param=test_data_param,\n        extra_test_kwargs=extra_tester_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/pytorch/","title":"Pytorch","text":""},{"location":"api_reference/inspectors/pytorch/#src.fed_rag.inspectors.pytorch.inspect_trainer_signature","title":"inspect_trainer_signature","text":"<pre><code>inspect_trainer_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/pytorch/trainer.py</code> <pre><code>def inspect_trainer_signature(fn: Callable) -&gt; TrainerSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TrainResult):\n        msg = \"Trainer should return a fed_rag.types.TrainResult or a subclsas of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_train_kwargs = []\n    net_param = None\n    train_data_param = None\n    val_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if type_name == \"Module\" and net_param is None:\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"DataLoader\" and train_data_param is None:\n                train_data_param = name\n                continue\n\n            if type_name == \"DataLoader\" and val_data_param is None:\n                val_data_param = name\n                continue\n\n        extra_train_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For PyTorch this param must have type `nn.Module`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if train_data_param is None:\n        msg = (\n            \"Inspection failed to find two data params for train and val datasets.\"\n            \"For PyTorch these params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingMultipleDataParams(msg)\n\n    if val_data_param is None:\n        msg = (\n            \"Inspection found one data param but failed to find another. \"\n            \"Two data params are required for train and val datasets.\"\n            \"For PyTorch these params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TrainerSignatureSpec(\n        net_parameter=net_param,\n        train_data_param=train_data_param,\n        val_data_param=val_data_param,\n        extra_train_kwargs=extra_train_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/pytorch/#src.fed_rag.inspectors.pytorch.inspect_tester_signature","title":"inspect_tester_signature","text":"<pre><code>inspect_tester_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/pytorch/tester.py</code> <pre><code>def inspect_tester_signature(fn: Callable) -&gt; TesterSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TestResult):\n        msg = \"Tester should return a fed_rag.types.TestResult or a subclsas of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_tester_kwargs = []\n    net_param = None\n    test_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if type_name == \"Module\" and net_param is None:\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"DataLoader\" and test_data_param is None:\n                test_data_param = name\n                continue\n\n        extra_tester_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For PyTorch this param must have type `nn.Module`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if test_data_param is None:\n        msg = (\n            \"Inspection failed to find a data param for a test dataset.\"\n            \"For PyTorch this params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TesterSignatureSpec(\n        net_parameter=net_param,\n        test_data_param=test_data_param,\n        extra_test_kwargs=extra_tester_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/knowledge_nodes/","title":"Index","text":"<p>Knowledge Node</p>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.types.knowledge_node.KnowledgeNode","title":"KnowledgeNode","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/types/knowledge_node.py</code> <pre><code>class KnowledgeNode(BaseModel):\n    model_config = ConfigDict(\n        # ensures that validation is performed for defaulted None values\n        validate_default=True\n    )\n    node_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    embedding: list[float] = Field(\n        description=\"Encoded representation of node. If multimodal type, then this is shared embedding between image and text.\"\n    )\n    node_type: NodeType = Field(description=\"Type of node.\")\n    text_content: str | None = Field(\n        description=\"Text content. Used for TEXT and potentially MULTIMODAL node types.\",\n        default=None,\n    )\n    image_content: bytes | None = Field(\n        description=\"Image content as binary data (decoded from base64)\",\n        default=None,\n    )\n    metadata: dict = Field(\n        description=\"Metadata for node.\", default_factory=dict\n    )\n\n    # validators\n    @field_validator(\"text_content\", mode=\"before\")\n    @classmethod\n    def validate_text_content(\n        cls, value: str | None, info: ValidationInfo\n    ) -&gt; str | None:\n        node_type = info.data.get(\"node_type\")\n        node_type = cast(NodeType, node_type)\n        if node_type == NodeType.TEXT and value is None:\n            raise ValueError(\"NodeType == 'text', but text_content is None.\")\n\n        if node_type == NodeType.MULTIMODAL and value is None:\n            raise ValueError(\n                \"NodeType == 'multimodal', but text_content is None.\"\n            )\n\n        return value\n\n    @field_validator(\"image_content\", mode=\"after\")\n    @classmethod\n    def validate_image_content(\n        cls, value: str | None, info: ValidationInfo\n    ) -&gt; str | None:\n        node_type = info.data.get(\"node_type\")\n        node_type = cast(NodeType, node_type)\n        if node_type == NodeType.IMAGE:\n            if value is None:\n                raise ValueError(\n                    \"NodeType == 'image', but image_content is None.\"\n                )\n\n        if node_type == NodeType.MULTIMODAL:\n            if value is None:\n                raise ValueError(\n                    \"NodeType == 'multimodal', but image_content is None.\"\n                )\n\n        return value\n\n    def get_content(self) -&gt; NodeContent:\n        \"\"\"Return dict of node content.\"\"\"\n        content: NodeContent = {\n            \"image_content\": self.image_content,\n            \"text_content\": self.text_content,\n        }\n        return content\n\n    @field_serializer(\"metadata\")\n    def serialize_metadata(\n        self, metadata: dict[Any, Any] | None\n    ) -&gt; str | None:\n        \"\"\"\n        Custom serializer for the metadata field.\n\n        Will serialize the metadata field into a json string.\n\n        Args:\n            metadata: Metadata dictionary to serialize.\n\n        Returns:\n            Serialized metadata as a json string.\n        \"\"\"\n        if metadata:\n            return json.dumps(metadata)\n        return None\n\n    @field_validator(\"metadata\", mode=\"before\")\n    @classmethod\n    def deserialize_metadata(\n        cls, metadata: dict[Any, Any] | str | None\n    ) -&gt; dict[Any, Any] | None:\n        \"\"\"\n        Custom validator for the metadata field.\n\n        Will deserialize the metadata from a json string if it's a string.\n\n        Args:\n            metadata: Metadata to validate. If it is a json string, it will be deserialized into a dictionary.\n\n        Returns:\n            Validated metadata.\n        \"\"\"\n        if isinstance(metadata, str):\n            deserialized_metadata = json.loads(metadata)\n            return cast(dict[Any, Any], deserialized_metadata)\n        if metadata is None:\n            return {}\n        return metadata\n</code></pre>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.types.knowledge_node.KnowledgeNode.get_content","title":"get_content","text":"<pre><code>get_content()\n</code></pre> <p>Return dict of node content.</p> Source code in <code>src/fed_rag/types/knowledge_node.py</code> <pre><code>def get_content(self) -&gt; NodeContent:\n    \"\"\"Return dict of node content.\"\"\"\n    content: NodeContent = {\n        \"image_content\": self.image_content,\n        \"text_content\": self.text_content,\n    }\n    return content\n</code></pre>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.types.knowledge_node.KnowledgeNode.serialize_metadata","title":"serialize_metadata","text":"<pre><code>serialize_metadata(metadata)\n</code></pre> <p>Custom serializer for the metadata field.</p> <p>Will serialize the metadata field into a json string.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[Any, Any] | None</code> <p>Metadata dictionary to serialize.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Serialized metadata as a json string.</p> Source code in <code>src/fed_rag/types/knowledge_node.py</code> <pre><code>@field_serializer(\"metadata\")\ndef serialize_metadata(\n    self, metadata: dict[Any, Any] | None\n) -&gt; str | None:\n    \"\"\"\n    Custom serializer for the metadata field.\n\n    Will serialize the metadata field into a json string.\n\n    Args:\n        metadata: Metadata dictionary to serialize.\n\n    Returns:\n        Serialized metadata as a json string.\n    \"\"\"\n    if metadata:\n        return json.dumps(metadata)\n    return None\n</code></pre>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.types.knowledge_node.KnowledgeNode.deserialize_metadata","title":"deserialize_metadata  <code>classmethod</code>","text":"<pre><code>deserialize_metadata(metadata)\n</code></pre> <p>Custom validator for the metadata field.</p> <p>Will deserialize the metadata from a json string if it's a string.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[Any, Any] | str | None</code> <p>Metadata to validate. If it is a json string, it will be deserialized into a dictionary.</p> required <p>Returns:</p> Type Description <code>dict[Any, Any] | None</code> <p>Validated metadata.</p> Source code in <code>src/fed_rag/types/knowledge_node.py</code> <pre><code>@field_validator(\"metadata\", mode=\"before\")\n@classmethod\ndef deserialize_metadata(\n    cls, metadata: dict[Any, Any] | str | None\n) -&gt; dict[Any, Any] | None:\n    \"\"\"\n    Custom validator for the metadata field.\n\n    Will deserialize the metadata from a json string if it's a string.\n\n    Args:\n        metadata: Metadata to validate. If it is a json string, it will be deserialized into a dictionary.\n\n    Returns:\n        Validated metadata.\n    \"\"\"\n    if isinstance(metadata, str):\n        deserialized_metadata = json.loads(metadata)\n        return cast(dict[Any, Any], deserialized_metadata)\n    if metadata is None:\n        return {}\n    return metadata\n</code></pre>"},{"location":"api_reference/knowledge_stores/","title":"Base KnowledgeStore","text":"<p>Base Knowledge Store</p>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore","title":"BaseKnowledgeStore","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Knowledge Store Class.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>class BaseKnowledgeStore(BaseModel, ABC):\n    \"\"\"Base Knowledge Store Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    name: str = Field(\n        description=\"Name of Knowledge Store used for caching and loading.\",\n        default=DEFAULT_KNOWLEDGE_STORE_NAME,\n    )\n\n    @abstractmethod\n    def load_node(self, node: KnowledgeNode) -&gt; None:\n        \"\"\"Load a KnowledgeNode into the KnowledgeStore.\"\"\"\n\n    @abstractmethod\n    def load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n        \"\"\"Load multiple KnowledgeNodes in batch.\"\"\"\n\n    @abstractmethod\n    def retrieve(\n        self, query_emb: list[float], top_k: int\n    ) -&gt; list[tuple[float, KnowledgeNode]]:\n        \"\"\"Retrieve top-k nodes from KnowledgeStore against a provided user query.\n\n        Returns:\n            A list of tuples where the first element represents the similarity score\n            of the node to the query, and the second element is the node itself.\n        \"\"\"\n\n    @abstractmethod\n    def delete_node(self, node_id: str) -&gt; bool:\n        \"\"\"Remove a node from the KnowledgeStore by ID, returning success status.\"\"\"\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Clear all nodes from the KnowledgeStore.\"\"\"\n\n    @property\n    @abstractmethod\n    def count(self) -&gt; int:\n        \"\"\"Return the number of nodes in the store.\"\"\"\n\n    @abstractmethod\n    def persist(self) -&gt; None:\n        \"\"\"Save the KnowledgeStore nodes to a permanent storage.\"\"\"\n\n    @abstractmethod\n    def load(self) -&gt; None:\n        \"\"\"\n        Load the KnowledgeStore nodes from a permanent storage using `name`.\n\n        Args:\n            ks_id: The id of the knowledge store to load.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.count","title":"count  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>count\n</code></pre> <p>Return the number of nodes in the store.</p>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load_node","title":"load_node  <code>abstractmethod</code>","text":"<pre><code>load_node(node)\n</code></pre> <p>Load a KnowledgeNode into the KnowledgeStore.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef load_node(self, node: KnowledgeNode) -&gt; None:\n    \"\"\"Load a KnowledgeNode into the KnowledgeStore.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load_nodes","title":"load_nodes  <code>abstractmethod</code>","text":"<pre><code>load_nodes(nodes)\n</code></pre> <p>Load multiple KnowledgeNodes in batch.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n    \"\"\"Load multiple KnowledgeNodes in batch.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.retrieve","title":"retrieve  <code>abstractmethod</code>","text":"<pre><code>retrieve(query_emb, top_k)\n</code></pre> <p>Retrieve top-k nodes from KnowledgeStore against a provided user query.</p> <p>Returns:</p> Type Description <code>list[tuple[float, KnowledgeNode]]</code> <p>A list of tuples where the first element represents the similarity score</p> <code>list[tuple[float, KnowledgeNode]]</code> <p>of the node to the query, and the second element is the node itself.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef retrieve(\n    self, query_emb: list[float], top_k: int\n) -&gt; list[tuple[float, KnowledgeNode]]:\n    \"\"\"Retrieve top-k nodes from KnowledgeStore against a provided user query.\n\n    Returns:\n        A list of tuples where the first element represents the similarity score\n        of the node to the query, and the second element is the node itself.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.delete_node","title":"delete_node  <code>abstractmethod</code>","text":"<pre><code>delete_node(node_id)\n</code></pre> <p>Remove a node from the KnowledgeStore by ID, returning success status.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef delete_node(self, node_id: str) -&gt; bool:\n    \"\"\"Remove a node from the KnowledgeStore by ID, returning success status.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.clear","title":"clear  <code>abstractmethod</code>","text":"<pre><code>clear()\n</code></pre> <p>Clear all nodes from the KnowledgeStore.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Clear all nodes from the KnowledgeStore.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.persist","title":"persist  <code>abstractmethod</code>","text":"<pre><code>persist()\n</code></pre> <p>Save the KnowledgeStore nodes to a permanent storage.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef persist(self) -&gt; None:\n    \"\"\"Save the KnowledgeStore nodes to a permanent storage.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load()\n</code></pre> <p>Load the KnowledgeStore nodes from a permanent storage using <code>name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ks_id</code> <p>The id of the knowledge store to load.</p> required Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef load(self) -&gt; None:\n    \"\"\"\n    Load the KnowledgeStore nodes from a permanent storage using `name`.\n\n    Args:\n        ks_id: The id of the knowledge store to load.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/in_memory/","title":"InMemory","text":"<p>In Memory Knowledge Store</p>"},{"location":"api_reference/knowledge_stores/in_memory/#src.fed_rag.knowledge_stores.in_memory.InMemoryKnowledgeStore","title":"InMemoryKnowledgeStore","text":"<p>               Bases: <code>BaseKnowledgeStore</code></p> <p>InMemoryKnowledgeStore Class.</p> Source code in <code>src/fed_rag/knowledge_stores/in_memory.py</code> <pre><code>class InMemoryKnowledgeStore(BaseKnowledgeStore):\n    \"\"\"InMemoryKnowledgeStore Class.\"\"\"\n\n    cache_dir: str = Field(default=DEFAULT_CACHE_DIR)\n    _data: dict[str, KnowledgeNode] = PrivateAttr(default_factory=dict)\n\n    @classmethod\n    def from_nodes(cls, nodes: list[KnowledgeNode], **kwargs: Any) -&gt; Self:\n        instance = cls(**kwargs)\n        instance.load_nodes(nodes)\n        return instance\n\n    def load_node(self, node: KnowledgeNode) -&gt; None:\n        if node.node_id not in self._data:\n            self._data[node.node_id] = node\n\n    def load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n        for node in nodes:\n            self.load_node(node)\n\n    def retrieve(\n        self, query_emb: list[float], top_k: int\n    ) -&gt; list[tuple[float, KnowledgeNode]]:\n        all_nodes = list(self._data.values())\n        node_ids_and_scores = _get_top_k_nodes(\n            nodes=all_nodes, query_emb=query_emb, top_k=top_k\n        )\n        return [(el[1], self._data[el[0]]) for el in node_ids_and_scores]\n\n    def delete_node(self, node_id: str) -&gt; bool:\n        if node_id in self._data:\n            del self._data[node_id]\n            return True\n        else:\n            return False\n\n    def clear(self) -&gt; None:\n        self._data = {}\n\n    @property\n    def count(self) -&gt; int:\n        return len(self._data)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(self, handler: Any) -&gt; Dict[str, Any]:\n        data = handler(self)\n        data = cast(Dict[str, Any], data)\n        # include _data in serialization\n        if self._data:\n            data[\"_data\"] = self._data\n        return data  # type: ignore[no-any-return]\n\n    def persist(self) -&gt; None:\n        serialized_model = self.model_dump()\n        data_values = list(serialized_model[\"_data\"].values())\n\n        parquet_table = pa.Table.from_pylist(data_values)\n\n        filename = Path(self.cache_dir) / f\"{self.name}.parquet\"\n        Path(filename).parent.mkdir(parents=True, exist_ok=True)\n        pq.write_table(parquet_table, filename)\n\n    def load(self) -&gt; None:\n        filename = Path(self.cache_dir) / f\"{self.name}.parquet\"\n        if not filename.exists():\n            msg = f\"Knowledge store '{self.name}' not found at expected location: {filename}\"\n            raise KnowledgeStoreNotFoundError(msg)\n\n        parquet_data = pq.read_table(filename).to_pylist()\n        nodes = [KnowledgeNode(**data) for data in parquet_data]\n        self.load_nodes(nodes)\n</code></pre>"},{"location":"api_reference/knowledge_stores/in_memory/#src.fed_rag.knowledge_stores.in_memory.ManagedInMemoryKnowledgeStore","title":"ManagedInMemoryKnowledgeStore","text":"<p>               Bases: <code>ManagedMixin</code>, <code>InMemoryKnowledgeStore</code></p> Source code in <code>src/fed_rag/knowledge_stores/in_memory.py</code> <pre><code>class ManagedInMemoryKnowledgeStore(ManagedMixin, InMemoryKnowledgeStore):\n    def persist(self) -&gt; None:\n        serialized_model = self.model_dump()\n        data_values = list(serialized_model[\"_data\"].values())\n\n        parquet_table = pa.Table.from_pylist(data_values)\n\n        filename = Path(self.cache_dir) / self.name / f\"{self.ks_id}.parquet\"\n        Path(filename).parent.mkdir(parents=True, exist_ok=True)\n        pq.write_table(parquet_table, filename)\n\n    @classmethod\n    def from_name_and_id(\n        cls, name: str, ks_id: str, cache_dir: str | None = None\n    ) -&gt; Self:\n        cache_dir = cache_dir if cache_dir else DEFAULT_CACHE_DIR\n        filename = Path(cache_dir) / name / f\"{ks_id}.parquet\"\n        if not filename.exists():\n            msg = f\"Knowledge store '{name}/{ks_id}' not found at expected location: {filename}\"\n            raise KnowledgeStoreNotFoundError(msg)\n\n        parquet_data = pq.read_table(filename).to_pylist()\n        nodes = [KnowledgeNode(**data) for data in parquet_data]\n        knowledge_store = ManagedInMemoryKnowledgeStore.from_nodes(\n            nodes, name=name, cache_dir=cache_dir\n        )\n        # set id\n        knowledge_store.ks_id = ks_id\n        return knowledge_store\n</code></pre>"},{"location":"api_reference/knowledge_stores/mixins/","title":"Mixins","text":""},{"location":"api_reference/knowledge_stores/mixins/#src.fed_rag.knowledge_stores.mixins.ManagedMixin","title":"ManagedMixin","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> Source code in <code>src/fed_rag/knowledge_stores/mixins.py</code> <pre><code>class ManagedMixin(BaseModel, ABC):\n    ks_id: str = Field(default_factory=generate_ks_id)\n\n    @classmethod\n    @abstractmethod\n    def from_name_and_id(cls, ks_id: str) -&gt; Self:\n        \"\"\"Load a managed Knowledge Store by id.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/mixins/#src.fed_rag.knowledge_stores.mixins.ManagedMixin.from_name_and_id","title":"from_name_and_id  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>from_name_and_id(ks_id)\n</code></pre> <p>Load a managed Knowledge Store by id.</p> Source code in <code>src/fed_rag/knowledge_stores/mixins.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_name_and_id(cls, ks_id: str) -&gt; Self:\n    \"\"\"Load a managed Knowledge Store by id.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/","title":"Qdrant","text":""},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore","title":"QdrantKnowledgeStore","text":"<p>               Bases: <code>BaseKnowledgeStore</code></p> <p>Qdrant Knowledge Store Class</p> <p>NOTE: This is a minimal implementation in order to just get started using Qdrant.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>class QdrantKnowledgeStore(BaseKnowledgeStore):\n    \"\"\"Qdrant Knowledge Store Class\n\n    NOTE: This is a minimal implementation in order to just get started using Qdrant.\n    \"\"\"\n\n    host: str = Field(default=\"localhost\")\n    port: int = Field(default=6334)\n    ssl: bool = Field(default=False)\n    api_key: SecretStr | None = Field(default=None)\n    collection_name: str = Field(description=\"Name of Qdrant collection\")\n    collection_distance: Literal[\n        \"Cosine\", \"Euclid\", \"Dot\", \"Manhattan\"\n    ] = Field(\n        description=\"Distance definition for collection\", default=\"Cosine\"\n    )\n    client_kwargs: dict[str, Any] = Field(default_factory=dict)\n    _client: Optional[\"QdrantClient\"] = PrivateAttr(default=None)\n\n    def _collection_exists(self) -&gt; bool:\n        \"\"\"Check if a collection exists.\"\"\"\n        return self.client.collection_exists(self.collection_name)  # type: ignore[no-any-return]\n\n    def _create_collection(\n        self, collection_name: str, vector_size: int, distance: str\n    ) -&gt; None:\n        from qdrant_client.models import Distance, VectorParams\n\n        try:\n            # Try to convert to enum\n            distance = Distance(distance)\n        except ValueError:\n            # Catch the ValueError from enum conversion and raise your custom error\n            raise InvalidDistanceError(\n                f\"Unsupported distance: {distance}. \"\n                f\"Mode must be one of: {', '.join([m.value for m in Distance])}\"\n            )\n\n        try:\n            self.client.create_collection(\n                collection_name=collection_name,\n                vectors_config=VectorParams(\n                    size=vector_size, distance=distance\n                ),\n            )\n        except Exception as e:\n            raise KnowledgeStoreError(\n                f\"Failed to create collection: {str(e)}\"\n            ) from e\n\n    def _ensure_collection_exists(self) -&gt; None:\n        if not self._collection_exists():\n            raise KnowledgeStoreNotFoundError(\n                f\"Collection '{self.collection_name}' does not exist.\"\n            )\n\n    def _check_if_collection_exists_otherwise_create_one(\n        self, vector_size: int\n    ) -&gt; None:\n        if not self._collection_exists():\n            try:\n                self._create_collection(\n                    collection_name=self.collection_name,\n                    vector_size=vector_size,\n                    distance=self.collection_distance,\n                )\n            except Exception as e:\n                raise KnowledgeStoreError(\n                    f\"Failed to create new collection: '{self.collection_name}'\"\n                ) from e\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_dependencies(cls, data: Any) -&gt; Any:\n        \"\"\"Validate that qdrant dependencies are installed.\"\"\"\n        check_qdrant_installed()\n        return data\n\n    @property\n    def client(self) -&gt; \"QdrantClient\":\n        if self._client is None:\n            # get and set client\n            self._client = _get_qdrant_client(\n                host=self.host,\n                port=self.port,\n                ssl=self.ssl,\n                api_key=self.api_key.get_secret_value()\n                if self.api_key\n                else None,\n                **self.client_kwargs,\n            )\n        return self._client\n\n    def load_node(self, node: KnowledgeNode) -&gt; None:\n        self._check_if_collection_exists_otherwise_create_one(\n            vector_size=len(node.embedding)\n        )\n\n        point = _convert_knowledge_node_to_qdrant_point(node)\n        try:\n            self.client.upsert(\n                collection_name=self.collection_name, points=[point]\n            )\n        except Exception as e:\n            raise LoadNodeError(\n                f\"Failed to load node {node.node_id} into collection '{self.collection_name}': {str(e)}\"\n            ) from e\n\n    def load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n        if not nodes:\n            return\n\n        self._check_if_collection_exists_otherwise_create_one(\n            vector_size=len(nodes[0].embedding)\n        )\n\n        points = [_convert_knowledge_node_to_qdrant_point(n) for n in nodes]\n        try:\n            self.client.upload_points(\n                collection_name=self.collection_name, points=points\n            )\n        except Exception as e:\n            raise LoadNodeError(\n                f\"Loading nodes into collection '{self.collection_name}' failed: {str(e)}\"\n            ) from e\n\n    def retrieve(\n        self, query_emb: list[float], top_k: int\n    ) -&gt; list[tuple[float, KnowledgeNode]]:\n        \"\"\"Retrieve top-k nodes from the vector store.\"\"\"\n        from qdrant_client.conversions.common_types import QueryResponse\n\n        self._ensure_collection_exists()\n\n        try:\n            hits: QueryResponse = self.client.query_points(\n                collection_name=self.collection_name,\n                query=query_emb,\n                limit=top_k,\n            )\n        except Exception as e:\n            raise KnowledgeStoreError(\n                f\"Failed to retrieve from collection '{self.collection_name}': {str(e)}\"\n            ) from e\n\n        return [\n            _convert_scored_point_to_knowledge_node_and_score_tuple(pt)\n            for pt in hits.points\n        ]\n\n    def delete_node(self, node_id: str) -&gt; bool:\n        \"\"\"Delete a node based on its node_id.\"\"\"\n        from qdrant_client.http.models import (\n            FieldCondition,\n            Filter,\n            MatchValue,\n            UpdateResult,\n            UpdateStatus,\n        )\n\n        self._ensure_collection_exists()\n\n        try:\n            res: UpdateResult = self.client.delete(\n                collection_name=self.collection_name,\n                points_selector=Filter(\n                    must=[\n                        FieldCondition(\n                            key=\"node_id\", match=MatchValue(value=node_id)\n                        )\n                    ]\n                ),\n            )\n        except Exception:\n            raise KnowledgeStoreError(\n                f\"Failed to delete node: '{node_id}' from collection '{self.collection_name}'\"\n            )\n\n        return bool(res.status == UpdateStatus.COMPLETED)\n\n    def clear(self) -&gt; None:\n        self._ensure_collection_exists()\n\n        # delete the collection\n        try:\n            self.client.delete_collection(collection_name=self.collection_name)\n        except Exception as e:\n            raise KnowledgeStoreError(\n                f\"Failed to delete collection '{self.collection_name}': {str(e)}\"\n            ) from e\n\n    @property\n    def count(self) -&gt; int:\n        from qdrant_client.http.models import CountResult\n\n        self._ensure_collection_exists()\n\n        try:\n            res: CountResult = self.client.count(\n                collection_name=self.collection_name\n            )\n            return int(res.count)\n        except Exception as e:\n            raise KnowledgeStoreError(\n                f\"Failed to get vector count for collection '{self.collection_name}': {str(e)}\"\n            ) from e\n\n    def persist(self) -&gt; None:\n        \"\"\"Persist a knowledge store to disk.\"\"\"\n        raise NotImplementedError(\n            \"`persist()` is not available in QdrantKnowledgeStore.\"\n        )\n\n    def load(self) -&gt; None:\n        \"\"\"Load a previously persisted knowledge store.\"\"\"\n        raise NotImplementedError(\n            \"`load()` is not available in QdrantKnowledgeStore. \"\n            \"Data is automatically persisted and loaded from the Qdrant server.\"\n        )\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore.check_dependencies","title":"check_dependencies  <code>classmethod</code>","text":"<pre><code>check_dependencies(data)\n</code></pre> <p>Validate that qdrant dependencies are installed.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef check_dependencies(cls, data: Any) -&gt; Any:\n    \"\"\"Validate that qdrant dependencies are installed.\"\"\"\n    check_qdrant_installed()\n    return data\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore.retrieve","title":"retrieve","text":"<pre><code>retrieve(query_emb, top_k)\n</code></pre> <p>Retrieve top-k nodes from the vector store.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>def retrieve(\n    self, query_emb: list[float], top_k: int\n) -&gt; list[tuple[float, KnowledgeNode]]:\n    \"\"\"Retrieve top-k nodes from the vector store.\"\"\"\n    from qdrant_client.conversions.common_types import QueryResponse\n\n    self._ensure_collection_exists()\n\n    try:\n        hits: QueryResponse = self.client.query_points(\n            collection_name=self.collection_name,\n            query=query_emb,\n            limit=top_k,\n        )\n    except Exception as e:\n        raise KnowledgeStoreError(\n            f\"Failed to retrieve from collection '{self.collection_name}': {str(e)}\"\n        ) from e\n\n    return [\n        _convert_scored_point_to_knowledge_node_and_score_tuple(pt)\n        for pt in hits.points\n    ]\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore.delete_node","title":"delete_node","text":"<pre><code>delete_node(node_id)\n</code></pre> <p>Delete a node based on its node_id.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>def delete_node(self, node_id: str) -&gt; bool:\n    \"\"\"Delete a node based on its node_id.\"\"\"\n    from qdrant_client.http.models import (\n        FieldCondition,\n        Filter,\n        MatchValue,\n        UpdateResult,\n        UpdateStatus,\n    )\n\n    self._ensure_collection_exists()\n\n    try:\n        res: UpdateResult = self.client.delete(\n            collection_name=self.collection_name,\n            points_selector=Filter(\n                must=[\n                    FieldCondition(\n                        key=\"node_id\", match=MatchValue(value=node_id)\n                    )\n                ]\n            ),\n        )\n    except Exception:\n        raise KnowledgeStoreError(\n            f\"Failed to delete node: '{node_id}' from collection '{self.collection_name}'\"\n        )\n\n    return bool(res.status == UpdateStatus.COMPLETED)\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore.persist","title":"persist","text":"<pre><code>persist()\n</code></pre> <p>Persist a knowledge store to disk.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>def persist(self) -&gt; None:\n    \"\"\"Persist a knowledge store to disk.\"\"\"\n    raise NotImplementedError(\n        \"`persist()` is not available in QdrantKnowledgeStore.\"\n    )\n</code></pre>"},{"location":"api_reference/knowledge_stores/qdrant/#src.fed_rag.knowledge_stores.qdrant.QdrantKnowledgeStore.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load a previously persisted knowledge store.</p> Source code in <code>src/fed_rag/knowledge_stores/qdrant/sync.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load a previously persisted knowledge store.\"\"\"\n    raise NotImplementedError(\n        \"`load()` is not available in QdrantKnowledgeStore. \"\n        \"Data is automatically persisted and loaded from the Qdrant server.\"\n    )\n</code></pre>"},{"location":"api_reference/loss/pytorch/","title":"Pytorch","text":"<p>LM-Supervised Retriever Loss.</p>"},{"location":"api_reference/loss/pytorch/#src.fed_rag.loss.pytorch.lsr.ReductionMode","title":"ReductionMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Reduction mode enum.</p> Source code in <code>src/fed_rag/loss/pytorch/lsr.py</code> <pre><code>class ReductionMode(str, Enum):\n    \"\"\"Reduction mode enum.\"\"\"\n\n    MEAN = \"mean\"\n    SUM = \"sum\"\n\n    @classmethod\n    def members_list(cls) -&gt; list[str]:\n        return [member for member in cls]\n</code></pre>"},{"location":"api_reference/loss/pytorch/#src.fed_rag.loss.pytorch.lsr.LSRLoss","title":"LSRLoss","text":"<p>               Bases: <code>Module</code></p> <p>PyTorch implementation of the LM-Supervised Retriever Loss.</p> <p>Given input context x and ground truth continuation y, computes KL divergence between retrieval likelihood P_R(d|x) and language model likelihood Q_LM(d|x,y), where d is the retrieved document.</p> Shi, Weijia, et al. \"Replug: Retrieval-augmented black-box language models.\" <p>arXiv preprint arXiv:2301.12652 (2023).</p> <p>Arxiv: https://arxiv.org/pdf/2301.12652</p> Source code in <code>src/fed_rag/loss/pytorch/lsr.py</code> <pre><code>class LSRLoss(nn.Module):\n    \"\"\"PyTorch implementation of the LM-Supervised Retriever Loss.\n\n    Given input context x and ground truth continuation y, computes KL divergence\n    between retrieval likelihood P_R(d|x) and language model likelihood Q_LM(d|x,y),\n    where d is the retrieved document.\n\n    Source: Shi, Weijia, et al. \"Replug: Retrieval-augmented black-box language models.\"\n        arXiv preprint arXiv:2301.12652 (2023).\n    Arxiv: https://arxiv.org/pdf/2301.12652\n    \"\"\"\n\n    def __init__(self, reduction: ReductionMode = ReductionMode.MEAN):\n        # This line is critical - it initializes all the Module machinery\n        super(LSRLoss, self).__init__()\n\n        if reduction not in ReductionMode.members_list():\n            msg = (\n                f\"Invalid reduction {reduction}. \"\n                f\"Valid reductions are: {', '.join(ReductionMode.members_list())}\"\n            )\n            raise InvalidReductionParam(msg)\n\n        self.reduction = reduction\n\n    def forward(\n        self, retrieval_scores: torch.Tensor, lm_scores: torch.Tensor\n    ) -&gt; torch.Tensor:\n        retrieval_log_probs = F.log_softmax(retrieval_scores, dim=1)\n        lm_probs = F.softmax(lm_scores, dim=1)\n        kl_div = F.kl_div(retrieval_log_probs, lm_probs, reduction=\"none\").sum(\n            dim=-1\n        )\n\n        match self.reduction:\n            case ReductionMode.MEAN:\n                return kl_div.mean()\n            case ReductionMode.SUM:\n                return kl_div.sum()\n            case _:  # pragma: no cover\n                assert_never(self.reduction)  # pragma: no cover\n</code></pre>"},{"location":"api_reference/rag_system/","title":"RAG System and Auxiliary Types","text":"<p>RAG System</p>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.SourceNode","title":"SourceNode","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>class SourceNode(BaseModel):\n    score: float\n    node: KnowledgeNode\n\n    def __getattr__(self, __name: str) -&gt; Any:\n        \"\"\"Convenient wrapper on getattr of associated node.\"\"\"\n        return getattr(self.node, __name)\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.SourceNode.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(__name)\n</code></pre> <p>Convenient wrapper on getattr of associated node.</p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>def __getattr__(self, __name: str) -&gt; Any:\n    \"\"\"Convenient wrapper on getattr of associated node.\"\"\"\n    return getattr(self.node, __name)\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.RAGSystem","title":"RAGSystem","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>class RAGSystem(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    generator: BaseGenerator\n    retriever: BaseRetriever\n    knowledge_store: BaseKnowledgeStore\n    rag_config: RAGConfig\n\n    def query(self, query: str) -&gt; RAGResponse:\n        \"\"\"Query the RAG system.\"\"\"\n        source_nodes = self.retrieve(query)\n        context = self._format_context(source_nodes)\n        response = self.generate(query=query, context=context)\n        return RAGResponse(source_nodes=source_nodes, response=response)\n\n    def retrieve(self, query: str) -&gt; list[SourceNode]:\n        \"\"\"Retrieve from KnowledgeStore.\"\"\"\n        query_emb: list[float] = self.retriever.encode_query(query).tolist()\n        raw_retrieval_result = self.knowledge_store.retrieve(\n            query_emb=query_emb, top_k=self.rag_config.top_k\n        )\n        return [\n            SourceNode(score=el[0], node=el[1]) for el in raw_retrieval_result\n        ]\n\n    def generate(self, query: str, context: str) -&gt; str:\n        \"\"\"Generate response to query with context.\"\"\"\n        return self.generator.generate(query=query, context=context)  # type: ignore\n\n    def _format_context(self, source_nodes: list[SourceNode]) -&gt; str:\n        \"\"\"Format the context from the source nodes.\"\"\"\n        # TODO: how to format image context\n        return self.rag_config.context_separator.join(\n            [node.get_content()[\"text_content\"] for node in source_nodes]\n        )\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.RAGSystem.query","title":"query","text":"<pre><code>query(query)\n</code></pre> <p>Query the RAG system.</p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>def query(self, query: str) -&gt; RAGResponse:\n    \"\"\"Query the RAG system.\"\"\"\n    source_nodes = self.retrieve(query)\n    context = self._format_context(source_nodes)\n    response = self.generate(query=query, context=context)\n    return RAGResponse(source_nodes=source_nodes, response=response)\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.RAGSystem.retrieve","title":"retrieve","text":"<pre><code>retrieve(query)\n</code></pre> <p>Retrieve from KnowledgeStore.</p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>def retrieve(self, query: str) -&gt; list[SourceNode]:\n    \"\"\"Retrieve from KnowledgeStore.\"\"\"\n    query_emb: list[float] = self.retriever.encode_query(query).tolist()\n    raw_retrieval_result = self.knowledge_store.retrieve(\n        query_emb=query_emb, top_k=self.rag_config.top_k\n    )\n    return [\n        SourceNode(score=el[0], node=el[1]) for el in raw_retrieval_result\n    ]\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.RAGSystem.generate","title":"generate","text":"<pre><code>generate(query, context)\n</code></pre> <p>Generate response to query with context.</p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>def generate(self, query: str, context: str) -&gt; str:\n    \"\"\"Generate response to query with context.\"\"\"\n    return self.generator.generate(query=query, context=context)  # type: ignore\n</code></pre>"},{"location":"api_reference/retrievers/","title":"Retrievers","text":"<p>Base Retriever</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever","title":"BaseRetriever","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Retriever Class.</p> Source code in <code>src/fed_rag/base/retriever.py</code> <pre><code>class BaseRetriever(BaseModel, ABC):\n    \"\"\"Base Retriever Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def encode_query(\n        self, query: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode query.\"\"\"\n\n    @abstractmethod\n    def encode_context(\n        self, context: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode context.\"\"\"\n\n    @property\n    @abstractmethod\n    def encoder(self) -&gt; torch.nn.Module | None:\n        \"\"\"PyTorch model associated with the encoder associated with retriever.\"\"\"\n\n    @property\n    @abstractmethod\n    def query_encoder(self) -&gt; torch.nn.Module | None:\n        \"\"\"PyTorch model associated with the query encoder associated with retriever.\"\"\"\n\n    @property\n    @abstractmethod\n    def context_encoder(self) -&gt; torch.nn.Module | None:\n        \"\"\"PyTorch model associated with the context encoder associated with retriever.\"\"\"\n</code></pre>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.encoder","title":"encoder  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>encoder\n</code></pre> <p>PyTorch model associated with the encoder associated with retriever.</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.query_encoder","title":"query_encoder  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>query_encoder\n</code></pre> <p>PyTorch model associated with the query encoder associated with retriever.</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.context_encoder","title":"context_encoder  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>context_encoder\n</code></pre> <p>PyTorch model associated with the context encoder associated with retriever.</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.encode_query","title":"encode_query  <code>abstractmethod</code>","text":"<pre><code>encode_query(query, **kwargs)\n</code></pre> <p>Encode query.</p> Source code in <code>src/fed_rag/base/retriever.py</code> <pre><code>@abstractmethod\ndef encode_query(\n    self, query: str | list[str], **kwargs: Any\n) -&gt; torch.Tensor:\n    \"\"\"Encode query.\"\"\"\n</code></pre>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.encode_context","title":"encode_context  <code>abstractmethod</code>","text":"<pre><code>encode_context(context, **kwargs)\n</code></pre> <p>Encode context.</p> Source code in <code>src/fed_rag/base/retriever.py</code> <pre><code>@abstractmethod\ndef encode_context(\n    self, context: str | list[str], **kwargs: Any\n) -&gt; torch.Tensor:\n    \"\"\"Encode context.\"\"\"\n</code></pre>"},{"location":"api_reference/retrievers/huggingface/","title":"Huggingface","text":"<p>HuggingFace SentenceTransformer Retriever</p>"},{"location":"api_reference/retrievers/huggingface/#src.fed_rag.retrievers.huggingface.hf_sentence_transformer.HFSentenceTransformerRetriever","title":"HFSentenceTransformerRetriever","text":"<p>               Bases: <code>BaseRetriever</code></p> Source code in <code>src/fed_rag/retrievers/huggingface/hf_sentence_transformer.py</code> <pre><code>class HFSentenceTransformerRetriever(BaseRetriever):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str | None = Field(\n        description=\"Name of HuggingFace SentenceTransformer model.\",\n        default=None,\n    )\n    query_model_name: str | None = Field(\n        description=\"Name of HuggingFace SentenceTransformer model used for encoding queries.\",\n        default=None,\n    )\n    context_model_name: str | None = Field(\n        description=\"Name of HuggingFace SentenceTransformer model used for encoding context.\",\n        default=None,\n    )\n    load_model_kwargs: LoadKwargs = Field(\n        description=\"Optional kwargs dict for loading models from HF. Defaults to None.\",\n        default_factory=LoadKwargs,\n    )\n    _encoder: Optional[\"SentenceTransformer\"] = PrivateAttr(default=None)\n    _query_encoder: Optional[\"SentenceTransformer\"] = PrivateAttr(default=None)\n    _context_encoder: Optional[\"SentenceTransformer\"] = PrivateAttr(\n        default=None\n    )\n\n    def __init__(\n        self,\n        model_name: str | None = None,\n        query_model_name: str | None = None,\n        context_model_name: str | None = None,\n        load_model_kwargs: LoadKwargs | dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        if isinstance(load_model_kwargs, dict):\n            # use same dict for all\n            load_model_kwargs = LoadKwargs(\n                encoder=load_model_kwargs,\n                query_encoder=load_model_kwargs,\n                context_encoder=load_model_kwargs,\n            )\n\n        load_model_kwargs = (\n            load_model_kwargs if load_model_kwargs else LoadKwargs()\n        )\n\n        super().__init__(\n            model_name=model_name,\n            query_model_name=query_model_name,\n            context_model_name=context_model_name,\n            load_model_kwargs=load_model_kwargs,\n        )\n        if load_model_at_init:\n            if model_name:\n                self._encoder = self._load_model_from_hf(load_type=\"encoder\")\n            else:\n                self._query_encoder = self._load_model_from_hf(\n                    load_type=\"query_encoder\"\n                )\n                self._context_encoder = self._load_model_from_hf(\n                    load_type=\"context_encoder\"\n                )\n\n    def _load_model_from_hf(\n        self,\n        load_type: Literal[\"encoder\", \"query_encoder\", \"context_encoder\"],\n        **kwargs: Any,\n    ) -&gt; \"SentenceTransformer\":\n        if load_type == \"encoder\":\n            load_kwargs = self.load_model_kwargs.encoder\n            load_kwargs.update(kwargs)\n            return SentenceTransformer(self.model_name, **load_kwargs)\n        elif load_type == \"context_encoder\":\n            load_kwargs = self.load_model_kwargs.context_encoder\n            load_kwargs.update(kwargs)\n            return SentenceTransformer(self.context_model_name, **load_kwargs)\n        elif load_type == \"query_encoder\":\n            load_kwargs = self.load_model_kwargs.query_encoder\n            load_kwargs.update(kwargs)\n            return SentenceTransformer(self.query_model_name, **load_kwargs)\n        else:\n            raise InvalidLoadType(\"Invalid `load_type` supplied.\")\n\n    def encode_context(\n        self, context: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        # validation guarantees one of these is not None\n        encoder = self.encoder if self.encoder else self.context_encoder\n        encoder = cast(SentenceTransformer, encoder)\n\n        return encoder.encode(context)\n\n    def encode_query(\n        self, query: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        # validation guarantees one of these is not None\n        encoder = self.encoder if self.encoder else self.query_encoder\n        encoder = cast(SentenceTransformer, encoder)\n\n        return encoder.encode(query)\n\n    @property\n    def encoder(self) -&gt; Optional[\"SentenceTransformer\"]:\n        if self.model_name and self._encoder is None:\n            self._encoder = self._load_model_from_hf(load_type=\"encoder\")\n        return self._encoder\n\n    @property\n    def query_encoder(self) -&gt; Optional[\"SentenceTransformer\"]:\n        if self.query_model_name and self._query_encoder is None:\n            self._query_encoder = self._load_model_from_hf(\n                load_type=\"query_encoder\"\n            )\n        return self._query_encoder\n\n    @property\n    def context_encoder(self) -&gt; Optional[\"SentenceTransformer\"]:\n        if self.context_model_name and self._context_encoder is None:\n            self._context_encoder = self._load_model_from_hf(\n                load_type=\"context_encoder\"\n            )\n        return self._context_encoder\n</code></pre>"},{"location":"api_reference/tokenizers/","title":"Tokenizers","text":"<p>Base Tokenizer</p>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer","title":"BaseTokenizer","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Tokenizer Class.</p> Source code in <code>src/fed_rag/base/tokenizer.py</code> <pre><code>class BaseTokenizer(BaseModel, ABC):\n    \"\"\"Base Tokenizer Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def encode(self, input: str, **kwargs: dict) -&gt; EncodeResult:\n        \"\"\"Encode the input string into list of integers.\"\"\"\n\n    @abstractmethod\n    def decode(self, input_ids: str, **kwargs: dict) -&gt; str:\n        \"\"\"Decode the input token ids into a string.\"\"\"\n\n    @property\n    @abstractmethod\n    def unwrapped(self) -&gt; Any:\n        \"\"\"Return the underlying tokenizer if there is one.\"\"\"\n</code></pre>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer.unwrapped","title":"unwrapped  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>unwrapped\n</code></pre> <p>Return the underlying tokenizer if there is one.</p>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer.encode","title":"encode  <code>abstractmethod</code>","text":"<pre><code>encode(input, **kwargs)\n</code></pre> <p>Encode the input string into list of integers.</p> Source code in <code>src/fed_rag/base/tokenizer.py</code> <pre><code>@abstractmethod\ndef encode(self, input: str, **kwargs: dict) -&gt; EncodeResult:\n    \"\"\"Encode the input string into list of integers.\"\"\"\n</code></pre>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer.decode","title":"decode  <code>abstractmethod</code>","text":"<pre><code>decode(input_ids, **kwargs)\n</code></pre> <p>Decode the input token ids into a string.</p> Source code in <code>src/fed_rag/base/tokenizer.py</code> <pre><code>@abstractmethod\ndef decode(self, input_ids: str, **kwargs: dict) -&gt; str:\n    \"\"\"Decode the input token ids into a string.\"\"\"\n</code></pre>"},{"location":"api_reference/tokenizers/huggingface/","title":"Huggingface","text":"<p>HuggingFace PretrainedTokenizer</p>"},{"location":"api_reference/tokenizers/huggingface/#src.fed_rag.tokenizers.hf_pretrained_tokenizer.HFPretrainedTokenizer","title":"HFPretrainedTokenizer","text":"<p>               Bases: <code>BaseTokenizer</code></p> Source code in <code>src/fed_rag/tokenizers/hf_pretrained_tokenizer.py</code> <pre><code>class HFPretrainedTokenizer(BaseTokenizer):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str = Field(\n        description=\"Name of HuggingFace model. Used for loading the model from HF hub or local.\"\n    )\n    load_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading models from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    _tokenizer: Optional[\"PreTrainedTokenizer\"] = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str,\n        load_model_kwargs: dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n        super().__init__(\n            model_name=model_name,\n            load_model_kwargs=load_model_kwargs if load_model_kwargs else {},\n        )\n        if load_model_at_init:\n            self._tokenizer = self._load_model_from_hf()\n\n    def _load_model_from_hf(self, **kwargs: Any) -&gt; \"PreTrainedTokenizer\":\n        load_kwargs = self.load_model_kwargs\n        load_kwargs.update(kwargs)\n        self.load_model_kwargs = load_kwargs\n        return AutoTokenizer.from_pretrained(self.model_name, **load_kwargs)\n\n    @property\n    def unwrapped(self) -&gt; \"PreTrainedTokenizer\":\n        if self._tokenizer is None:\n            # load HF Pretrained Tokenizer\n            tokenizer = self._load_model_from_hf()\n            self._tokenizer = tokenizer\n        return self._tokenizer\n\n    @unwrapped.setter\n    def unwrapped(self, value: \"PreTrainedTokenizer\") -&gt; None:\n        self._tokenizer = value\n\n    def encode(self, input: str, **kwargs: Any) -&gt; EncodeResult:\n        tokenizer_result = self.unwrapped(text=input, **kwargs)\n        retval: EncodeResult = {\n            \"input_ids\": tokenizer_result.get(\"input_ids\"),\n            \"attention_mask\": tokenizer_result.get(\"attention_mask\", None),\n        }\n        return retval\n\n    def decode(self, input_ids: list[int], **kwargs: Any) -&gt; str:\n        return self.unwrapped.decode(token_ids=input_ids, **kwargs)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_reference/trainer_managers/","title":"Base RAG Trainer Manager","text":"<p>Base RAG Trainer Manager</p>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager","title":"BaseRAGTrainerManager","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base RAG Trainer Class.</p> <p>The manager becomes solely responsible for orchestration, not for maintaining state (i.e., the RAGSystem).</p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>class BaseRAGTrainerManager(BaseModel, ABC):\n    \"\"\"Base RAG Trainer Class.\n\n    The manager becomes solely responsible for orchestration, not for maintaining state\n    (i.e., the RAGSystem).\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    mode: RAGTrainMode\n    retriever_trainer: BaseRetrieverTrainer | None = None\n    generator_trainer: BaseGeneratorTrainer | None = None\n\n    @field_validator(\"mode\", mode=\"before\")\n    @classmethod\n    def validate_mode(cls, v: str) -&gt; str:\n        try:\n            # Try to convert to enum\n            mode = RAGTrainMode(v)\n            return mode\n        except ValueError:\n            # Catch the ValueError from enum conversion and raise your custom error\n            raise UnsupportedTrainerMode(\n                f\"Unsupported RAG train mode: {v}. \"\n                f\"Mode must be one of: {', '.join([m.value for m in RAGTrainMode])}\"\n            )\n\n    # Validate trainer presence\n    @model_validator(mode=\"after\")\n    def validate_trainers(self) -&gt; \"BaseRAGTrainerManager\":\n        \"\"\"Validate trainer requirements.\"\"\"\n        # Validate trainer presence based on mode\n        if (\n            self.mode == RAGTrainMode.RETRIEVER\n            and self.retriever_trainer is None\n        ):\n            raise UnspecifiedRetrieverTrainer(\n                \"Retriever trainer must be set when in retriever mode\"\n            )\n        if (\n            self.mode == RAGTrainMode.GENERATOR\n            and self.generator_trainer is None\n        ):\n            raise UnspecifiedGeneratorTrainer(\n                \"Generator trainer must be set when in generator mode\"\n            )\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_trainers_consistency(self) -&gt; \"BaseRAGTrainerManager\":\n        \"\"\"Validate that trainers use consistent RAG systems if both are present.\"\"\"\n        if (\n            self.retriever_trainer is not None\n            and self.generator_trainer is not None\n        ):\n            # Check if both trainers have the same RAG system reference\n            if id(self.retriever_trainer.rag_system) != id(\n                self.generator_trainer.rag_system\n            ):\n                raise InconsistentRAGSystems(\n                    \"Inconsistent RAG systems detected between retriever and generator trainers. \"\n                    \"Both trainers must use the same RAG system instance for consistent training.\"\n                )\n\n        return self\n\n    @abstractmethod\n    def _prepare_retriever_for_training(\n        self, freeze_context_encoder: bool = True, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Prepare retriever model for training.\"\"\"\n\n    @abstractmethod\n    def _prepare_generator_for_training(self, **kwargs: Any) -&gt; None:\n        \"\"\"Prepare generator model for training.\"\"\"\n\n    @abstractmethod\n    def _train_retriever(self, **kwargs: Any) -&gt; Any:\n        \"\"\"Train loop for retriever.\"\"\"\n\n    @abstractmethod\n    def _train_generator(self, **kwargs: Any) -&gt; Any:\n        \"\"\"Train loop for generator.\"\"\"\n\n    @abstractmethod\n    def train(self, **kwargs: Any) -&gt; Any:\n        \"\"\"Train loop for rag system.\"\"\"\n\n    @abstractmethod\n    def get_federated_task(self) -&gt; BaseFLTask:\n        \"\"\"Get the federated task.\"\"\"\n\n    @property\n    def model(self) -&gt; Any:\n        \"\"\"Return the model to be trained.\"\"\"\n        match self.mode:\n            case RAGTrainMode.RETRIEVER:\n                trainer = cast(BaseRetrieverTrainer, self.retriever_trainer)\n            case RAGTrainMode.GENERATOR:\n                trainer = cast(BaseGeneratorTrainer, self.generator_trainer)\n            case _:  # pragma: no cover\n                assert_never(self.mode)  # pragma: no cover\n        return trainer.model\n</code></pre>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager.model","title":"model  <code>property</code>","text":"<pre><code>model\n</code></pre> <p>Return the model to be trained.</p>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager.validate_trainers","title":"validate_trainers","text":"<pre><code>validate_trainers()\n</code></pre> <p>Validate trainer requirements.</p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_trainers(self) -&gt; \"BaseRAGTrainerManager\":\n    \"\"\"Validate trainer requirements.\"\"\"\n    # Validate trainer presence based on mode\n    if (\n        self.mode == RAGTrainMode.RETRIEVER\n        and self.retriever_trainer is None\n    ):\n        raise UnspecifiedRetrieverTrainer(\n            \"Retriever trainer must be set when in retriever mode\"\n        )\n    if (\n        self.mode == RAGTrainMode.GENERATOR\n        and self.generator_trainer is None\n    ):\n        raise UnspecifiedGeneratorTrainer(\n            \"Generator trainer must be set when in generator mode\"\n        )\n\n    return self\n</code></pre>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager.validate_trainers_consistency","title":"validate_trainers_consistency","text":"<pre><code>validate_trainers_consistency()\n</code></pre> <p>Validate that trainers use consistent RAG systems if both are present.</p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_trainers_consistency(self) -&gt; \"BaseRAGTrainerManager\":\n    \"\"\"Validate that trainers use consistent RAG systems if both are present.\"\"\"\n    if (\n        self.retriever_trainer is not None\n        and self.generator_trainer is not None\n    ):\n        # Check if both trainers have the same RAG system reference\n        if id(self.retriever_trainer.rag_system) != id(\n            self.generator_trainer.rag_system\n        ):\n            raise InconsistentRAGSystems(\n                \"Inconsistent RAG systems detected between retriever and generator trainers. \"\n                \"Both trainers must use the same RAG system instance for consistent training.\"\n            )\n\n    return self\n</code></pre>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager.train","title":"train  <code>abstractmethod</code>","text":"<pre><code>train(**kwargs)\n</code></pre> <p>Train loop for rag system.</p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>@abstractmethod\ndef train(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Train loop for rag system.\"\"\"\n</code></pre>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.BaseRAGTrainerManager.get_federated_task","title":"get_federated_task  <code>abstractmethod</code>","text":"<pre><code>get_federated_task()\n</code></pre> <p>Get the federated task.</p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>@abstractmethod\ndef get_federated_task(self) -&gt; BaseFLTask:\n    \"\"\"Get the federated task.\"\"\"\n</code></pre>"},{"location":"api_reference/trainer_managers/#src.fed_rag.base.trainer_manager.RAGTrainMode","title":"RAGTrainMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/fed_rag/base/trainer_manager.py</code> <pre><code>class RAGTrainMode(str, Enum):\n    RETRIEVER = \"retriever\"\n    GENERATOR = \"generator\"\n</code></pre>"},{"location":"api_reference/trainer_managers/huggingface/","title":"Huggingface","text":"<p>HuggingFace RAG Trainer</p>"},{"location":"api_reference/trainer_managers/huggingface/#src.fed_rag.trainer_managers.huggingface.HuggingFaceRAGTrainerManager","title":"HuggingFaceRAGTrainerManager","text":"<p>               Bases: <code>BaseRAGTrainerManager</code></p> <p>HuggingFace RAG Trainer Manager</p> Source code in <code>src/fed_rag/trainer_managers/huggingface.py</code> <pre><code>class HuggingFaceRAGTrainerManager(BaseRAGTrainerManager):\n    \"\"\"HuggingFace RAG Trainer Manager\"\"\"\n\n    def __init__(\n        self,\n        mode: RAGTrainMode,\n        retriever_trainer: BaseTrainer | None = None,\n        generator_trainer: BaseTrainer | None = None,\n        **kwargs: Any,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n        super().__init__(\n            mode=mode,\n            retriever_trainer=retriever_trainer,\n            generator_trainer=generator_trainer,\n            **kwargs,\n        )\n\n    def _prepare_generator_for_training(self, **kwargs: Any) -&gt; None:\n        self.generator_trainer.model.train()\n\n        # freeze generator\n        if self.retriever_trainer:\n            self.retriever_trainer.model.eval()\n\n    def _prepare_retriever_for_training(\n        self, freeze_context_encoder: bool = True, **kwargs: Any\n    ) -&gt; None:\n        self.retriever_trainer.model.train()\n\n        # freeze generator\n        if self.generator_trainer:\n            self.generator_trainer.model.eval()\n\n    def _train_retriever(self, **kwargs: Any) -&gt; TrainResult:\n        if self.retriever_trainer:\n            self._prepare_retriever_for_training()\n            return self.retriever_trainer.train()\n        else:\n            raise UnspecifiedRetrieverTrainer(\n                \"Attempted to perform retriever trainer with an unspecified trainer.\"\n            )\n\n    def _train_generator(self, **kwargs: Any) -&gt; TrainResult:\n        if self.generator_trainer:\n            self._prepare_generator_for_training()\n            return self.generator_trainer.train()\n        else:\n            raise UnspecifiedGeneratorTrainer(\n                \"Attempted to perform generator trainer with an unspecified trainer.\"\n            )\n\n    def train(self, **kwargs: Any) -&gt; TrainResult:\n        if self.mode == \"retriever\":\n            return self._train_retriever()\n        elif self.mode == \"generator\":\n            return self._train_generator()\n        else:\n            assert_never(self.mode)  # pragma: no cover\n\n    def _get_federated_trainer(self) -&gt; tuple[Callable, \"HFModelType\"]:\n        if self.mode == \"retriever\":\n            if self.retriever_trainer is None:\n                raise UnspecifiedRetrieverTrainer(\n                    \"Cannot federate an unspecified retriever trainer.\"\n                )\n            retriever_train_fn = self.retriever_trainer.train\n            retriever_module = self.retriever_trainer.model\n            retriever_module = cast(\"SentenceTransformer\", retriever_module)\n\n            # Create a standalone function for federation\n            def train_wrapper(\n                model: \"HFModelType\",\n                train_dataset: \"Dataset\",\n                val_dataset: \"Dataset\",\n            ) -&gt; TrainResult:\n                _ = retriever_train_fn()\n                return TrainResult(loss=0)\n\n            return (\n                federate.trainer.huggingface(train_wrapper),\n                retriever_module,\n            )\n\n        elif self.mode == \"generator\":\n            if self.generator_trainer is None:\n                raise UnspecifiedGeneratorTrainer(\n                    \"Cannot federate an unspecified generator trainer.\"\n                )\n            generator_train_fn = self.generator_trainer.train\n            generator_module = self.generator_trainer.model\n\n            # Create a standalone function for federation\n            def train_wrapper(\n                model: \"HFModelType\",  # TODO: handle union types in inspector\n                train_dataset: \"Dataset\",\n                val_dataset: \"Dataset\",\n            ) -&gt; TrainResult:\n                _ = generator_train_fn()\n                # TODO get loss from out\n                return TrainResult(loss=0)\n\n            return (\n                federate.trainer.huggingface(train_wrapper),\n                generator_module,\n            )\n        else:\n            assert_never(self.mode)  # pragma: no cover\n\n    def get_federated_task(self) -&gt; \"HuggingFaceFLTask\":\n        from fed_rag.fl_tasks.huggingface import HuggingFaceFLTask\n\n        federated_trainer, _module = self._get_federated_trainer()\n\n        # TODO: add logic for getting evaluator/tester and then federate it as well\n        # federated_tester = self.get_federated_tester(tester_decorator)\n        # For now, using a simple placeholder test function\n        def test_fn(\n            model: \"HFModelType\", eval_dataset: \"Dataset\"\n        ) -&gt; TestResult:\n            # Implement simple testing or return a placeholder\n            return TestResult(loss=0.42, metrics={})  # pragma: no cover\n\n        federated_tester = federate.tester.huggingface(test_fn)\n\n        return HuggingFaceFLTask.from_trainer_and_tester(\n            trainer=federated_trainer,\n            tester=federated_tester,\n        )\n</code></pre>"},{"location":"api_reference/trainer_managers/pytorch/","title":"Pytorch","text":"<p>PyTorch RAG Trainer</p>"},{"location":"api_reference/trainer_managers/pytorch/#src.fed_rag.trainer_managers.pytorch.PyTorchRAGTrainerManager","title":"PyTorchRAGTrainerManager","text":"<p>               Bases: <code>BaseRAGTrainerManager</code></p> <p>PyTorch native RAG Trainer Manager</p> Source code in <code>src/fed_rag/trainer_managers/pytorch.py</code> <pre><code>class PyTorchRAGTrainerManager(BaseRAGTrainerManager):\n    \"\"\"PyTorch native RAG Trainer Manager\"\"\"\n\n    def _prepare_generator_for_training(self, **kwargs: Any) -&gt; None:\n        self.generator_trainer.model.train()\n\n        # freeze retriever\n        if self.retriever_trainer:\n            self.retriever_trainer.model.eval()\n\n    def _prepare_retriever_for_training(\n        self, freeze_context_encoder: bool = True, **kwargs: Any\n    ) -&gt; None:\n        self.retriever_trainer.model.train()\n\n        # freeze generator\n        if self.generator_trainer:\n            self.generator_trainer.model.eval()\n\n    def _train_retriever(self, **kwargs: Any) -&gt; None:\n        if self.retriever_trainer:\n            self._prepare_retriever_for_training()\n            self.retriever_trainer.train()\n        else:\n            raise UnspecifiedRetrieverTrainer(\n                \"Attempted to perform retriever trainer with an unspecified trainer.\"\n            )\n\n    def _train_generator(self, **kwargs: Any) -&gt; None:\n        if self.generator_trainer:\n            self._prepare_generator_for_training()\n            self.generator_trainer.train()\n        else:\n            raise UnspecifiedGeneratorTrainer(\n                \"Attempted to perform generator trainer with an unspecified trainer.\"\n            )\n\n    def train(self, **kwargs: Any) -&gt; None:\n        if self.mode == \"retriever\":\n            self._train_retriever()\n        elif self.mode == \"generator\":\n            self._train_generator()\n        else:\n            assert_never(self.mode)  # pragma: no cover\n\n    def _get_federated_trainer(self) -&gt; tuple[Callable, nn.Module]:\n        if self.mode == \"retriever\":\n            if self.retriever_trainer is None:\n                raise UnspecifiedRetrieverTrainer(\n                    \"Cannot federate an unspecified retriever trainer.\"\n                )\n            retriever_train_fn = self.retriever_trainer.train\n            retriever_module = self.retriever_trainer.model\n\n            # Create a standalone function for federation\n            def train_wrapper(\n                _mdl: nn.Module,\n                _train_dataloader: DataLoader,\n                _val_dataloader: DataLoader,\n            ) -&gt; TrainResult:\n                _ = retriever_train_fn()\n                return TrainResult(loss=0)\n\n            return federate.trainer.pytorch(train_wrapper), retriever_module\n\n        elif self.mode == \"generator\":\n            if self.generator_trainer is None:\n                raise UnspecifiedGeneratorTrainer(\n                    \"Cannot federate an unspecified generator trainer.\"\n                )\n            generator_train_fn = self.generator_trainer.train\n            generator_module = self.generator_trainer.model\n\n            # Create a standalone function for federation\n            def train_wrapper(\n                _mdl: nn.Module,\n                _train_dataloader: DataLoader,\n                _val_dataloader: DataLoader,\n            ) -&gt; TrainResult:\n                _ = generator_train_fn()\n                # TODO get loss from out\n                return TrainResult(loss=0)\n\n            return federate.trainer.pytorch(train_wrapper), generator_module\n        else:\n            assert_never(self.mode)  # pragma: no cover\n\n    def get_federated_task(self) -&gt; PyTorchFLTask:\n        federated_trainer, _module = self._get_federated_trainer()\n\n        # TODO: add logic for getting evaluator/tester and then federate it as well\n        # federated_tester = self.get_federated_tester(tester_decorator)\n        # For now, using a simple placeholder test function\n        def test_fn(_mdl: nn.Module, _dataloader: DataLoader) -&gt; TestResult:\n            # Implement simple testing or return a placeholder\n            return TestResult(loss=0.42, metrics={})  # pragma: no cover\n\n        federated_tester = federate.tester.pytorch(test_fn)\n\n        return PyTorchFLTask.from_trainer_and_tester(\n            trainer=federated_trainer,\n            tester=federated_tester,\n        )\n</code></pre>"},{"location":"api_reference/trainers/","title":"Base Trainer","text":"<p>Base Trainer</p>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseTrainer","title":"BaseTrainer","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Trainer Class.</p> Source code in <code>src/fed_rag/base/trainer.py</code> <pre><code>class BaseTrainer(BaseModel, ABC):\n    \"\"\"Base Trainer Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    rag_system: RAGSystem\n    train_dataset: Any\n    _model = PrivateAttr()\n\n    @abstractmethod\n    def train(self) -&gt; TrainResult:\n        \"\"\"Train loop.\"\"\"\n\n    @abstractmethod\n    def evaluate(self) -&gt; TestResult:\n        \"\"\"Evaluation\"\"\"\n\n    @abstractmethod\n    def _get_model_from_rag_system(self) -&gt; Any:\n        \"\"\"Get the model from the RAG system.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def set_model(self) -&gt; \"BaseTrainer\":\n        self._model = self._get_model_from_rag_system()\n        return self\n\n    @property\n    def model(self) -&gt; Any:\n        \"\"\"Return the model to be trained.\"\"\"\n        return self._model\n\n    @model.setter\n    def model(self, v: Any) -&gt; None:\n        \"\"\"Set the model to be trained.\"\"\"\n        self._model = v\n</code></pre>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseTrainer.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model\n</code></pre> <p>Return the model to be trained.</p>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseTrainer.train","title":"train  <code>abstractmethod</code>","text":"<pre><code>train()\n</code></pre> <p>Train loop.</p> Source code in <code>src/fed_rag/base/trainer.py</code> <pre><code>@abstractmethod\ndef train(self) -&gt; TrainResult:\n    \"\"\"Train loop.\"\"\"\n</code></pre>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseTrainer.evaluate","title":"evaluate  <code>abstractmethod</code>","text":"<pre><code>evaluate()\n</code></pre> <p>Evaluation</p> Source code in <code>src/fed_rag/base/trainer.py</code> <pre><code>@abstractmethod\ndef evaluate(self) -&gt; TestResult:\n    \"\"\"Evaluation\"\"\"\n</code></pre>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseRetrieverTrainer","title":"BaseRetrieverTrainer","text":"<p>               Bases: <code>BaseTrainer</code>, <code>ABC</code></p> <p>Base Retriever Trainer Class.</p> Source code in <code>src/fed_rag/base/trainer.py</code> <pre><code>class BaseRetrieverTrainer(BaseTrainer, ABC):\n    \"\"\"Base Retriever Trainer Class.\"\"\"\n\n    def _get_model_from_rag_system(self) -&gt; Any:\n        if self.rag_system.retriever.encoder:\n            return self.rag_system.retriever.encoder\n        else:\n            return (\n                self.rag_system.retriever.query_encoder\n            )  # only update query encoder\n</code></pre>"},{"location":"api_reference/trainers/#src.fed_rag.base.trainer.BaseGeneratorTrainer","title":"BaseGeneratorTrainer","text":"<p>               Bases: <code>BaseTrainer</code>, <code>ABC</code></p> <p>Base Retriever Trainer Class.</p> Source code in <code>src/fed_rag/base/trainer.py</code> <pre><code>class BaseGeneratorTrainer(BaseTrainer, ABC):\n    \"\"\"Base Retriever Trainer Class.\"\"\"\n\n    def _get_model_from_rag_system(self) -&gt; Any:\n        return self.rag_system.generator.model\n</code></pre>"},{"location":"api_reference/trainers/huggingface/","title":"Huggingface","text":"<p>HuggingFace Trainer Mixin</p> <p>HuggingFace LM-Supervised Retriever Trainer</p> <p>HuggingFace Retrieval-Augmented Generator Trainer</p>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.mixin.HuggingFaceTrainerProtocol","title":"HuggingFaceTrainerProtocol","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/fed_rag/trainers/huggingface/mixin.py</code> <pre><code>@runtime_checkable\nclass HuggingFaceTrainerProtocol(Protocol):\n    train_dataset: \"Dataset\"\n    training_arguments: Optional[\"TrainingArguments\"]\n\n    def model(\n        self,\n    ) -&gt; Union[\"SentenceTransformer\", \"PreTrainedModel\", \"PeftModel\"]:\n        pass  # pragma: no cover\n</code></pre>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.mixin.HuggingFaceTrainerMixin","title":"HuggingFaceTrainerMixin","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>HuggingFace Trainer Mixin.</p> Source code in <code>src/fed_rag/trainers/huggingface/mixin.py</code> <pre><code>class HuggingFaceTrainerMixin(BaseModel, ABC):\n    \"\"\"HuggingFace Trainer Mixin.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n    train_dataset: \"Dataset\"\n    training_arguments: Optional[\"TrainingArguments\"] = None\n\n    def __init__(\n        self,\n        train_dataset: \"Dataset\",\n        training_arguments: Optional[\"TrainingArguments\"] = None,\n        **kwargs: Any,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n        super().__init__(\n            train_dataset=train_dataset,\n            training_arguments=training_arguments,\n            **kwargs,\n        )\n\n    @property\n    @abstractmethod\n    def hf_trainer_obj(self) -&gt; \"Trainer\":\n        \"\"\"A ~transformers.Trainer object.\"\"\"\n</code></pre>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.mixin.HuggingFaceTrainerMixin.hf_trainer_obj","title":"hf_trainer_obj  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>hf_trainer_obj\n</code></pre> <p>A ~transformers.Trainer object.</p>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.lsr.HuggingFaceTrainerForLSR","title":"HuggingFaceTrainerForLSR","text":"<p>               Bases: <code>HuggingFaceTrainerMixin</code>, <code>BaseRetrieverTrainer</code></p> <p>HuggingFace LM-Supervised Retriever Trainer.</p> Source code in <code>src/fed_rag/trainers/huggingface/lsr.py</code> <pre><code>class HuggingFaceTrainerForLSR(HuggingFaceTrainerMixin, BaseRetrieverTrainer):\n    \"\"\"HuggingFace LM-Supervised Retriever Trainer.\"\"\"\n\n    _hf_trainer: Optional[\"SentenceTransformerTrainer\"] = PrivateAttr(\n        default=None\n    )\n\n    def __init__(\n        self,\n        rag_system: RAGSystem,\n        train_dataset: \"Dataset\",\n        training_arguments: Optional[\"TrainingArguments\"] = None,\n        **kwargs: Any,\n    ):\n        super().__init__(\n            train_dataset=train_dataset,\n            rag_system=rag_system,\n            training_arguments=training_arguments,\n            **kwargs,\n        )\n\n    @model_validator(mode=\"after\")\n    def set_private_attributes(self) -&gt; \"HuggingFaceTrainerForLSR\":\n        # if made it to here, then this import is available\n        from sentence_transformers import SentenceTransformer\n\n        # validate rag system\n        _validate_rag_system(self.rag_system)\n\n        # validate model\n        if not isinstance(self.model, SentenceTransformer):\n            raise TrainerError(\n                \"For `HuggingFaceTrainerForLSR`, attribute `model` must be of type \"\n                \"`~sentence_transformers.SentenceTransformer`.\"\n            )\n\n        self._hf_trainer = LSRSentenceTransformerTrainer(\n            model=self.model,\n            args=self.training_arguments,\n            data_collator=DataCollatorForLSR(rag_system=self.rag_system),\n            train_dataset=self.train_dataset,\n        )\n\n        return self\n\n    def train(self) -&gt; TrainResult:\n        output: TrainOutput = self.hf_trainer_obj.train()\n        return TrainResult(loss=output.training_loss)\n\n    def evaluate(self) -&gt; TestResult:\n        # TODO: implement this\n        raise NotImplementedError\n\n    @property\n    def hf_trainer_obj(self) -&gt; \"SentenceTransformerTrainer\":\n        return self._hf_trainer\n</code></pre>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.lsr.LSRSentenceTransformerTrainer","title":"LSRSentenceTransformerTrainer","text":"<p>               Bases: <code>SentenceTransformerTrainer</code></p> Source code in <code>src/fed_rag/trainers/huggingface/lsr.py</code> <pre><code>class LSRSentenceTransformerTrainer(SentenceTransformerTrainer):\n    def __init__(\n        self,\n        *args: Any,\n        data_collator: DataCollatorForLSR,\n        loss: Optional[LSRLoss] = None,\n        **kwargs: Any,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        # set loss\n        if loss is None:\n            loss = LSRLoss()\n        else:\n            if not isinstance(loss, LSRLoss):\n                raise InvalidLossError(\n                    \"`LSRSentenceTransformerTrainer` must use ~fed_rag.loss.LSRLoss`.\"\n                )\n\n        if not isinstance(data_collator, DataCollatorForLSR):\n            raise InvalidDataCollatorError(\n                \"`LSRSentenceTransformerTrainer` must use ~fed_rag.data_collators.DataCollatorForLSR`.\"\n            )\n\n        super().__init__(\n            *args, loss=loss, data_collator=data_collator, **kwargs\n        )\n\n    def collect_scores(\n        self, inputs: dict[str, torch.Tensor | Any]\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        if \"retrieval_scores\" not in inputs:\n            raise MissingInputTensor(\n                \"Collated `inputs` are missing key `retrieval_scores`\"\n            )\n\n        if \"lm_scores\" not in inputs:\n            raise MissingInputTensor(\n                \"Collated `inputs` are missing key `lm_scores`\"\n            )\n\n        retrieval_scores = inputs.get(\"retrieval_scores\")\n        lm_scores = inputs.get(\"lm_scores\")\n\n        return retrieval_scores, lm_scores\n\n    def compute_loss(\n        self,\n        model: \"SentenceTransformer\",\n        inputs: dict[str, torch.Tensor | Any],\n        return_outputs: bool = False,\n        num_items_in_batch: Any | None = None,\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, dict[str, Any]]:\n        \"\"\"Compute LSR loss.\n\n        NOTE: the forward pass of the model is taken care of in the DataCollatorForLSR.\n\n        Args:\n            model (SentenceTransformer): _description_\n            inputs (dict[str, torch.Tensor  |  Any]): _description_\n            return_outputs (bool, optional): _description_. Defaults to False.\n            num_items_in_batch (Any | None, optional): _description_. Defaults to None.\n\n        Raises:\n            NotImplementedError: _description_\n\n        Returns:\n            torch.Tensor | tuple[torch.Tensor, dict[str, Any]]: _description_\n        \"\"\"\n        retrieval_scores, lm_scores = self.collect_scores(inputs)\n        loss = self.loss(retrieval_scores, lm_scores)\n\n        # inputs are actually the outputs of RAGSystem's \"forward\" pass\n        return (loss, inputs) if return_outputs else loss\n</code></pre>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.lsr.LSRSentenceTransformerTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(\n    model,\n    inputs,\n    return_outputs=False,\n    num_items_in_batch=None,\n)\n</code></pre> <p>Compute LSR loss.</p> <p>NOTE: the forward pass of the model is taken care of in the DataCollatorForLSR.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SentenceTransformer</code> <p>description</p> required <code>inputs</code> <code>dict[str, Tensor | Any]</code> <p>description</p> required <code>return_outputs</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <code>num_items_in_batch</code> <code>Any | None</code> <p>description. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <p>Returns:</p> Type Description <code>Tensor | tuple[Tensor, dict[str, Any]]</code> <p>torch.Tensor | tuple[torch.Tensor, dict[str, Any]]: description</p> Source code in <code>src/fed_rag/trainers/huggingface/lsr.py</code> <pre><code>def compute_loss(\n    self,\n    model: \"SentenceTransformer\",\n    inputs: dict[str, torch.Tensor | Any],\n    return_outputs: bool = False,\n    num_items_in_batch: Any | None = None,\n) -&gt; torch.Tensor | tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"Compute LSR loss.\n\n    NOTE: the forward pass of the model is taken care of in the DataCollatorForLSR.\n\n    Args:\n        model (SentenceTransformer): _description_\n        inputs (dict[str, torch.Tensor  |  Any]): _description_\n        return_outputs (bool, optional): _description_. Defaults to False.\n        num_items_in_batch (Any | None, optional): _description_. Defaults to None.\n\n    Raises:\n        NotImplementedError: _description_\n\n    Returns:\n        torch.Tensor | tuple[torch.Tensor, dict[str, Any]]: _description_\n    \"\"\"\n    retrieval_scores, lm_scores = self.collect_scores(inputs)\n    loss = self.loss(retrieval_scores, lm_scores)\n\n    # inputs are actually the outputs of RAGSystem's \"forward\" pass\n    return (loss, inputs) if return_outputs else loss\n</code></pre>"},{"location":"api_reference/trainers/huggingface/#src.fed_rag.trainers.huggingface.ralt.HuggingFaceTrainerForRALT","title":"HuggingFaceTrainerForRALT","text":"<p>               Bases: <code>HuggingFaceTrainerMixin</code>, <code>BaseGeneratorTrainer</code></p> <p>HuggingFace Trainer for Retrieval-Augmented LM Training/Fine-Tuning.</p> Source code in <code>src/fed_rag/trainers/huggingface/ralt.py</code> <pre><code>class HuggingFaceTrainerForRALT(HuggingFaceTrainerMixin, BaseGeneratorTrainer):\n    \"\"\"HuggingFace Trainer for Retrieval-Augmented LM Training/Fine-Tuning.\"\"\"\n\n    _hf_trainer: Optional[\"Trainer\"] = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        rag_system: RAGSystem,\n        train_dataset: \"Dataset\",\n        training_arguments: Optional[\"TrainingArguments\"] = None,\n        **kwargs: Any,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise MissingExtraError(msg)\n\n        if training_arguments is None:\n            training_arguments = _get_default_training_args()\n        else:\n            training_arguments.remove_unused_columns = (\n                False  # pragma: no cover\n            )\n\n        super().__init__(\n            train_dataset=train_dataset,\n            rag_system=rag_system,\n            training_arguments=training_arguments,\n            **kwargs,\n        )\n\n    @model_validator(mode=\"after\")\n    def set_private_attributes(self) -&gt; \"HuggingFaceTrainerForRALT\":\n        # if made it to here, then this import is available\n        from transformers import Trainer\n\n        # validate rag system\n        _validate_rag_system(self.rag_system)\n\n        self._hf_trainer = Trainer(\n            model=self.model,\n            args=self.training_arguments,\n            data_collator=DataCollatorForRALT(rag_system=self.rag_system),\n            train_dataset=self.train_dataset,\n        )\n\n        return self\n\n    def train(self) -&gt; TrainResult:\n        output: TrainOutput = self.hf_trainer_obj.train()\n        return TrainResult(loss=output.training_loss)\n\n    def evaluate(self) -&gt; TestResult:\n        # TODO: implement this\n        raise NotImplementedError\n\n    @property\n    def hf_trainer_obj(self) -&gt; \"Trainer\":\n        return self._hf_trainer\n</code></pre>"},{"location":"api_reference/trainers/pytorch/","title":"Pytorch","text":"<p>PyTorch Trainer Mixin</p>"},{"location":"api_reference/trainers/pytorch/#src.fed_rag.trainers.pytorch.mixin.PyTorchTrainerProtocol","title":"PyTorchTrainerProtocol","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/fed_rag/trainers/pytorch/mixin.py</code> <pre><code>@runtime_checkable\nclass PyTorchTrainerProtocol(Protocol):\n    train_dataset: Dataset\n    training_arguments: TrainingArgs | None\n    train_dataloader: DataLoader\n\n    def model(self) -&gt; nn.Module:\n        pass  # pragma: no cover\n</code></pre>"},{"location":"api_reference/trainers/pytorch/#src.fed_rag.trainers.pytorch.mixin.PyTorchTrainerMixin","title":"PyTorchTrainerMixin","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>HuggingFace Trainer Mixin.</p> Source code in <code>src/fed_rag/trainers/pytorch/mixin.py</code> <pre><code>class PyTorchTrainerMixin(BaseModel, ABC):\n    \"\"\"HuggingFace Trainer Mixin.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n    train_dataset: Dataset\n    train_dataloader: DataLoader\n    training_arguments: TrainingArgs | None = None\n\n    def __init__(\n        self,\n        train_dataloader: DataLoader,\n        train_dataset: Dataset | None = None,\n        training_arguments: TrainingArgs | None = None,\n        **kwargs: Any,\n    ):\n        if train_dataset is None:\n            train_dataset = train_dataloader.dataset\n        else:\n            # ensure consistency between loader.dataset and the supplied one\n            if id(train_dataset) != id(train_dataloader.dataset):\n                raise InconsistentDatasetError(\n                    \"Inconsistent datasets detected between supplied `train_dataset` and that \"\n                    \"associated with the `train_dataloader`. These two datasets must be the same.\"\n                )\n\n        super().__init__(\n            train_dataset=train_dataset,\n            train_dataloader=train_dataloader,\n            training_arguments=training_arguments,\n            **kwargs,\n        )\n</code></pre>"},{"location":"community/resources/pocket_references/","title":"AI Pocket References","text":"<p>The AI Pocket Reference project is maintained by Vector AI Engineering as an accessible resource for the AI community. It provides a collection of pocket references offering concise information on a wide range of AI topics, including Natural Language Processing (NLP) and Federated Learning (FL).</p>"},{"location":"community/resources/pocket_references/#recommended-collections","title":"Recommended Collections","text":"<ul> <li> <p>NLP Collection \u2014 Covers various topics within NLP, including RAG, LoRA, Quantization, Chain of Thought, Agents, and more.</p> </li> <li> <p>FL Collection \u2014 Encompasses the fundamentals of federated learning along with advanced topics such as personalized federated learning and vertical federated learning.</p> </li> </ul>"},{"location":"examples/","title":"Examples","text":""},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/essentials/","title":"Essentials","text":"<p>To get to know FedRAG a bit better and understand its purpose, we provide the answers to the following four essential questions.</p>"},{"location":"getting_started/essentials/#four-essential-questions","title":"Four Essential Questions","text":""},{"location":"getting_started/essentials/#what-is-rag","title":"What is RAG?","text":"<p>Retrieval-Augmented Generation (RAG) is a widely used technique that addresses a main drawback of Large Language Models (LLM), which is that they're trained on historical corpora and thus answering user queries that heavily rely on recent data are not really possible. Further, using the parametric knowledge of LLMs alone has yielded subpar performance on knowledge-intensive benchmarks.</p> <p>RAG provides access to relevant (and potentially more recent) non-parametric knowledge (i.e. data) that are stored in Knowledge Stores to the LLM so that it can use it in order to more accurately respond to user queries.</p>"},{"location":"getting_started/essentials/#what-is-federated-learning","title":"What is Federated Learning?","text":"<p>Federated Learning (FL) is a technique for building machine learning (as well as deep learning) models when the data is decentralized. Rather than first centralizing the datasets to a central location, which may not be possible due to strict data residency regulations or may be uneconomical due to the significant monetary costs in moving massive datasets, FL enables collaborative model building by facilitating the sharing of the model weights between the data providers.</p>"},{"location":"getting_started/essentials/#why-federated-fine-tuning-of-rag","title":"Why Federated Fine-Tuning of RAG?","text":"<p>Fine-tuning is a technique that is used to enhance the performance of LLMs by speciliazing its general capabilities towards a specific domain. It has also been shown that fine-tuning the model components of RAG systems, namely the generator and retriever, on domain-specific datasets can lead to its overall improved performance.</p> <p>Accessing fine-tuning datasets may be challenging. And, in situations where the data is dispersed across several nodes, and centralizing is either not possible or uneconomical, the fine-tuning of these RAG systems can be made possible through FL.</p>"},{"location":"getting_started/essentials/#who-is-fedrag-for","title":"Who is FedRAG for?","text":"<p>FedRAG is for the model builders, data scientists, and researchers who wish to fine-tune their RAG systems on their own datasets.</p> <p>Note \u2014 FedRAG prioritizes both centralized and federated RAG fine-tuning</p> <p>While FedRAG supports federated learning scenarios, it's designed first and foremost as a comprehensive RAG fine-tuning library. Most users deploy FedRAG in completely centralized environments to take advantage of its intuitive API, powerful abstractions, and integration with popular frameworks.</p> <p>Centralized mode offers the full range of RAG fine-tuning techniques with zero federation overhead. The federated capabilities are available when you need them for privacy-sensitive or distributed data scenarios.</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#installing-from-package-managers","title":"Installing from package managers","text":""},{"location":"getting_started/installation/#pypi","title":"PyPi","text":"<p>As seen in the previous quickstart examples, we can install FedRAG via <code>pip</code>:</p> <pre><code>pip install fed-rag\n</code></pre>"},{"location":"getting_started/installation/#conda","title":"Conda","text":"<p>For <code>conda</code> users, <code>fed-rag</code> has been published to the <code>conda-forge</code> channel, and thus can be installed with <code>conda</code> using the below command:</p> <pre><code>conda install -c conda-forge fed-rag\n</code></pre>"},{"location":"getting_started/installation/#installing-from-source","title":"Installing from source","text":"<p>To install from source, first clone the repository:</p> <pre><code># https\ngit clone https://github.com/VectorInstitute/fed-rag.git\n\n# ssh\ngit clone git@github.com:VectorInstitute/fed-rag.git\n</code></pre> <p>After cloning the repository, you have a few options for installing the library. The next two subsections outline how to complete the installation using either <code>pip</code> or <code>uv</code>, respectively.</p>"},{"location":"getting_started/installation/#using-pip","title":"Using <code>pip</code>","text":"<p>To complete the installation, first <code>cd</code> into the <code>fed-rag</code> directory and then run the following <code>pip install</code> command:</p> <pre><code>cd fed-rag\npip install -e .\n</code></pre> <p>Tip</p> <p>We recommended to always use a fresh virtual environment for new projects. Before running the above command, ensure that your dedicated virtual environment is active.</p>"},{"location":"getting_started/installation/#using-uv","title":"Using <code>uv</code>","text":"<p>FedRAG uses <code>uv</code> for dependency management, publishing to PyPi, and for setting up development environments.</p> <p>Users can also use <code>uv</code> to complete the source installation of FedRAG.</p> <p>Note</p> <p>This method requires <code>uv</code> to be installed onto the users development machine. For installation instructions visit <code>uv</code>'s official documentation.</p> <pre><code>cd fed-rag\nuv sync\n</code></pre> <p>To install with desired extras and groups, add the flags <code>--extra &lt;extra-name&gt;</code> and <code>--optional &lt;optional-name&gt;</code>, respectively. As an example:</p> <pre><code>cd fed-rag\nuv sync --extra huggingface --group dev\n</code></pre>"},{"location":"getting_started/standard_usage/","title":"Standard Usage","text":"<p>The standard usage pattern for FedRAG aligns to a natural RAG fine-tuning workflow, and looks as follows:</p> <ol> <li>Build a <code>RAGSystem</code></li> <li>Create a <code>RAGFinetuningDataset</code></li> <li>Define a training loop and evaluation function and decorate both of these with the appropriate <code>decorators</code>.</li> <li>Create an <code>FLTask</code></li> <li>Spin up FL servers and clients to begin federated fine-tuning!</li> </ol> <p>In the following subsections, we briefly elaborate on what's involved in each of these listed steps.</p> <p>Note</p> <p>Steps 1. through 3. are\u2014minus the decoration of trainers and testers\u2014typical steps one would perform for a centralized RAG fine-tuning task.</p> <p>Tip</p> <p>Before proceeding to federated learning, one should verify that the centralized task runs as intended with a representative dataset. In fact, centralized learning represents a standard baseline with which to compare federated learning results.</p>"},{"location":"getting_started/standard_usage/#build-a-ragsystem","title":"Build a <code>RAGSystem</code>","text":"<p>Building a <code>RAGSystem</code> involves defining a <code>Retriever</code>, <code>KnowledgeStore</code> as well as <code>Generator</code>, and subsequently supplying these along with a <code>RAGConfig</code> (to define parameters such a <code>top_k</code>) to the <code>RAGSystem</code> constructor.</p> building a rag system<pre><code>from fed_rag import RAGSystem, RAGConfig\n\n# three main components\nretriever = ...\nknowledge_store = ...\ngenerator = ...\n\nrag = RAGSystem(\n    generator=generator,\n    retriever=retriever,\n    knowledge_store=knowledge_store,\n    rag_config=RAGConfig(top_k=2),\n)\n</code></pre>"},{"location":"getting_started/standard_usage/#create-a-ragfinetuningdataset","title":"Create a <code>RAGFinetuningDataset</code>","text":"<p>With a <code>RAGSystem</code> in place, we can create a fine-tuning dataset using examples that contain queries and their associated answers. In retrieval-augmented generator fine-tuning, we process each example by calling the <code>RAGSystem.retrieve()</code> method with the query to fetch relevant knowledge nodes from the connected <code>KnowledgeStore</code>. These contextual nodes enhance each example, creating a collection of retrieval-augmented examples that form the RAG fine-tuning dataset for generator model training. Our how-to guides provide detailed instructions on performing this type of fine-tuning, as well as other approaches.</p> pytorchhuggingface creating a RAG fine-tuning dataset<pre><code>from fed_rag.utils.data import build_finetune_dataset\n\nexamples: list[dict[str, str]] = [{\"query\": ..., \"answer\": ...}, ...]\n\ndataset = build_finetune_dataset(\n    rag_system=rag_system, examples=examples, return_dataset=\"pt\", ...  # (1)!\n)\n</code></pre> <ol> <li>Check the API Reference for the remaining required parameters</li> </ol> creating a RAG fine-tuning dataset<pre><code>from fed_rag.utils.data import build_finetune_dataset\n\nexamples: list[dict[str, str]] = [{\"query\": ..., \"answer\": ...}, ...]\n\ndataset = build_finetune_dataset(\n    rag_system=rag_system, examples=examples, return_dataset=\"hf\", ...  # (1)!\n)\n</code></pre> <ol> <li>Check the API Reference for the remaining required parameters</li> </ol>"},{"location":"getting_started/standard_usage/#define-a-training-loop-and-evaluation-function","title":"Define a training loop and evaluation function","text":"<p>Like any model training process, a training loop establishes how the model learns from the dataset. Since RAG systems are essentially assemblies of component models (namely retriever and generator), we need to define a specific training loop to effectively learn from RAG fine-tuning datasets.</p> <p>The lift to transform this from a centralized task is to a federated one is minimal with FedRAG, and the first step towards this endeavour amounts to the application of trainer and tester <code>decorators</code> on the respective functions.</p> pytorchhuggingface decorating training loops and evaluation functions<pre><code>from fed_rag.decorators import federate\n\n\n@federate.trainer.pytorch\ndef training_loop():\n    ...\n\n\n@federate.tester.pytorch\ndef evaluate():\n    ...\n</code></pre> decorating training loops and evaluation functions<pre><code>from fed_rag.decorators import federate\n\n\n@federate.trainer.huggingface\ndef training_loop():\n    ...\n\n\n@federate.tester.huggingface\ndef evaluate():\n    ...\n</code></pre> <p>These decorators perform inspection on these functions to automatically parse the model as well as training and validation datasets.</p>"},{"location":"getting_started/standard_usage/#create-an-fltask","title":"Create an <code>FLTask</code>","text":"<p>The final step in the federation transformation involves building an <code>FLTask</code> using the decorated trainer and evaluation function.</p> pytorchhuggingface defining the FL task<pre><code>from fed_rag.fl_tasks.pytorch import PyTorchFLTask\n\n# use from_trainer_tester class method\nfl_task = PyTorchFLTask.from_trainer_and_tester(\n    trainer=decorated_trainer, tester=decorated_tester  # (1)!\n)\n</code></pre> <ol> <li>decorated with <code>federate.trainer.pytorch</code> and <code>federate.tester.pytorch</code>, respectively</li> </ol> defining the FL task<pre><code>from fed_rag.fl_tasks.huggingface import HuggingFaceFLTask\n\n# use from_trainer_tester class method\nfl_task = HuggingFaceFLTask.from_trainer_and_tester(\n    trainer=decorated_trainer, tester=decorated_tester  # (1)!\n)\n</code></pre> <ol> <li>decorated with <code>federate.trainer.huggingface</code> and <code>federate.tester.huggingface</code>, respectively</li> </ol>"},{"location":"getting_started/standard_usage/#spin-up-fl-servers-and-clients","title":"Spin up FL servers and clients","text":"<p>With an <code>FLTask</code>, we can obtain an FL server as well as clients. Starting a server and required number of clients will commence the federated training.</p> getting server and clients<pre><code>import flwr as fl  # (1)!\n\n# federate generator fine-tuning\nmodel = rag_system.generator.model\n\n# server\nserver = fl_task.server(model, ...)  # (2)!\n\n# client\nclient = fl_task.client(...)  # (3)!\n\n# the below commands are blocking and would need to be run in separate processes\nfl.server.start_server(server=server, server_address=\"[::]:8080\")\nfl.client.start_client(client=client, server_address=\"[::]:8080\")\n</code></pre> <ol> <li><code>flwr</code> is the backend federated learning framework for FedRAG and comes included with the installation of <code>fed-rag</code>.</li> <li>Can pass in FL aggregation strategy, otherwise defaults to federated averaging.</li> <li>Requires the same arguments as the centralized <code>training_loop</code>.</li> </ol> <p>Note</p> <p>Under the hood, <code>FLTask.server()</code> and <code>FLTask.client()</code> build <code>~flwr.Server</code> and <code>~flwr.Client</code> objects, respectively.</p>"},{"location":"getting_started/how_to/","title":"How-To Guides","text":""},{"location":"getting_started/quick_starts/","title":"Quick Starts","text":"<p>In this next part in getting to know FedRAG, we provide a mini series of quick start examples in order to get a better feeling of the library.</p> <ul> <li> Centralized to Federated \u2014 Transform   a centralized training task into a federated learning task.</li> <li> Build a RAG System \u2014 Assemble   a RAG System using FedRAG's lightweight abstractions.</li> <li> Fine-tune a RAG System \u2014 Fine-tune   a RAG system on custom QA data, demonstrating both centralized training and   optional federation capabilities.</li> </ul>"},{"location":"getting_started/quick_starts/federated/","title":"Centralized to Federated","text":"<p>In this quick start example, we'll demonstrate how to easily transform a centralized training task into a federated one with just a few additional lines of code.</p> <p>Let's start by installing the <code>fed-rag</code> library by using <code>pip</code>:</p> <pre><code>pip install fed-rag\n</code></pre>"},{"location":"getting_started/quick_starts/federated/#defining-the-centralized-task","title":"Defining the centralized task","text":"<p>As with any model training endeavour, we define the model, its training loop, and finally a function for evaluations. Experienced model builders will find this workflow comfortably familiar, as FedRAG maintains the same essential structure they're accustomed to while seamlessly introducing federation capabilities (as we will see shortly in the next sub section).</p>"},{"location":"getting_started/quick_starts/federated/#model","title":"Model","text":"<p>We define a simple multi-layer perceptron as our model.</p> the model<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# the model\nclass Net(torch.nn.Module):\n    def __init__(self) -&gt; None:\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(42, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n</code></pre> <p>Note</p> <p>FedRAG uses PyTorch as its main deep learning framework and so installing <code>fed-rag</code> also comes with the <code>torch</code> library.</p>"},{"location":"getting_started/quick_starts/federated/#training-loop","title":"Training loop","text":"<p>We use a standard training loop that is PyTorch native, making use of the <code>~torch.utils.data.DataLoader</code> class.</p> training loop<pre><code>...  # (1)!\nfrom torch.types import Device\nfrom torch.utils.data import DataLoader\nfrom fed_rag.types import TestResult\n\n\ndef train_loop(\n    model: torch.nn.Module,\n    train_data: DataLoader,\n    val_data: DataLoader,\n    device: Device,\n    num_epochs: int,\n    learning_rate: float | None,\n) -&gt; TrainResult:\n    \"\"\"My custom train loop.\"\"\"\n\n    model.to(device)  # move model to GPU if available\n    criterion = torch.nn.CrossEntropyLoss().to(device)\n    optimizer = torch.optim.SGD(\n        model.parameters(), lr=learning_rate, momentum=0.9\n    )\n    model.train()\n    running_loss = 0.0\n    for _ in range(num_epochs):\n        for batch in train_data:\n            features = batch[\"features\"]\n            labels = batch[\"label\"]\n            optimizer.zero_grad()\n            loss = criterion(model(features.to(device)), labels.to(device))\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n    avg_trainloss = running_loss / len(train_data)\n    return TrainResult(loss=avg_trainloss)\n</code></pre> <ol> <li>Includes the import statements from the previous code block.</li> </ol>"},{"location":"getting_started/quick_starts/federated/#evaluation-function","title":"Evaluation function","text":"<p>Finally, a typical evaluation function that computes the accuracy of the model.</p> evaluation function<pre><code>...  # (1)!\nfrom fed_rag.types import TestResult\n\n\ndef test(m: torch.nn.Module, test_loader: DataLoader) -&gt; TestResult:\n    \"\"\"My custom tester.\"\"\"\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    m.to(device)\n    criterion = torch.nn.CrossEntropyLoss()\n    correct, loss = 0, 0.0\n    with torch.no_grad():\n        for batch in test_loader:\n            features = batch[\"features\"].to(device)\n            labels = batch[\"label\"].to(device)\n            outputs = m(images)\n            loss += criterion(outputs, labels).item()\n            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n    accuracy = correct / len(test_loader.dataset)\n    return TestResult(loss=loss, metrics={\"accuracy\": accuracy})\n</code></pre> <ol> <li>Includes the import statements from the two previous code blocks.</li> </ol>"},{"location":"getting_started/quick_starts/federated/#centralized-training","title":"Centralized training","text":"<p>Training the model under the centralized setting is a simple matter of instantiating a model and invoking the <code>train()</code> loop.</p> Centralized training<pre><code>train_data = ...  # (1)!\nval_data = ...  # (2)!\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = Net()\ntrain(\n    model=model,\n    train_data=train_data,\n    val_data=val_data,\n    device=device,\n    num_epochs=3,\n    learning_rate=0.1,\n)\n</code></pre> <ol> <li>Pass in a train data loader.</li> <li>Pass in a validation data loader.</li> </ol>"},{"location":"getting_started/quick_starts/federated/#federating-the-centralized-task","title":"Federating the centralized task","text":"<p>In this section, we demonstrate how we can take the centralized task above, sensibly represented by the triple (model, trainer, tester) where trainer is the training loop, and tester is the evaluation function.</p>"},{"location":"getting_started/quick_starts/federated/#defining-the-fl-task","title":"Defining the FL task","text":"<p>The code block below shows how to define a <code>PyTorchFLTask</code> from the centralized trainer and tester functions, but not before automatically performing some required inspection on them.</p> Getting an FL Task<pre><code>from fed_rag.decorators import federate\nfrom fed_rag.fl_tasks.pytorch import PyTorchFLTask\n\n\n# apply decorators on the previously established trainer\ntrain_loop = federate.trainer.pytorch(train_loop)  # (1)!\ntest = federate.tester.pytorch(test)  # (2)!\n\n# define the fl task\nfl_task = PyTorchFLTask.from_trainer_and_tester(\n    trainer=train_loop, tester=test\n)\n</code></pre> <ol> <li><code>train_loop</code> as defined in the training loop code block.</li> <li><code>test</code> as defined in the evaluation function code block.</li> </ol> <p>Note</p> <p><code>federate.trainer.pytorch</code> and <code>federate.tester.pytorch</code> are both decorators and could have been incorporated in the training loop and evaluation function code blocks, respectively. We separated them here to clearly demonstrate the progression from centralized to federated implementation, making the transformation process more explicit. In typical usage, you would apply these decorators directly to your functions when defining them.</p>"},{"location":"getting_started/quick_starts/federated/#getting-a-server-and-clients","title":"Getting a server and clients","text":"<p>With the <code>FLTask</code> in hand, we can create a server and some clients in order to establish a federated network.</p> Getting a server and two clients<pre><code># the server\nmodel = Net()\nserver = fl_task.server(model=model)\n\n# defining two clients\nclients = []\nfor i in range(2):\n    train_data, val_data = get_loaders(partition_id=i)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    num_epochs = 1\n    learning_rate = 0.1\n\n    client = fl_task.client(\n        # train params\n        model=model,\n        train_data=train_data,\n        val_data=val_data,\n        device=device,\n        num_epochs=num_epochs,\n        learning_rate=learning_rate,\n    )\n    clients.append(client)\n</code></pre>"},{"location":"getting_started/quick_starts/federated/#federated-training","title":"Federated training","text":"<p>To perform the training, we simply need to start the servers and clients!</p> Starting the server and clients<pre><code>import flwr as fl\n\n# the below commands are blocking and would need to be run in separate processes\nfl.server.start_server(server=server, server_address=\"[::]:8080\")\nfl.client.start_client(client=clients[0], server_address=\"[::]:8080\")\nfl.client.start_client(client=clients[1], server_address=\"[::]:8080\")\n</code></pre> <p>Note</p> <p>FedRAG uses the <code>flwr</code> library as its backend federated learning framework, and like <code>torch</code>, comes bundled with installation of <code>fed-rag</code>.</p>"},{"location":"getting_started/quick_starts/rag_finetuning/","title":"Fine-tune a RAG System","text":"<p>In this quick start, we'll go over how we can take the RAG system we built from the previous quick start example, and fine-tune it.</p> <p>Note</p> <p>For this fine-tuning tutorial, you'll need the <code>huggingface</code> extra installed. If you haven't added it yet, run:</p> <p><code>pip install fed-rag[huggingface]</code></p> <p>This provides access to the HuggingFace models and training utilities we'll use for both retriever and generator fine-tuning.</p>"},{"location":"getting_started/quick_starts/rag_finetuning/#the-train-dataset","title":"The Train Dataset","text":"<p>Training a RAG system requires a train dataset that is familiarly shaped as a question-answering dataset.</p> training examples for RAG fine-tuning<pre><code>train_dataset = [  # (1)!\n    {\n        \"query\": [\n            \"What is machine learning?\",\n            \"Tell me about climate change\",\n            \"How do computers work?\",\n        ],\n        \"response\": [\n            \"Machine learning is a field of AI focused on algorithms that learn from data.\",\n            \"Climate change refers to long-term shifts in temperatures and weather patterns.\",\n            \"Computers work by processing information using logic gates and electronic components.\",\n        ],\n    }\n]\n</code></pre> <ol> <li>A train example is essentially a (<code>query</code>, <code>response</code>) pair.</li> </ol>"},{"location":"getting_started/quick_starts/rag_finetuning/#define-our-trainer-objects","title":"Define our Trainer objects","text":"<p>To perform RAG fine-tuning, FedRAG offers both a <code>BaseGeneratorTrainer</code> and a <code>BaseRetrieverTrainer</code> that incorporate the training logic for each of these respective RAG components.</p> <p>For this quick start, we make use of the following trainers:</p> <ul> <li><code>HuggingFaceTrainerForRALT</code> \u2014 A   generator trainer that fine-tunes the LLM using retrieval-augmented instruction   examples.</li> <li><code>HuggingFaceTrainerForLSR</code> \u2014 A   retriever trainer that fine-tunes the retriever model using retrieval chunk scores   and the log probabilities derived from the generator LLM using the ground truth   response.</li> </ul> retrieval-augmented fine-tuning<pre><code>from fed_rag.trainers.huggingface.ralt import HuggingFaceTrainerForRALT\nfrom fed_rag.trainers.huggingface.lsr import HuggingFaceTrainerForLSR\n\n\nrag_system = ...  # from previous quick start\ngenerator_trainer = HuggingFaceTrainerForRALT(\n    rag_system=rag_system,\n    train_dataset=train_dataset,\n)\nretriever_trainer = HuggingFaceTrainerForRALT(\n    rag_system=rag_system,\n    train_dataset=train_dataset,\n)\n</code></pre>"},{"location":"getting_started/quick_starts/rag_finetuning/#define-our-trainer-manager-object","title":"Define our Trainer Manager object","text":"<p>To orchestrate training between the two RAG components, FedRAG offers a manager class called <code>BaseTrainerManager</code>. The training manager contains logic to prepare the component and system for the specific training task (i.e., retriever or generator), and also contains a simple method to transform the task into a federated one.</p> training with managers<pre><code>from fed_rag.trainer_managers.huggingface import HuggingFaceTrainerManager\n\nmanager = HuggingFaceTrainerManager(\n    mode=\"retriever\",  # (1)!\n    retriever_trainer=retriever_trainer,\n    generator_trainer=generator_trainer,\n)\ntrain_result = manager.train()\nprint(f\"loss: {train_result.loss}\")\n\n# get your federated learning task (optional)\nfl_task = manager.get_federated_task()\n</code></pre> <ol> <li>Mode can be \"retriever\" or \"generator\"\u2014see <code>RAGTrainMode</code></li> </ol>"},{"location":"getting_started/quick_starts/rag_inference/","title":"Build a RAG System","text":"<p>In this quick start example, we'll demonstrate how to build a RAG system and subequently query it using FedRAG. We begin with a short primer on the components of RAG which mirror the abstractions that FedRAG uses to build such systems.</p>"},{"location":"getting_started/quick_starts/rag_inference/#components-of-rag","title":"Components of RAG","text":"The three main components of RAG. The three main components of RAG. <p>A RAG system is comprised of three main components, namely:</p> <ul> <li>Knowledge Store \u2014 contains non-parametric knowledge facts that the system   can use at inference time in order to produce more accurate responses to queries.</li> <li>Retriever \u2014 a model that takes in a user query and retrieves the most relevant   knowledge facts from the knowledge store.</li> <li>Generator \u2014 a model that takes in the user's query and additional context   and provides a response to that query.</li> </ul> <p>Info</p> <p>The Retriever is also used to populate (i.e index) the Knowledge Store during setup.</p>"},{"location":"getting_started/quick_starts/rag_inference/#building-a-rag-system","title":"Building a RAG system","text":"<p>We'll install FedRAG with the <code>huggingface</code> extra this time in order to build our RAG system using HuggingFace models.</p> <pre><code>pip install \"fed-rag[huggingface]\"\n</code></pre>"},{"location":"getting_started/quick_starts/rag_inference/#retriever","title":"Retriever","text":"<p>With the <code>huggingface</code> extra, we can use any <code>~sentence_transformers.SentenceTransfomer</code> as the retriever model, including dual encoders like the one used below. This HuggingFace encoder is used to define a <code>HFSentenceTransformerRetriever</code>.</p> retriever<pre><code>from fed_rag.retrievers.huggingface.hf_sentence_transformer import (\n    HFSentenceTransformerRetriever,\n)\n\nQUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\"\nCONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\"\n\nretriever = HFSentenceTransformerRetriever(\n    query_model_name=QUERY_ENCODER_NAME,\n    context_model_name=CONTEXT_ENCODER_NAME,\n    load_model_at_init=False,\n)\n</code></pre>"},{"location":"getting_started/quick_starts/rag_inference/#knowledge-store","title":"Knowledge Store","text":"<p>To create a knowledge store, we've create a toy set of only two knowledge artifacts that we'll encode and subsequently load into an <code>InMemoryKnowledgeStore</code>.</p> knowledge artifacts<pre><code>import json\n\n# knowledge chunks\nchunks_json_strs = [\n    '{\"id\": \"0\", \"title\": \"Orchid\", \"text\": \"Orchids are easily distinguished from other plants, as they share some very evident derived characteristics or synapomorphies. Among these are: bilateral symmetry of the flower (zygomorphism), many resupinate flowers, a nearly always highly modified petal (labellum), fused stamens and carpels, and extremely small seeds\"}'\n    '{\"id\": \"1\", \"title\": \"Tulip\", \"text\": \"Tulips are easily distinguished from other plants, as they share some very evident derived characteristics or synapomorphies. Among these are: bilateral symmetry of the flower (zygomorphism), many resupinate flowers, a nearly always highly modified petal (labellum), fused stamens and carpels, and extremely small seeds\"}'\n]\nchunks = [json.loads(line) for line in chunks_json_strs]\n</code></pre> knowledge store<pre><code>from fed_rag.knowledge_stores.in_memory import InMemoryKnowledgeStore\nfrom fed_rag.types.knowledge_node import KnowledgeNode, NodeType\n\nknowledge_store = InMemoryKnowledgeStore()\n\n# create knowledge nodes\nnodes = []\nfor c in chunks:\n    node = KnowledgeNode(\n        embedding=retriever.encode_context(c[\"text\"]).tolist(),\n        node_type=NodeType.TEXT,\n        text_content=c[\"text\"],\n        metadata={\"title\": c[\"title\"], \"id\": c[\"id\"]},\n    )\n    nodes.append(node)\n\n# load into knowledge_store\nknowledge_store.load_nodes(nodes=nodes)\n</code></pre>"},{"location":"getting_started/quick_starts/rag_inference/#generator","title":"Generator","text":"<p>With the <code>huggingface</code> extra installed, we can use any <code>~transformers.PreTrainedModel</code> as well as any <code>~peft.PeftModel</code>. For this example, we use the latter and define a HFPeftModelGenerator.</p> generator<pre><code>from fed_rag.generators.huggingface import HFPeftModelGenerator\nfrom transformers.generation.utils import GenerationConfig\nfrom transformers.utils.quantization_config import BitsAndBytesConfig\n\nPEFT_MODEL_NAME = \"Styxxxx/llama2_7b_lora-quac\"\nBASE_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n\ngeneration_cfg = GenerationConfig(\n    do_sample=True,\n    eos_token_id=[128000, 128009],\n    bos_token_id=128000,\n    max_new_tokens=4096,\n    top_p=0.9,\n    temperature=0.6,\n    cache_implementation=\"offloaded\",\n    stop_strings=\"&lt;/response&gt;\",\n)\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ngenerator = HFPeftModelGenerator(\n    model_name=PEFT_MODEL_NAME,\n    base_model_name=BASE_MODEL_NAME,\n    generation_config=generation_cfg,\n    load_model_at_init=False,\n    load_model_kwargs={\"is_trainable\": True, \"device_map\": \"auto\"},\n    load_base_model_kwargs={\n        \"device_map\": \"auto\",\n        \"quantization_config\": quantization_config,\n    },\n)\n</code></pre>"},{"location":"getting_started/quick_starts/rag_inference/#rag-system","title":"RAG System","text":"<p>Finally, with our three main components in hand, we can build our first <code>RAGSystem</code>!</p> RAG system<pre><code>from fed_rag.types.rag_system import RAGConfig, RAGSystem\n\nrag_config = RAGConfig(top_k=2)\nrag_system = RAGSystem(\n    knowledge_store=knowledge_store,\n    generator=generator,\n    retriever=retriever,\n    rag_config=rag_config,\n)\n</code></pre> querying our RAGSystem<pre><code># query the rag system\nresponse = rag_system.query(\"What is a Tulip?\")\n\nprint(f\"\\n{response}\")\n\n# inspect source nodes\nprint(response.source_nodes)\n</code></pre>"},{"location":"getting_started/quick_starts/rag_inference/#whats-next","title":"What's next?","text":"<p>Having learned how to build a RAG system, let's move on to learning how to fine-tune one!</p>"}]}