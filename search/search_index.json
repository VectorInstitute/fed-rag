{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/decorators/","title":"Trainer and Tester Decorators","text":"<p>Trainer Decorators</p> <p>Tester Decorators</p>"},{"location":"api_reference/decorators/#src.fed_rag.decorators.trainer.TrainerDecorators","title":"TrainerDecorators","text":"Source code in <code>src/fed_rag/decorators/trainer.py</code> <pre><code>class TrainerDecorators:\n    def pytorch(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.pytorch import inspect_trainer_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_trainer_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_trainer_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n\n    def huggingface(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.huggingface import inspect_trainer_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_trainer_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_trainer_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n</code></pre>"},{"location":"api_reference/decorators/#src.fed_rag.decorators.tester.TesterDecorators","title":"TesterDecorators","text":"Source code in <code>src/fed_rag/decorators/tester.py</code> <pre><code>class TesterDecorators:\n    def pytorch(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.pytorch import inspect_tester_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_tester_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_tester_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n\n    def huggingface(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.huggingface import inspect_tester_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_tester_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_tester_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n</code></pre>"},{"location":"api_reference/finetuning_datasets/","title":"Finetuning Datasets","text":"<p>Data utils</p>"},{"location":"api_reference/finetuning_datasets/#src.fed_rag.utils.data._functions.ReturnType","title":"ReturnType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/fed_rag/utils/data/_functions.py</code> <pre><code>class ReturnType(str, Enum):\n    PYTORCH = \"pt\"\n    HUGGINGFACE = \"hf\"\n    TEXT = \"txt\"\n</code></pre>"},{"location":"api_reference/finetuning_datasets/#src.fed_rag.utils.data._functions.build_finetune_dataset","title":"build_finetune_dataset","text":"<pre><code>build_finetune_dataset(\n    rag_system,\n    examples,\n    eos_token_id,\n    finetune_example_template=DEFAULT_FINETUNE_EXAMPLE_TEMPLATE,\n    query_key=\"query\",\n    answer_key=\"answer\",\n    return_dataset=ReturnType.PYTORCH,\n)\n</code></pre> <p>Generates the finetuning dataset using the supplied rag_system and examples.</p> Source code in <code>src/fed_rag/utils/data/_functions.py</code> <pre><code>def build_finetune_dataset(\n    rag_system: RAGSystem,\n    examples: Sequence[dict],\n    eos_token_id: int,\n    finetune_example_template: str = DEFAULT_FINETUNE_EXAMPLE_TEMPLATE,\n    query_key: str = \"query\",\n    answer_key: str = \"answer\",\n    return_dataset: ReturnType = ReturnType.PYTORCH,\n) -&gt; Any:\n    \"\"\"Generates the finetuning dataset using the supplied rag_system and examples.\"\"\"\n\n    if (\n        isinstance(return_dataset, str)\n        and return_dataset not in ReturnType._value2member_map_.keys()\n    ):\n        raise ValueError(\n            \"Invalid `return_type` specified.\"\n        )  # TODO: give a proper exception to this\n\n    inputs_list = []\n    targets_list = []\n    finetuning_instances = []\n    for example in examples:\n        # retrieve\n        source_nodes = rag_system.retrieve(query=example[query_key])\n        total_sum_scores = sum(s.score for s in source_nodes)\n\n        # parallel in-context retrieval-augmentation creates\n        # top_k separated finetuning instances\n        for source in source_nodes:\n            finetune_instance_text = finetune_example_template.format(\n                query=example[query_key],\n                answer=example[answer_key],\n                context=source.node.get_content()[\"text_content\"],\n            )\n            finetuning_instances.append(finetune_instance_text)\n            _weight = source.score / total_sum_scores\n\n            # tokenize to get input_ids and target_ids\n            tokenizer = rag_system.generator.tokenizer\n            input_ids = tokenizer.encode(finetune_instance_text)\n            target_ids = input_ids[1:] + [eos_token_id]\n\n            inputs_list.append(input_ids)\n            targets_list.append(target_ids)\n\n    if return_dataset == ReturnType.TEXT:\n        return finetuning_instances\n    elif return_dataset == ReturnType.PYTORCH:\n        return PyTorchRAGFinetuningDataset(\n            input_ids=[torch.Tensor(el) for el in inputs_list],\n            target_ids=[torch.Tensor(el) for el in targets_list],\n        )\n    elif return_dataset == ReturnType.HUGGINGFACE:\n        # needs `fed-rag[huggingface]` extra to be installed\n        # this import will fail if not installed\n        from fed_rag.utils.data.finetuning_datasets.huggingface import (\n            HuggingFaceRAGFinetuningDataset,\n        )\n\n        return HuggingFaceRAGFinetuningDataset.from_inputs(\n            input_ids=inputs_list, target_ids=targets_list\n        )\n    else:\n        assert_never(return_dataset)  # pragma: no cover\n</code></pre>"},{"location":"api_reference/finetuning_datasets/huggingface/","title":"Huggingface","text":"<p>HuggingFace RAG Finetuning Dataset</p>"},{"location":"api_reference/finetuning_datasets/huggingface/#src.fed_rag.utils.data.finetuning_datasets.huggingface.HuggingFaceRAGFinetuningDataset","title":"HuggingFaceRAGFinetuningDataset","text":"<p>               Bases: <code>Dataset</code></p> <p>Thin wrapper over ~datasets.Dataset.</p> Source code in <code>src/fed_rag/utils/data/finetuning_datasets/huggingface.py</code> <pre><code>class HuggingFaceRAGFinetuningDataset(Dataset):\n    \"\"\"Thin wrapper over ~datasets.Dataset.\"\"\"\n\n    @classmethod\n    def from_inputs(\n        cls, input_ids: list[list[int]], target_ids: list[list[int]]\n    ) -&gt; Self:\n        return cls.from_dict(  # type: ignore[no-any-return]\n            {\"input_ids\": input_ids, \"target_ids\": target_ids}\n        )\n</code></pre>"},{"location":"api_reference/finetuning_datasets/pytorch/","title":"Pytorch","text":"<p>PyTorch RAG Finetuning Dataset</p>"},{"location":"api_reference/finetuning_datasets/pytorch/#src.fed_rag.utils.data.finetuning_datasets.pytorch.PyTorchRAGFinetuningDataset","title":"PyTorchRAGFinetuningDataset","text":"<p>               Bases: <code>Dataset</code></p> <p>PyTorch RAG Fine-Tuning Dataset Class.</p> <p>Parameters:</p> Name Type Description Default <code>Dataset</code> <code>_type_</code> <p>description</p> required Source code in <code>src/fed_rag/utils/data/finetuning_datasets/pytorch.py</code> <pre><code>class PyTorchRAGFinetuningDataset(Dataset):\n    \"\"\"PyTorch RAG Fine-Tuning Dataset Class.\n\n    Args:\n        Dataset (_type_): _description_\n    \"\"\"\n\n    def __init__(\n        self, input_ids: list[torch.Tensor], target_ids: list[torch.Tensor]\n    ):\n        self.input_ids = input_ids\n        self.target_ids = target_ids\n\n    def __len__(self) -&gt; int:\n        return len(self.input_ids)\n\n    def __getitem__(self, idx: int) -&gt; Any:\n        return self.input_ids[idx], self.target_ids[idx]\n</code></pre>"},{"location":"api_reference/fl_tasks/","title":"Base FL Task Classes","text":"<p>Base FL Task</p>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask","title":"BaseFLTask","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>class BaseFLTask(BaseModel, ABC):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def training_loop(self) -&gt; Callable:\n        ...\n\n    @classmethod\n    @abstractmethod\n    def from_configs(\n        cls, trainer_cfg: BaseFLTaskConfig, tester_cfg: Any\n    ) -&gt; Self:\n        ...\n\n    @classmethod\n    @abstractmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        try:\n            trainer_cfg = getattr(trainer, \"__fl_task_trainer_config\")\n        except AttributeError:\n            msg = (\n                \"`__fl_task_trainer_config` has not been set on training loop. Make \"\n                \"sure to decorate your training loop with the appropriate \"\n                \"decorator.\"\n            )\n            raise MissingFLTaskConfig(msg)\n\n        try:\n            tester_cfg = getattr(tester, \"__fl_task_tester_config\")\n        except AttributeError:\n            msg = (\n                \"`__fl_task_tester_config` has not been set on tester callable. Make \"\n                \"sure to decorate your tester with the appropriate decorator.\"\n            )\n            raise MissingFLTaskConfig(msg)\n        return cls.from_configs(trainer_cfg, tester_cfg)\n\n    @abstractmethod\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        \"\"\"Simulate the FL task.\n\n        Either use flwr's simulation tools, or create our own here.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def server(self, **kwargs: Any) -&gt; Server:\n        \"\"\"Create a flwr.Server object.\"\"\"\n        ...\n\n    @abstractmethod\n    def client(self, **kwargs: Any) -&gt; Client:\n        \"\"\"Create a flwr.Client object.\"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.simulate","title":"simulate  <code>abstractmethod</code>","text":"<pre><code>simulate(num_clients, **kwargs)\n</code></pre> <p>Simulate the FL task.</p> <p>Either use flwr's simulation tools, or create our own here.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n    \"\"\"Simulate the FL task.\n\n    Either use flwr's simulation tools, or create our own here.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.server","title":"server  <code>abstractmethod</code>","text":"<pre><code>server(**kwargs)\n</code></pre> <p>Create a flwr.Server object.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef server(self, **kwargs: Any) -&gt; Server:\n    \"\"\"Create a flwr.Server object.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.client","title":"client  <code>abstractmethod</code>","text":"<pre><code>client(**kwargs)\n</code></pre> <p>Create a flwr.Client object.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef client(self, **kwargs: Any) -&gt; Client:\n    \"\"\"Create a flwr.Client object.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTaskConfig","title":"BaseFLTaskConfig","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>class BaseFLTaskConfig(BaseModel):\n    pass\n</code></pre>"},{"location":"api_reference/fl_tasks/huggingface/","title":"Huggingface","text":"<p>HuggingFace FL Task.</p> <p>NOTE: Using this module requires the <code>huggingface</code> extra to be installed.</p>"},{"location":"api_reference/fl_tasks/huggingface/#src.fed_rag.fl_tasks.huggingface.HuggingFaceFlowerClient","title":"HuggingFaceFlowerClient","text":"<p>               Bases: <code>NumPyClient</code></p> Source code in <code>src/fed_rag/fl_tasks/huggingface.py</code> <pre><code>class HuggingFaceFlowerClient(NumPyClient):\n    def __init__(\n        self,\n        task_bundle: BaseFLTaskBundle,\n    ) -&gt; None:\n        super().__init__()\n        self.task_bundle = task_bundle\n\n    def __getattr__(self, name: str) -&gt; Any:\n        if name in self.task_bundle.model_fields:\n            return getattr(self.task_bundle, name)\n        else:\n            return super().__getattr__(name)\n\n    def get_weights(self) -&gt; NDArrays:\n        return _get_weights(self.net)\n\n    def set_weights(self, parameters: NDArrays) -&gt; None:\n        if isinstance(self.net, PeftModel):\n            # get state dict\n            state_dict = get_peft_model_state_dict(self.net)\n            state_dict = cast(Dict[str, Any], state_dict)\n\n            # update state dict with supplied parameters\n            params_dict = zip(state_dict.keys(), parameters)\n            state_dict = OrderedDict(\n                {k: torch.tensor(v) for k, v in params_dict}\n            )\n            set_peft_model_state_dict(self.net, state_dict)\n        else:  # SentenceTransformer | PreTrainedModel\n            # get state dict\n            state_dict = self.net.state_dict()\n\n            # update state dict with supplied parameters\n            params_dict = zip(state_dict.keys(), parameters)\n            state_dict = OrderedDict(\n                {k: torch.tensor(v) for k, v in params_dict}\n            )\n            self.net.load_state_dict(state_dict, strict=True)\n\n    def fit(\n        self, parameters: NDArrays, config: dict[str, Scalar]\n    ) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n        self.set_weights(parameters)\n\n        result: TrainResult = self.trainer(\n            self.net,\n            self.train_dataset,\n            self.val_dataset,\n            **self.task_bundle.extra_train_kwargs,\n        )\n        return (\n            self.get_weights(),\n            len(self.train_dataset),\n            {\"loss\": result.loss},\n        )\n\n    def evaluate(\n        self, parameters: NDArrays, config: dict[str, Scalar]\n    ) -&gt; tuple[float, int, dict[str, Scalar]]:\n        self.set_weights(parameters)\n        result: TestResult = self.tester(\n            self.net, self.val_dataset, **self.extra_test_kwargs\n        )\n        return result.loss, len(self.val_dataset), result.metrics\n</code></pre>"},{"location":"api_reference/fl_tasks/huggingface/#src.fed_rag.fl_tasks.huggingface.HuggingFaceFLTask","title":"HuggingFaceFLTask","text":"<p>               Bases: <code>BaseFLTask</code></p> Source code in <code>src/fed_rag/fl_tasks/huggingface.py</code> <pre><code>class HuggingFaceFLTask(BaseFLTask):\n    _client: NumPyClient = PrivateAttr()\n    _trainer: Callable = PrivateAttr()\n    _trainer_spec: TrainerSignatureSpec = PrivateAttr()\n    _tester: Callable = PrivateAttr()\n    _tester_spec: TesterSignatureSpec = PrivateAttr()\n\n    def __init__(\n        self,\n        trainer: Callable,\n        trainer_spec: TrainerSignatureSpec,\n        tester: Callable,\n        tester_spec: TesterSignatureSpec,\n        **kwargs: Any,\n    ) -&gt; None:\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise ValueError(msg)\n\n        if (\n            trainer_spec.net_parameter_class_name\n            != tester_spec.net_parameter_class_name\n        ):\n            msg = (\n                \"`trainer`'s model class is not the same as that for `tester`.\"\n            )\n            raise NetTypeMismatch(msg)\n\n        if trainer_spec.net_parameter != tester_spec.net_parameter:\n            msg = (\n                \"`trainer`'s model parameter name is not the same as that for `tester`. \"\n                \"Will use the name supplied in `trainer`.\"\n            )\n            warnings.warn(msg, UnequalNetParamWarning)\n\n        super().__init__(**kwargs)\n        self._trainer = trainer\n        self._trainer_spec = trainer_spec\n        self._tester = tester\n        self._tester_spec = tester_spec\n\n    @property\n    def training_loop(self) -&gt; Callable:\n        return self._trainer\n\n    @classmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        # extract trainer spec\n        try:\n            trainer_spec: TrainerSignatureSpec = getattr(\n                trainer, \"__fl_task_trainer_config\"\n            )\n        except AttributeError:\n            msg = \"Cannot extract `TrainerSignatureSpec` from supplied `trainer`.\"\n            raise MissingTrainerSpec(msg)\n\n        # extract tester spec\n        try:\n            tester_spec: TesterSignatureSpec = getattr(\n                tester, \"__fl_task_tester_config\"\n            )\n        except AttributeError:\n            msg = (\n                \"Cannot extract `TesterSignatureSpec` from supplied `tester`.\"\n            )\n            raise MissingTesterSpec(msg)\n\n        return cls(\n            trainer=trainer,\n            trainer_spec=trainer_spec,\n            tester=tester,\n            tester_spec=tester_spec,\n        )\n\n    def server(\n        self,\n        strategy: Strategy | None = None,\n        client_manager: ClientManager | None = None,\n        **kwargs: Any,\n    ) -&gt; HuggingFaceFlowerServer | None:\n        if strategy is None:\n            if self._trainer_spec.net_parameter not in kwargs:\n                msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n                raise MissingRequiredNetParam(msg)\n\n            model = kwargs.pop(self._trainer_spec.net_parameter)\n\n            ndarrays = _get_weights(model)\n            parameters = ndarrays_to_parameters(ndarrays)\n            strategy = FedAvg(\n                fraction_evaluate=1.0,\n                initial_parameters=parameters,\n            )\n\n        if client_manager is None:\n            client_manager = SimpleClientManager()\n\n        return HuggingFaceFlowerServer(\n            client_manager=client_manager, strategy=strategy\n        )\n\n    def client(self, **kwargs: Any) -&gt; Client | None:\n        # validate kwargs\n        if self._trainer_spec.net_parameter not in kwargs:\n            msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n            raise MissingRequiredNetParam(msg)\n        # build bundle\n        net = kwargs.pop(self._trainer_spec.net_parameter)\n        train_dataset = kwargs.pop(self._trainer_spec.train_data_param)\n        val_dataset = kwargs.pop(self._trainer_spec.val_data_param)\n\n        bundle = BaseFLTaskBundle(\n            net=net,\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            extra_train_kwargs=kwargs,\n            extra_test_kwargs={},  # TODO make this functional or get rid of it\n            trainer=self._trainer,\n            tester=self._tester,\n        )\n        return HuggingFaceFlowerClient(task_bundle=bundle)\n\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        raise NotImplementedError\n\n    @classmethod\n    def from_configs(cls, trainer_cfg: Any, tester_cfg: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/fl_tasks/pytorch/","title":"Pytorch","text":"<p>PyTorch FL Task</p>"},{"location":"api_reference/fl_tasks/pytorch/#src.fed_rag.fl_tasks.pytorch.PyTorchFLTask","title":"PyTorchFLTask","text":"<p>               Bases: <code>BaseFLTask</code></p> Source code in <code>src/fed_rag/fl_tasks/pytorch.py</code> <pre><code>class PyTorchFLTask(BaseFLTask):\n    _client: NumPyClient = PrivateAttr()\n    _trainer: Callable = PrivateAttr()\n    _trainer_spec: TrainerSignatureSpec = PrivateAttr()\n    _tester: Callable = PrivateAttr()\n    _tester_spec: TesterSignatureSpec = PrivateAttr()\n\n    def __init__(\n        self,\n        trainer: Callable,\n        trainer_spec: TrainerSignatureSpec,\n        tester: Callable,\n        tester_spec: TesterSignatureSpec,\n        **kwargs: Any,\n    ) -&gt; None:\n        if trainer_spec.net_parameter != tester_spec.net_parameter:\n            msg = (\n                \"`trainer`'s model parameter name is not the same as that for `tester`. \"\n                \"Will use the name supplied in `trainer`.\"\n            )\n            warnings.warn(msg, UnequalNetParamWarning)\n\n        super().__init__(**kwargs)\n        self._trainer = trainer\n        self._trainer_spec = trainer_spec\n        self._tester = tester\n        self._tester_spec = tester_spec\n\n    @property\n    def training_loop(self) -&gt; Callable:\n        return self._trainer\n\n    @classmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        # extract trainer spec\n        try:\n            trainer_spec: TrainerSignatureSpec = getattr(\n                trainer, \"__fl_task_trainer_config\"\n            )\n        except AttributeError:\n            msg = \"Cannot extract `TrainerSignatureSpec` from supplied `trainer`.\"\n            raise MissingTrainerSpec(msg)\n\n        # extract tester spec\n        try:\n            tester_spec: TesterSignatureSpec = getattr(\n                tester, \"__fl_task_tester_config\"\n            )\n        except AttributeError:\n            msg = (\n                \"Cannot extract `TesterSignatureSpec` from supplied `tester`.\"\n            )\n            raise MissingTesterSpec(msg)\n\n        return cls(\n            trainer=trainer,\n            trainer_spec=trainer_spec,\n            tester=tester,\n            tester_spec=tester_spec,\n        )\n\n    @classmethod\n    def from_configs(cls, trainer_cfg: Any, tester_cfg: Any) -&gt; Any:\n        return super().from_configs(trainer_cfg, tester_cfg)\n\n    def server(\n        self,\n        strategy: Strategy | None = None,\n        client_manager: ClientManager | None = None,\n        **kwargs: Any,\n    ) -&gt; PyTorchFlowerServer | None:\n        if strategy is None:\n            if self._trainer_spec.net_parameter not in kwargs:\n                msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n                raise MissingRequiredNetParam(msg)\n\n            model = kwargs.pop(self._trainer_spec.net_parameter)\n\n            ndarrays = _get_weights(model)\n            parameters = ndarrays_to_parameters(ndarrays)\n            strategy = FedAvg(\n                fraction_evaluate=1.0,\n                initial_parameters=parameters,\n            )\n\n        if client_manager is None:\n            client_manager = SimpleClientManager()\n\n        return PyTorchFlowerServer(\n            client_manager=client_manager, strategy=strategy\n        )\n\n    def client(self, **kwargs: Any) -&gt; Client | None:\n        # validate kwargs\n        if self._trainer_spec.net_parameter not in kwargs:\n            msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n            raise MissingRequiredNetParam(msg)\n        # build bundle\n        net = kwargs.pop(self._trainer_spec.net_parameter)\n        trainloader = kwargs.pop(self._trainer_spec.train_data_param)\n        valloader = kwargs.pop(self._trainer_spec.val_data_param)\n\n        bundle = BaseFLTaskBundle(\n            net=net,\n            trainloader=trainloader,\n            valloader=valloader,\n            extra_train_kwargs=kwargs,\n            extra_test_kwargs={},  # TODO make this functional or get rid of it\n            trainer=self._trainer,\n            tester=self._tester,\n        )\n        return PyTorchFlowerClient(task_bundle=bundle)\n\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/generators/","title":"Generators","text":"<p>Base Generator</p>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator","title":"BaseGenerator","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Generator Class.</p> Source code in <code>src/fed_rag/base/generator.py</code> <pre><code>class BaseGenerator(BaseModel, ABC):\n    \"\"\"Base Generator Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def generate(self, query: str, context: str, **kwargs: dict) -&gt; str:\n        \"\"\"Generate an output from a given query and context.\"\"\"\n\n    @property\n    @abstractmethod\n    def model(self) -&gt; torch.nn.Module:\n        \"\"\"Model associated with this generator.\"\"\"\n\n    @property\n    @abstractmethod\n    def tokenizer(self) -&gt; BaseTokenizer:\n        \"\"\"Tokenizer associated with this generator.\"\"\"\n</code></pre>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.model","title":"model  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>model\n</code></pre> <p>Model associated with this generator.</p>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.tokenizer","title":"tokenizer  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>tokenizer\n</code></pre> <p>Tokenizer associated with this generator.</p>"},{"location":"api_reference/generators/#src.fed_rag.base.generator.BaseGenerator.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(query, context, **kwargs)\n</code></pre> <p>Generate an output from a given query and context.</p> Source code in <code>src/fed_rag/base/generator.py</code> <pre><code>@abstractmethod\ndef generate(self, query: str, context: str, **kwargs: dict) -&gt; str:\n    \"\"\"Generate an output from a given query and context.\"\"\"\n</code></pre>"},{"location":"api_reference/generators/huggingface/","title":"Huggingface","text":"<p>HuggingFace PeftModel Generator</p> <p>HuggingFace PretrainedModel Generator</p>"},{"location":"api_reference/generators/huggingface/#src.fed_rag.generators.hf_peft_model.HFPeftModelGenerator","title":"HFPeftModelGenerator","text":"<p>               Bases: <code>BaseGenerator</code></p> <p>HFPeftModelGenerator Class.</p> <p>NOTE: this class supports loading PeftModel's from HF Hub or from local. TODO: support loading custom models via a <code>~peft.Config</code> and <code>~peft.get_peft_model</code></p> Source code in <code>src/fed_rag/generators/hf_peft_model.py</code> <pre><code>class HFPeftModelGenerator(BaseGenerator):\n    \"\"\"HFPeftModelGenerator Class.\n\n    NOTE: this class supports loading PeftModel's from HF Hub or from local.\n    TODO: support loading custom models via a `~peft.Config` and `~peft.get_peft_model`\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str = Field(\n        description=\"Name of Peft model. Used for loading model from HF hub or local.\"\n    )\n    base_model_name: str = Field(\n        description=\"Name of the frozen HuggingFace base model. Used for loading the model from HF hub or local.\"\n    )\n    generation_config: \"GenerationConfig\" = Field(\n        description=\"The generation config used for generating with the PreTrainedModel.\"\n    )\n    load_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading peft model from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    load_base_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading base model from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    prompt_template: str = Field(description=\"Prompt template for RAG.\")\n    _model: Optional[\"PeftModel\"] = PrivateAttr(default=None)\n    _tokenizer: HFPretrainedTokenizer | None = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str,\n        base_model_name: str,\n        generation_config: Optional[\"GenerationConfig\"] = None,\n        prompt_template: str | None = None,\n        load_model_kwargs: dict | None = None,\n        load_base_model_kwargs: dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise ValueError(msg)\n\n        generation_config = (\n            generation_config if generation_config else GenerationConfig()\n        )\n        prompt_template = (\n            prompt_template if prompt_template else DEFAULT_PROMPT_TEMPLATE\n        )\n        super().__init__(\n            model_name=model_name,\n            base_model_name=base_model_name,\n            generation_config=generation_config,\n            prompt_template=prompt_template,\n            load_model_kwargs=load_model_kwargs if load_model_kwargs else {},\n            load_base_model_kwargs=(\n                load_base_model_kwargs if load_base_model_kwargs else {}\n            ),\n        )\n        self._tokenizer = HFPretrainedTokenizer(\n            model_name=base_model_name, load_model_at_init=load_model_at_init\n        )\n        if load_model_at_init:\n            self._model = self._load_model_from_hf()\n\n    def _load_model_from_hf(self, **kwargs: Any) -&gt; \"PeftModel\":\n        load_base_kwargs = self.load_base_model_kwargs\n        load_kwargs = self.load_model_kwargs\n        load_kwargs.update(kwargs)\n        self.load_model_kwargs = load_kwargs  # update load_model_kwargs\n        base_model = AutoModelForCausalLM.from_pretrained(\n            self.base_model_name, **load_base_kwargs\n        )\n\n        if \"quantization_config\" in load_base_kwargs:\n            # preprocess model for kbit fine-tuning\n            # https://huggingface.co/docs/peft/developer_guides/quantization\n            base_model = prepare_model_for_kbit_training(base_model)\n\n        return PeftModel.from_pretrained(\n            base_model, self.model_name, **load_kwargs\n        )\n\n    @property\n    def model(self) -&gt; \"PeftModel\":\n        if self._model is None:\n            # load HF PeftModel\n            self._model = self._load_model_from_hf()\n        return self._model\n\n    @model.setter\n    def model(self, value: \"PeftModel\") -&gt; None:\n        self._model = value\n\n    @property\n    def tokenizer(self) -&gt; HFPretrainedTokenizer:\n        return self._tokenizer\n\n    # generate\n    def generate(self, query: str, context: str, **kwargs: Any) -&gt; str:\n        formatted_query = self.prompt_template.format(\n            question=query, context=context\n        )\n\n        # encode query\n        tokenizer_result = self.tokenizer.unwrapped(\n            formatted_query, return_tensors=\"pt\"\n        )\n        inputs: torch.Tensor = tokenizer_result.input_ids\n        inputs = inputs.to(self.model.device)\n\n        # generate\n        generated_ids = self.model.generate(\n            inputs=inputs,\n            generation_config=self.generation_config,\n            tokenizer=self.tokenizer.unwrapped,\n            **kwargs,\n        )\n\n        # decode tokens\n        outputs: list[str] = self.tokenizer.unwrapped.batch_decode(\n            generated_ids, skip_special_tokens=True\n        )\n        return outputs[0]\n</code></pre>"},{"location":"api_reference/generators/huggingface/#src.fed_rag.generators.hf_pretrained_model.HFPretrainedModelGenerator","title":"HFPretrainedModelGenerator","text":"<p>               Bases: <code>BaseGenerator</code></p> Source code in <code>src/fed_rag/generators/hf_pretrained_model.py</code> <pre><code>class HFPretrainedModelGenerator(BaseGenerator):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str = Field(\n        description=\"Name of HuggingFace model. Used for loading the model from HF hub or local.\"\n    )\n    generation_config: \"GenerationConfig\" = Field(\n        description=\"The generation config used for generating with the PreTrainedModel.\"\n    )\n    load_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading models from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    prompt_template: str = Field(description=\"Prompt template for RAG.\")\n    _model: Optional[\"PreTrainedModel\"] = PrivateAttr(default=None)\n    _tokenizer: HFPretrainedTokenizer | None = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str,\n        generation_config: Optional[\"GenerationConfig\"] = None,\n        prompt_template: str | None = None,\n        load_model_kwargs: dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise ValueError(msg)\n\n        generation_config = (\n            generation_config if generation_config else GenerationConfig()\n        )\n        prompt_template = (\n            prompt_template if prompt_template else DEFAULT_PROMPT_TEMPLATE\n        )\n        super().__init__(\n            model_name=model_name,\n            generation_config=generation_config,\n            prompt_template=prompt_template,\n            load_model_kwargs=load_model_kwargs if load_model_kwargs else {},\n        )\n        self._tokenizer = HFPretrainedTokenizer(\n            model_name=model_name, load_model_at_init=load_model_at_init\n        )\n        if load_model_at_init:\n            self._model = self._load_model_from_hf()\n\n    def _load_model_from_hf(self, **kwargs: Any) -&gt; \"PreTrainedModel\":\n        load_kwargs = self.load_model_kwargs\n        load_kwargs.update(kwargs)\n        self.load_model_kwargs = load_kwargs\n        model = AutoModelForCausalLM.from_pretrained(\n            self.model_name, **load_kwargs\n        )\n        return model\n\n    @property\n    def model(self) -&gt; \"PreTrainedModel\":\n        if self._model is None:\n            # load HF Pretrained Model\n            model = self._load_model_from_hf()\n            self._model = model\n        return self._model\n\n    @model.setter\n    def model(self, value: \"PreTrainedModel\") -&gt; None:\n        self._model = value\n\n    @property\n    def tokenizer(self) -&gt; HFPretrainedTokenizer:\n        return self._tokenizer\n\n    # generate\n    def generate(self, query: str, context: str, **kwargs: Any) -&gt; str:\n        formatted_query = self.prompt_template.format(\n            question=query, context=context\n        )\n\n        # encode query\n        tokenizer_result = self.tokenizer.unwrapped(\n            formatted_query, return_tensors=\"pt\"\n        )\n        inputs: torch.Tensor = tokenizer_result.input_ids\n        inputs = inputs.to(self.model.device)\n\n        # generate\n        generated_ids = self.model.generate(\n            inputs=inputs,\n            generation_config=self.generation_config,\n            tokenizer=self.tokenizer.unwrapped,\n            **kwargs,\n        )\n\n        # decode tokens\n        outputs: list[str] = self.tokenizer.unwrapped.batch_decode(\n            generated_ids, skip_special_tokens=True\n        )\n        return outputs[0]\n</code></pre>"},{"location":"api_reference/inspectors/","title":"Inspectors","text":"<p>Common abstractions for inspectors</p> <p>TesterResult</p>"},{"location":"api_reference/inspectors/#src.fed_rag.inspectors.common.TesterSignatureSpec","title":"TesterSignatureSpec","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/inspectors/common.py</code> <pre><code>class TesterSignatureSpec(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    net_parameter: str\n    test_data_param: str\n    extra_test_kwargs: list[str] = []\n    net_parameter_class_name: str\n</code></pre>"},{"location":"api_reference/inspectors/#src.fed_rag.inspectors.common.TesterSignatureSpec","title":"TesterSignatureSpec","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/inspectors/common.py</code> <pre><code>class TesterSignatureSpec(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    net_parameter: str\n    test_data_param: str\n    extra_test_kwargs: list[str] = []\n    net_parameter_class_name: str\n</code></pre>"},{"location":"api_reference/inspectors/#src.fed_rag.types.results.TrainResult","title":"TrainResult","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/types/results.py</code> <pre><code>class TrainResult(BaseModel):\n    loss: float\n</code></pre>"},{"location":"api_reference/inspectors/#src.fed_rag.types.results.TestResult","title":"TestResult","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/types/results.py</code> <pre><code>class TestResult(BaseModel):\n    __test__ = (\n        False  # needed for Pytest collision. Avoids PytestCollectionWarning\n    )\n    loss: float\n    metrics: dict[str, Any] = Field(\n        description=\"Additional metrics computed on test set.\",\n        default_factory=dict,\n    )\n</code></pre>"},{"location":"api_reference/inspectors/huggingface/","title":"Huggingface","text":""},{"location":"api_reference/inspectors/huggingface/#src.fed_rag.inspectors.huggingface.inspect_trainer_signature","title":"inspect_trainer_signature","text":"<pre><code>inspect_trainer_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/huggingface/trainer.py</code> <pre><code>def inspect_trainer_signature(fn: Callable) -&gt; TrainerSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TrainResult):\n        msg = \"Trainer should return a fed_rag.types.TrainResult or a subclass of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_train_kwargs = []\n    net_param = None\n    train_data_param = None\n    val_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if (\n                type_name\n                in [\n                    \"SentenceTransformer\",\n                    \"PreTrainedModel\",\n                    \"PeftModel\",\n                ]  # TODO: should accept union types involving these two\n                and net_param is None\n            ):\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"Dataset\" and train_data_param is None:\n                train_data_param = name\n                continue\n\n            if type_name == \"Dataset\" and val_data_param is None:\n                val_data_param = name\n                continue\n\n        extra_train_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For HuggingFace this param must have type `PreTrainedModel` or `SentenceTransformers`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if train_data_param is None:\n        msg = (\n            \"Inspection failed to find two data params for train and val datasets.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingMultipleDataParams(msg)\n\n    if val_data_param is None:\n        msg = (\n            \"Inspection found one data param but failed to find another. \"\n            \"Two data params are required for train and val datasets.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TrainerSignatureSpec(\n        net_parameter=net_param,\n        train_data_param=train_data_param,\n        val_data_param=val_data_param,\n        extra_train_kwargs=extra_train_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/huggingface/#src.fed_rag.inspectors.huggingface.inspect_tester_signature","title":"inspect_tester_signature","text":"<pre><code>inspect_tester_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/huggingface/tester.py</code> <pre><code>def inspect_tester_signature(fn: Callable) -&gt; TesterSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TestResult):\n        msg = \"Tester should return a fed_rag.types.TestResult or a subclass of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_tester_kwargs = []\n    net_param = None\n    test_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if (\n                type_name\n                in [\"PreTrainedModel\", \"SentenceTransformer\", \"PeftModel\"]\n                and net_param is None\n            ):\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"Dataset\" and test_data_param is None:\n                test_data_param = name\n                continue\n\n        extra_tester_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For HuggingFace this param must have type `PreTrainedModel` or `SentenceTransformers`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if test_data_param is None:\n        msg = (\n            \"Inspection failed to find a data param for a test dataset.\"\n            \"For HuggingFace these params must be of type `datasets.Dataset`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TesterSignatureSpec(\n        net_parameter=net_param,\n        test_data_param=test_data_param,\n        extra_test_kwargs=extra_tester_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/pytorch/","title":"Pytorch","text":""},{"location":"api_reference/inspectors/pytorch/#src.fed_rag.inspectors.pytorch.inspect_trainer_signature","title":"inspect_trainer_signature","text":"<pre><code>inspect_trainer_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/pytorch/trainer.py</code> <pre><code>def inspect_trainer_signature(fn: Callable) -&gt; TrainerSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TrainResult):\n        msg = \"Trainer should return a fed_rag.types.TrainResult or a subclsas of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_train_kwargs = []\n    net_param = None\n    train_data_param = None\n    val_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if type_name == \"Module\" and net_param is None:\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"DataLoader\" and train_data_param is None:\n                train_data_param = name\n                continue\n\n            if type_name == \"DataLoader\" and val_data_param is None:\n                val_data_param = name\n                continue\n\n        extra_train_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For PyTorch this param must have type `nn.Module`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if train_data_param is None:\n        msg = (\n            \"Inspection failed to find two data params for train and val datasets.\"\n            \"For PyTorch these params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingMultipleDataParams(msg)\n\n    if val_data_param is None:\n        msg = (\n            \"Inspection found one data param but failed to find another. \"\n            \"Two data params are required for train and val datasets.\"\n            \"For PyTorch these params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TrainerSignatureSpec(\n        net_parameter=net_param,\n        train_data_param=train_data_param,\n        val_data_param=val_data_param,\n        extra_train_kwargs=extra_train_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/inspectors/pytorch/#src.fed_rag.inspectors.pytorch.inspect_tester_signature","title":"inspect_tester_signature","text":"<pre><code>inspect_tester_signature(fn)\n</code></pre> Source code in <code>src/fed_rag/inspectors/pytorch/tester.py</code> <pre><code>def inspect_tester_signature(fn: Callable) -&gt; TesterSignatureSpec:\n    sig = inspect.signature(fn)\n\n    # validate return type\n    return_type = sig.return_annotation\n    if (return_type is Any) or not issubclass(return_type, TestResult):\n        msg = \"Tester should return a fed_rag.types.TestResult or a subclsas of it.\"\n        raise InvalidReturnType(msg)\n\n    # inspect fn params\n    extra_tester_kwargs = []\n    net_param = None\n    test_data_param = None\n    net_parameter_class_name = None\n\n    for name, t in sig.parameters.items():\n        if name in (\"self\", \"cls\"):\n            continue\n\n        if type_name := getattr(t.annotation, \"__name__\", None):\n            if type_name == \"Module\" and net_param is None:\n                net_param = name\n                net_parameter_class_name = type_name\n                continue\n\n            if type_name == \"DataLoader\" and test_data_param is None:\n                test_data_param = name\n                continue\n\n        extra_tester_kwargs.append(name)\n\n    if net_param is None:\n        msg = (\n            \"Inspection failed to find a model param. \"\n            \"For PyTorch this param must have type `nn.Module`.\"\n        )\n        raise MissingNetParam(msg)\n\n    if test_data_param is None:\n        msg = (\n            \"Inspection failed to find a data param for a test dataset.\"\n            \"For PyTorch this params must be of type `torch.utils.data.DataLoader`\"\n        )\n        raise MissingDataParam(msg)\n\n    spec = TesterSignatureSpec(\n        net_parameter=net_param,\n        test_data_param=test_data_param,\n        extra_test_kwargs=extra_tester_kwargs,\n        net_parameter_class_name=net_parameter_class_name,\n    )\n    return spec\n</code></pre>"},{"location":"api_reference/knowledge_nodes/","title":"Index","text":"<p>Knowledge Node</p>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.types.knowledge_node.KnowledgeNode","title":"KnowledgeNode","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/types/knowledge_node.py</code> <pre><code>class KnowledgeNode(BaseModel):\n    model_config = ConfigDict(\n        # ensures that validation is performed for defaulted None values\n        validate_default=True\n    )\n    node_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    embedding: list[float] = Field(\n        description=\"Encoded representation of node. If multimodal type, then this is shared embedding between image and text.\"\n    )\n    node_type: NodeType = Field(description=\"Type of node.\")\n    text_content: str | None = Field(\n        description=\"Text content. Used for TEXT and potentially MULTIMODAL node types.\",\n        default=None,\n    )\n    image_content: bytes | None = Field(\n        description=\"Image content as binary data (decoded from base64)\",\n        default=None,\n    )\n    metadata: dict = Field(\n        description=\"Metadata for node.\", default_factory=dict\n    )\n\n    # validators\n    @field_validator(\"text_content\", mode=\"before\")\n    @classmethod\n    def validate_text_content(\n        cls, value: str | None, info: ValidationInfo\n    ) -&gt; str | None:\n        node_type = info.data.get(\"node_type\")\n        node_type = cast(NodeType, node_type)\n        if node_type == NodeType.TEXT and value is None:\n            raise ValueError(\"NodeType == 'text', but text_content is None.\")\n\n        if node_type == NodeType.MULTIMODAL and value is None:\n            raise ValueError(\n                \"NodeType == 'multimodal', but text_content is None.\"\n            )\n\n        return value\n\n    @field_validator(\"image_content\", mode=\"after\")\n    @classmethod\n    def validate_image_content(\n        cls, value: str | None, info: ValidationInfo\n    ) -&gt; str | None:\n        node_type = info.data.get(\"node_type\")\n        node_type = cast(NodeType, node_type)\n        if node_type == NodeType.IMAGE:\n            if value is None:\n                raise ValueError(\n                    \"NodeType == 'image', but image_content is None.\"\n                )\n\n        if node_type == NodeType.MULTIMODAL:\n            if value is None:\n                raise ValueError(\n                    \"NodeType == 'multimodal', but image_content is None.\"\n                )\n\n        return value\n\n    def get_content(self) -&gt; NodeContent:\n        \"\"\"Return dict of node content.\"\"\"\n        content: NodeContent = {\n            \"image_content\": self.image_content,\n            \"text_content\": self.text_content,\n        }\n        return content\n\n    @field_serializer(\"metadata\")\n    def serialize_metadata(\n        self, metadata: dict[Any, Any] | None\n    ) -&gt; str | None:\n        \"\"\"\n        Custom serializer for the metadata field.\n\n        Will serialize the metadata field into a json string.\n\n        Args:\n            metadata: Metadata dictionary to serialize.\n\n        Returns:\n            Serialized metadata as a json string.\n        \"\"\"\n        if metadata:\n            return json.dumps(metadata)\n        return None\n\n    @field_validator(\"metadata\", mode=\"before\")\n    @classmethod\n    def deserialize_metadata(\n        cls, metadata: dict[Any, Any] | str | None\n    ) -&gt; dict[Any, Any] | None:\n        \"\"\"\n        Custom validator for the metadata field.\n\n        Will deserialize the metadata from a json string if it's a string.\n\n        Args:\n            metadata: Metadata to validate. If it is a json string, it will be deserialized into a dictionary.\n\n        Returns:\n            Validated metadata.\n        \"\"\"\n        if isinstance(metadata, str):\n            deserialized_metadata = json.loads(metadata)\n            return cast(dict[Any, Any], deserialized_metadata)\n        return metadata\n</code></pre>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.types.knowledge_node.KnowledgeNode.get_content","title":"get_content","text":"<pre><code>get_content()\n</code></pre> <p>Return dict of node content.</p> Source code in <code>src/fed_rag/types/knowledge_node.py</code> <pre><code>def get_content(self) -&gt; NodeContent:\n    \"\"\"Return dict of node content.\"\"\"\n    content: NodeContent = {\n        \"image_content\": self.image_content,\n        \"text_content\": self.text_content,\n    }\n    return content\n</code></pre>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.types.knowledge_node.KnowledgeNode.serialize_metadata","title":"serialize_metadata","text":"<pre><code>serialize_metadata(metadata)\n</code></pre> <p>Custom serializer for the metadata field.</p> <p>Will serialize the metadata field into a json string.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[Any, Any] | None</code> <p>Metadata dictionary to serialize.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Serialized metadata as a json string.</p> Source code in <code>src/fed_rag/types/knowledge_node.py</code> <pre><code>@field_serializer(\"metadata\")\ndef serialize_metadata(\n    self, metadata: dict[Any, Any] | None\n) -&gt; str | None:\n    \"\"\"\n    Custom serializer for the metadata field.\n\n    Will serialize the metadata field into a json string.\n\n    Args:\n        metadata: Metadata dictionary to serialize.\n\n    Returns:\n        Serialized metadata as a json string.\n    \"\"\"\n    if metadata:\n        return json.dumps(metadata)\n    return None\n</code></pre>"},{"location":"api_reference/knowledge_nodes/#src.fed_rag.types.knowledge_node.KnowledgeNode.deserialize_metadata","title":"deserialize_metadata  <code>classmethod</code>","text":"<pre><code>deserialize_metadata(metadata)\n</code></pre> <p>Custom validator for the metadata field.</p> <p>Will deserialize the metadata from a json string if it's a string.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[Any, Any] | str | None</code> <p>Metadata to validate. If it is a json string, it will be deserialized into a dictionary.</p> required <p>Returns:</p> Type Description <code>dict[Any, Any] | None</code> <p>Validated metadata.</p> Source code in <code>src/fed_rag/types/knowledge_node.py</code> <pre><code>@field_validator(\"metadata\", mode=\"before\")\n@classmethod\ndef deserialize_metadata(\n    cls, metadata: dict[Any, Any] | str | None\n) -&gt; dict[Any, Any] | None:\n    \"\"\"\n    Custom validator for the metadata field.\n\n    Will deserialize the metadata from a json string if it's a string.\n\n    Args:\n        metadata: Metadata to validate. If it is a json string, it will be deserialized into a dictionary.\n\n    Returns:\n        Validated metadata.\n    \"\"\"\n    if isinstance(metadata, str):\n        deserialized_metadata = json.loads(metadata)\n        return cast(dict[Any, Any], deserialized_metadata)\n    return metadata\n</code></pre>"},{"location":"api_reference/knowledge_stores/","title":"Base KnowledgeStore","text":"<p>Base Knowledge Store</p>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore","title":"BaseKnowledgeStore","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Knowledge Store Class.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>class BaseKnowledgeStore(BaseModel, ABC):\n    \"\"\"Base Knowledge Store Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def load_node(self, node: KnowledgeNode) -&gt; None:\n        \"\"\"Load a KnowledgeNode into the KnowledgeStore.\"\"\"\n\n    @abstractmethod\n    def load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n        \"\"\"Load multiple KnowledgeNodes in batch.\"\"\"\n\n    @abstractmethod\n    def retrieve(\n        self, query_emb: list[float], top_k: int\n    ) -&gt; list[tuple[float, KnowledgeNode]]:\n        \"\"\"Retrieve top-k nodes from KnowledgeStore against a provided user query.\n\n        Returns:\n            A list of tuples where the first element represents the similarity score\n            of the node to the query, and the second element is the node itself.\n        \"\"\"\n\n    @abstractmethod\n    def delete_node(self, node_id: str) -&gt; bool:\n        \"\"\"Remove a node from the KnowledgeStore by ID, returning success status.\"\"\"\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Clear all nodes from the KnowledgeStore.\"\"\"\n\n    @property\n    @abstractmethod\n    def count(self) -&gt; int:\n        \"\"\"Return the number of nodes in the store.\"\"\"\n\n    @abstractmethod\n    def persist(self) -&gt; None:\n        \"\"\"Save the KnowledgeStore nodes to a permanent storage.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def load(cls, ks_id: str) -&gt; Self:\n        \"\"\"\n        Load the KnowledgeStore nodes from a permanent storage given an id.\n\n        Args:\n            ks_id: The id of the knowledge store to load.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.count","title":"count  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>count\n</code></pre> <p>Return the number of nodes in the store.</p>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load_node","title":"load_node  <code>abstractmethod</code>","text":"<pre><code>load_node(node)\n</code></pre> <p>Load a KnowledgeNode into the KnowledgeStore.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef load_node(self, node: KnowledgeNode) -&gt; None:\n    \"\"\"Load a KnowledgeNode into the KnowledgeStore.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load_nodes","title":"load_nodes  <code>abstractmethod</code>","text":"<pre><code>load_nodes(nodes)\n</code></pre> <p>Load multiple KnowledgeNodes in batch.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n    \"\"\"Load multiple KnowledgeNodes in batch.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.retrieve","title":"retrieve  <code>abstractmethod</code>","text":"<pre><code>retrieve(query_emb, top_k)\n</code></pre> <p>Retrieve top-k nodes from KnowledgeStore against a provided user query.</p> <p>Returns:</p> Type Description <code>list[tuple[float, KnowledgeNode]]</code> <p>A list of tuples where the first element represents the similarity score</p> <code>list[tuple[float, KnowledgeNode]]</code> <p>of the node to the query, and the second element is the node itself.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef retrieve(\n    self, query_emb: list[float], top_k: int\n) -&gt; list[tuple[float, KnowledgeNode]]:\n    \"\"\"Retrieve top-k nodes from KnowledgeStore against a provided user query.\n\n    Returns:\n        A list of tuples where the first element represents the similarity score\n        of the node to the query, and the second element is the node itself.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.delete_node","title":"delete_node  <code>abstractmethod</code>","text":"<pre><code>delete_node(node_id)\n</code></pre> <p>Remove a node from the KnowledgeStore by ID, returning success status.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef delete_node(self, node_id: str) -&gt; bool:\n    \"\"\"Remove a node from the KnowledgeStore by ID, returning success status.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.clear","title":"clear  <code>abstractmethod</code>","text":"<pre><code>clear()\n</code></pre> <p>Clear all nodes from the KnowledgeStore.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Clear all nodes from the KnowledgeStore.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.persist","title":"persist  <code>abstractmethod</code>","text":"<pre><code>persist()\n</code></pre> <p>Save the KnowledgeStore nodes to a permanent storage.</p> Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@abstractmethod\ndef persist(self) -&gt; None:\n    \"\"\"Save the KnowledgeStore nodes to a permanent storage.\"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/#src.fed_rag.base.knowledge_store.BaseKnowledgeStore.load","title":"load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>load(ks_id)\n</code></pre> <p>Load the KnowledgeStore nodes from a permanent storage given an id.</p> <p>Parameters:</p> Name Type Description Default <code>ks_id</code> <code>str</code> <p>The id of the knowledge store to load.</p> required Source code in <code>src/fed_rag/base/knowledge_store.py</code> <pre><code>@classmethod\n@abstractmethod\ndef load(cls, ks_id: str) -&gt; Self:\n    \"\"\"\n    Load the KnowledgeStore nodes from a permanent storage given an id.\n\n    Args:\n        ks_id: The id of the knowledge store to load.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/knowledge_stores/in_memory/","title":"InMemory","text":"<p>In Memory Knowledge Store</p>"},{"location":"api_reference/knowledge_stores/in_memory/#src.fed_rag.knowledge_stores.in_memory.InMemoryKnowledgeStore","title":"InMemoryKnowledgeStore","text":"<p>               Bases: <code>BaseKnowledgeStore</code></p> <p>InMemoryKnowledgeStore Class.</p> Source code in <code>src/fed_rag/knowledge_stores/in_memory.py</code> <pre><code>class InMemoryKnowledgeStore(BaseKnowledgeStore):\n    \"\"\"InMemoryKnowledgeStore Class.\"\"\"\n\n    default_save_path: ClassVar[str] = \".fed_rag/data_cache/{0}.parquet\"\n\n    ks_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    _data: dict[str, KnowledgeNode] = PrivateAttr(default_factory=dict)\n\n    @classmethod\n    def from_nodes(cls, nodes: list[KnowledgeNode]) -&gt; Self:\n        instance = cls()\n        instance.load_nodes(nodes)\n        return instance\n\n    def load_node(self, node: KnowledgeNode) -&gt; None:\n        if node.node_id not in self._data:\n            self._data[node.node_id] = node\n\n    def load_nodes(self, nodes: list[KnowledgeNode]) -&gt; None:\n        for node in nodes:\n            self.load_node(node)\n\n    def retrieve(\n        self, query_emb: list[float], top_k: int\n    ) -&gt; list[tuple[float, KnowledgeNode]]:\n        all_nodes = list(self._data.values())\n        node_ids_and_scores = _get_top_k_nodes(\n            nodes=all_nodes, query_emb=query_emb, top_k=top_k\n        )\n        return [(el[1], self._data[el[0]]) for el in node_ids_and_scores]\n\n    def delete_node(self, node_id: str) -&gt; bool:\n        if node_id in self._data:\n            del self._data[node_id]\n            return True\n        else:\n            return False\n\n    def clear(self) -&gt; None:\n        self._data = {}\n\n    @property\n    def count(self) -&gt; int:\n        return len(self._data)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(self, handler: Any) -&gt; Dict[str, Any]:\n        data = handler(self)\n        data = cast(Dict[str, Any], data)\n        # include _data in serialization\n        if self._data:\n            data[\"_data\"] = self._data\n        return data  # type: ignore[no-any-return]\n\n    def persist(self) -&gt; None:\n        serialized_model = self.model_dump()\n        data_values = list(serialized_model[\"_data\"].values())\n\n        parquet_table = pa.Table.from_pylist(data_values)\n\n        filename = self.__class__.default_save_path.format(self.ks_id)\n        Path(filename).parent.mkdir(parents=True, exist_ok=True)\n        pq.write_table(parquet_table, filename)\n\n    @classmethod\n    def load(cls, ks_id: str) -&gt; Self:\n        filename = cls.default_save_path.format(ks_id)\n        parquet_data = pq.read_table(filename).to_pylist()\n\n        knowledge_store = cls.from_nodes(\n            [KnowledgeNode(**data) for data in parquet_data]\n        )\n        knowledge_store.ks_id = ks_id\n        return knowledge_store\n</code></pre>"},{"location":"api_reference/rag_system/","title":"RAG System and Auxiliary Types","text":"<p>RAG System</p>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.SourceNode","title":"SourceNode","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>class SourceNode(BaseModel):\n    score: float\n    node: KnowledgeNode\n\n    def __getattr__(self, __name: str) -&gt; Any:\n        \"\"\"Convenient wrapper on getattr of associated node.\"\"\"\n        return getattr(self.node, __name)\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.SourceNode.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(__name)\n</code></pre> <p>Convenient wrapper on getattr of associated node.</p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>def __getattr__(self, __name: str) -&gt; Any:\n    \"\"\"Convenient wrapper on getattr of associated node.\"\"\"\n    return getattr(self.node, __name)\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.RAGSystem","title":"RAGSystem","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>class RAGSystem(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    generator: BaseGenerator\n    retriever: BaseRetriever\n    knowledge_store: BaseKnowledgeStore\n    rag_config: RAGConfig\n\n    def query(self, query: str) -&gt; RAGResponse:\n        \"\"\"Query the RAG system.\"\"\"\n        source_nodes = self.retrieve(query)\n        context = self._format_context(source_nodes)\n        response = self.generate(query=query, context=context)\n        return RAGResponse(source_nodes=source_nodes, response=response)\n\n    def retrieve(self, query: str) -&gt; list[SourceNode]:\n        \"\"\"Retrieve from KnowledgeStore.\"\"\"\n        query_emb: list[float] = self.retriever.encode_query(query).tolist()\n        raw_retrieval_result = self.knowledge_store.retrieve(\n            query_emb=query_emb, top_k=self.rag_config.top_k\n        )\n        return [\n            SourceNode(score=el[0], node=el[1]) for el in raw_retrieval_result\n        ]\n\n    def generate(self, query: str, context: str) -&gt; str:\n        \"\"\"Generate response to query with context.\"\"\"\n        return self.generator.generate(query=query, context=context)  # type: ignore\n\n    def _format_context(self, source_nodes: list[SourceNode]) -&gt; str:\n        \"\"\"Format the context from the source nodes.\"\"\"\n        # TODO: how to format image context\n        return self.rag_config.context_separator.join(\n            [node.get_content()[\"text_content\"] for node in source_nodes]\n        )\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.RAGSystem.query","title":"query","text":"<pre><code>query(query)\n</code></pre> <p>Query the RAG system.</p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>def query(self, query: str) -&gt; RAGResponse:\n    \"\"\"Query the RAG system.\"\"\"\n    source_nodes = self.retrieve(query)\n    context = self._format_context(source_nodes)\n    response = self.generate(query=query, context=context)\n    return RAGResponse(source_nodes=source_nodes, response=response)\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.RAGSystem.retrieve","title":"retrieve","text":"<pre><code>retrieve(query)\n</code></pre> <p>Retrieve from KnowledgeStore.</p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>def retrieve(self, query: str) -&gt; list[SourceNode]:\n    \"\"\"Retrieve from KnowledgeStore.\"\"\"\n    query_emb: list[float] = self.retriever.encode_query(query).tolist()\n    raw_retrieval_result = self.knowledge_store.retrieve(\n        query_emb=query_emb, top_k=self.rag_config.top_k\n    )\n    return [\n        SourceNode(score=el[0], node=el[1]) for el in raw_retrieval_result\n    ]\n</code></pre>"},{"location":"api_reference/rag_system/#src.fed_rag.types.rag_system.RAGSystem.generate","title":"generate","text":"<pre><code>generate(query, context)\n</code></pre> <p>Generate response to query with context.</p> Source code in <code>src/fed_rag/types/rag_system.py</code> <pre><code>def generate(self, query: str, context: str) -&gt; str:\n    \"\"\"Generate response to query with context.\"\"\"\n    return self.generator.generate(query=query, context=context)  # type: ignore\n</code></pre>"},{"location":"api_reference/retrievers/","title":"Retrievers","text":"<p>Base Retriever</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever","title":"BaseRetriever","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Retriever Class.</p> Source code in <code>src/fed_rag/base/retriever.py</code> <pre><code>class BaseRetriever(BaseModel, ABC):\n    \"\"\"Base Retriever Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def encode_query(\n        self, query: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode query.\"\"\"\n\n    @abstractmethod\n    def encode_context(\n        self, context: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode context.\"\"\"\n\n    @property\n    @abstractmethod\n    def encoder(self) -&gt; torch.nn.Module | None:\n        \"\"\"PyTorch model associated with the encoder associated with retriever.\"\"\"\n\n    @property\n    @abstractmethod\n    def query_encoder(self) -&gt; torch.nn.Module | None:\n        \"\"\"PyTorch model associated with the query encoder associated with retriever.\"\"\"\n\n    @property\n    @abstractmethod\n    def context_encoder(self) -&gt; torch.nn.Module | None:\n        \"\"\"PyTorch model associated with the context encoder associated with retriever.\"\"\"\n</code></pre>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.encoder","title":"encoder  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>encoder\n</code></pre> <p>PyTorch model associated with the encoder associated with retriever.</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.query_encoder","title":"query_encoder  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>query_encoder\n</code></pre> <p>PyTorch model associated with the query encoder associated with retriever.</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.context_encoder","title":"context_encoder  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>context_encoder\n</code></pre> <p>PyTorch model associated with the context encoder associated with retriever.</p>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.encode_query","title":"encode_query  <code>abstractmethod</code>","text":"<pre><code>encode_query(query, **kwargs)\n</code></pre> <p>Encode query.</p> Source code in <code>src/fed_rag/base/retriever.py</code> <pre><code>@abstractmethod\ndef encode_query(\n    self, query: str | list[str], **kwargs: Any\n) -&gt; torch.Tensor:\n    \"\"\"Encode query.\"\"\"\n</code></pre>"},{"location":"api_reference/retrievers/#src.fed_rag.base.retriever.BaseRetriever.encode_context","title":"encode_context  <code>abstractmethod</code>","text":"<pre><code>encode_context(context, **kwargs)\n</code></pre> <p>Encode context.</p> Source code in <code>src/fed_rag/base/retriever.py</code> <pre><code>@abstractmethod\ndef encode_context(\n    self, context: str | list[str], **kwargs: Any\n) -&gt; torch.Tensor:\n    \"\"\"Encode context.\"\"\"\n</code></pre>"},{"location":"api_reference/retrievers/huggingface/","title":"Huggingface","text":"<p>HuggingFace SentenceTransformer Retriever</p>"},{"location":"api_reference/retrievers/huggingface/#src.fed_rag.retrievers.hf_sentence_transformer.HFSentenceTransformerRetriever","title":"HFSentenceTransformerRetriever","text":"<p>               Bases: <code>BaseRetriever</code></p> Source code in <code>src/fed_rag/retrievers/hf_sentence_transformer.py</code> <pre><code>class HFSentenceTransformerRetriever(BaseRetriever):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str | None = Field(\n        description=\"Name of HuggingFace SentenceTransformer model.\",\n        default=None,\n    )\n    query_model_name: str | None = Field(\n        description=\"Name of HuggingFace SentenceTransformer model used for encoding queries.\",\n        default=None,\n    )\n    context_model_name: str | None = Field(\n        description=\"Name of HuggingFace SentenceTransformer model used for encoding context.\",\n        default=None,\n    )\n    load_model_kwargs: LoadKwargs = Field(\n        description=\"Optional kwargs dict for loading models from HF. Defaults to None.\",\n        default_factory=LoadKwargs,\n    )\n    _encoder: SentenceTransformer | None = PrivateAttr(default=None)\n    _query_encoder: SentenceTransformer | None = PrivateAttr(default=None)\n    _context_encoder: SentenceTransformer | None = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str | None = None,\n        query_model_name: str | None = None,\n        context_model_name: str | None = None,\n        load_model_kwargs: LoadKwargs | dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        if isinstance(load_model_kwargs, dict):\n            # use same dict for all\n            load_model_kwargs = LoadKwargs(\n                encoder=load_model_kwargs,\n                query_encoder=load_model_kwargs,\n                context_encoder=load_model_kwargs,\n            )\n\n        load_model_kwargs = (\n            load_model_kwargs if load_model_kwargs else LoadKwargs()\n        )\n\n        super().__init__(\n            model_name=model_name,\n            query_model_name=query_model_name,\n            context_model_name=context_model_name,\n            load_model_kwargs=load_model_kwargs,\n        )\n        if load_model_at_init:\n            if model_name:\n                self._encoder = self._load_model_from_hf(load_type=\"encoder\")\n            else:\n                self._query_encoder = self._load_model_from_hf(\n                    load_type=\"query_encoder\"\n                )\n                self._context_encoder = self._load_model_from_hf(\n                    load_type=\"context_encoder\"\n                )\n\n    def _load_model_from_hf(\n        self,\n        load_type: Literal[\"encoder\", \"query_encoder\", \"context_encoder\"],\n        **kwargs: Any,\n    ) -&gt; SentenceTransformer:\n        if load_type == \"encoder\":\n            load_kwargs = self.load_model_kwargs.encoder\n            load_kwargs.update(kwargs)\n            return SentenceTransformer(self.model_name, **load_kwargs)\n        elif load_type == \"context_encoder\":\n            load_kwargs = self.load_model_kwargs.context_encoder\n            load_kwargs.update(kwargs)\n            return SentenceTransformer(self.context_model_name, **load_kwargs)\n        elif load_type == \"query_encoder\":\n            load_kwargs = self.load_model_kwargs.query_encoder\n            load_kwargs.update(kwargs)\n            return SentenceTransformer(self.query_model_name, **load_kwargs)\n        else:\n            raise InvalidLoadType(\"Invalid `load_type` supplied.\")\n\n    def encode_context(\n        self, context: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        # validation guarantees one of these is not None\n        encoder = self.encoder if self.encoder else self.context_encoder\n        encoder = cast(SentenceTransformer, encoder)\n\n        return encoder.encode(context)\n\n    def encode_query(\n        self, query: str | list[str], **kwargs: Any\n    ) -&gt; torch.Tensor:\n        # validation guarantees one of these is not None\n        encoder = self.encoder if self.encoder else self.query_encoder\n        encoder = cast(SentenceTransformer, encoder)\n\n        return encoder.encode(query)\n\n    @property\n    def encoder(self) -&gt; SentenceTransformer | None:\n        if self.model_name and self._encoder is None:\n            self._encoder = self._load_model_from_hf(load_type=\"encoder\")\n        return self._encoder\n\n    @property\n    def query_encoder(self) -&gt; SentenceTransformer | None:\n        if self.query_model_name and self._query_encoder is None:\n            self._query_encoder = self._load_model_from_hf(\n                load_type=\"query_encoder\"\n            )\n        return self._query_encoder\n\n    @property\n    def context_encoder(self) -&gt; SentenceTransformer | None:\n        if self.context_model_name and self._context_encoder is None:\n            self._context_encoder = self._load_model_from_hf(\n                load_type=\"context_encoder\"\n            )\n        return self._context_encoder\n</code></pre>"},{"location":"api_reference/tokenizers/","title":"Tokenizers","text":"<p>Base Tokenizer</p>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer","title":"BaseTokenizer","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base Tokenizer Class.</p> Source code in <code>src/fed_rag/base/tokenizer.py</code> <pre><code>class BaseTokenizer(BaseModel, ABC):\n    \"\"\"Base Tokenizer Class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def encode(self, input: str, **kwargs: dict) -&gt; list[int]:\n        \"\"\"Encode the input string into list of integers.\"\"\"\n\n    @abstractmethod\n    def decode(self, input_ids: str, **kwargs: dict) -&gt; str:\n        \"\"\"Decode the input token ids into a string.\"\"\"\n\n    @property\n    @abstractmethod\n    def unwrapped(self) -&gt; Any:\n        \"\"\"Return the underlying tokenizer if there is one.\"\"\"\n</code></pre>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer.unwrapped","title":"unwrapped  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>unwrapped\n</code></pre> <p>Return the underlying tokenizer if there is one.</p>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer.encode","title":"encode  <code>abstractmethod</code>","text":"<pre><code>encode(input, **kwargs)\n</code></pre> <p>Encode the input string into list of integers.</p> Source code in <code>src/fed_rag/base/tokenizer.py</code> <pre><code>@abstractmethod\ndef encode(self, input: str, **kwargs: dict) -&gt; list[int]:\n    \"\"\"Encode the input string into list of integers.\"\"\"\n</code></pre>"},{"location":"api_reference/tokenizers/#src.fed_rag.base.tokenizer.BaseTokenizer.decode","title":"decode  <code>abstractmethod</code>","text":"<pre><code>decode(input_ids, **kwargs)\n</code></pre> <p>Decode the input token ids into a string.</p> Source code in <code>src/fed_rag/base/tokenizer.py</code> <pre><code>@abstractmethod\ndef decode(self, input_ids: str, **kwargs: dict) -&gt; str:\n    \"\"\"Decode the input token ids into a string.\"\"\"\n</code></pre>"},{"location":"api_reference/tokenizers/huggingface/","title":"Huggingface","text":"<p>HuggingFace PretrainedTokenizer</p>"},{"location":"api_reference/tokenizers/huggingface/#src.fed_rag.tokenizers.hf_pretrained_tokenizer.HFPretrainedTokenizer","title":"HFPretrainedTokenizer","text":"<p>               Bases: <code>BaseTokenizer</code></p> Source code in <code>src/fed_rag/tokenizers/hf_pretrained_tokenizer.py</code> <pre><code>class HFPretrainedTokenizer(BaseTokenizer):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: str = Field(\n        description=\"Name of HuggingFace model. Used for loading the model from HF hub or local.\"\n    )\n    load_model_kwargs: dict = Field(\n        description=\"Optional kwargs dict for loading models from HF. Defaults to None.\",\n        default_factory=dict,\n    )\n    _tokenizer: Optional[\"PreTrainedTokenizer\"] = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model_name: str,\n        load_model_kwargs: dict | None = None,\n        load_model_at_init: bool = True,\n    ):\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise ValueError(msg)\n        super().__init__(\n            model_name=model_name,\n            load_model_kwargs=load_model_kwargs if load_model_kwargs else {},\n        )\n        if load_model_at_init:\n            self._tokenizer = self._load_model_from_hf()\n\n    def _load_model_from_hf(self, **kwargs: Any) -&gt; \"PreTrainedTokenizer\":\n        load_kwargs = self.load_model_kwargs\n        load_kwargs.update(kwargs)\n        self.load_model_kwargs = load_kwargs\n        return AutoTokenizer.from_pretrained(self.model_name, **load_kwargs)\n\n    @property\n    def unwrapped(self) -&gt; \"PreTrainedTokenizer\":\n        if self._tokenizer is None:\n            # load HF Pretrained Tokenizer\n            tokenizer = self._load_model_from_hf()\n            self._tokenizer = tokenizer\n        return self._tokenizer\n\n    @unwrapped.setter\n    def unwrapped(self, value: \"PreTrainedTokenizer\") -&gt; None:\n        self._tokenizer = value\n\n    def encode(self, input: str, **kwargs: Any) -&gt; list[int]:\n        return self.unwrapped(text=input, **kwargs)  # type: ignore[no-any-return]\n\n    def decode(self, input_ids: list[int], **kwargs: Any) -&gt; str:\n        return self.unwrapped.decode(token_ids=input_ids, **kwargs)  # type: ignore[no-any-return]\n</code></pre>"}]}