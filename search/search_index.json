{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/decorators/","title":"Index","text":"<p>Trainer Decorators</p> <p>Tester Decorators</p>"},{"location":"api_reference/decorators/#src.fed_rag.decorators.trainer.TrainerDecorators","title":"<code>TrainerDecorators</code>","text":"Source code in <code>src/fed_rag/decorators/trainer.py</code> <pre><code>class TrainerDecorators:\n    def pytorch(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.pytorch import inspect_trainer_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_trainer_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_trainer_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n\n    def huggingface(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.huggingface import inspect_trainer_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_trainer_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_trainer_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n</code></pre>"},{"location":"api_reference/decorators/#src.fed_rag.decorators.tester.TesterDecorators","title":"<code>TesterDecorators</code>","text":"Source code in <code>src/fed_rag/decorators/tester.py</code> <pre><code>class TesterDecorators:\n    def pytorch(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.pytorch import inspect_tester_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_tester_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_tester_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n\n    def huggingface(self, func: Callable) -&gt; Callable:\n        from fed_rag.inspectors.huggingface import inspect_tester_signature\n\n        def decorator(func: Callable) -&gt; Callable:\n            # inspect func sig\n            spec = inspect_tester_signature(\n                func\n            )  # may need to create a cfg for this if decorater accepts params\n\n            # store fl_task config\n            func.__setattr__(\"__fl_task_tester_config\", spec)  # type: ignore[attr-defined]\n\n            return func\n\n        return decorator(func)\n</code></pre>"},{"location":"api_reference/fl_tasks/","title":"Base FL Task Classes","text":"<p>Base FL Task</p>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask","title":"<code>BaseFLTask</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>class BaseFLTask(BaseModel, ABC):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def training_loop(self) -&gt; Callable:\n        ...\n\n    @classmethod\n    @abstractmethod\n    def from_configs(\n        cls, trainer_cfg: BaseFLTaskConfig, tester_cfg: Any\n    ) -&gt; Self:\n        ...\n\n    @classmethod\n    @abstractmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        try:\n            trainer_cfg = getattr(trainer, \"__fl_task_trainer_config\")\n        except AttributeError:\n            msg = (\n                \"`__fl_task_trainer_config` has not been set on training loop. Make \"\n                \"sure to decorate your training loop with the appropriate \"\n                \"decorator.\"\n            )\n            raise MissingFLTaskConfig(msg)\n\n        try:\n            tester_cfg = getattr(tester, \"__fl_task_tester_config\")\n        except AttributeError:\n            msg = (\n                \"`__fl_task_tester_config` has not been set on tester callable. Make \"\n                \"sure to decorate your tester with the appropriate decorator.\"\n            )\n            raise MissingFLTaskConfig(msg)\n        return cls.from_configs(trainer_cfg, tester_cfg)\n\n    @abstractmethod\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        \"\"\"Simulate the FL task.\n\n        Either use flwr's simulation tools, or create our own here.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def server(self, **kwargs: Any) -&gt; Server:\n        \"\"\"Create a flwr.Server object.\"\"\"\n        ...\n\n    @abstractmethod\n    def client(self, **kwargs: Any) -&gt; Client:\n        \"\"\"Create a flwr.Client object.\"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.simulate","title":"<code>simulate(num_clients, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Simulate the FL task.</p> <p>Either use flwr's simulation tools, or create our own here.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n    \"\"\"Simulate the FL task.\n\n    Either use flwr's simulation tools, or create our own here.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.server","title":"<code>server(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create a flwr.Server object.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef server(self, **kwargs: Any) -&gt; Server:\n    \"\"\"Create a flwr.Server object.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTask.client","title":"<code>client(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create a flwr.Client object.</p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>@abstractmethod\ndef client(self, **kwargs: Any) -&gt; Client:\n    \"\"\"Create a flwr.Client object.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/fl_tasks/#src.fed_rag.base.fl_task.BaseFLTaskConfig","title":"<code>BaseFLTaskConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/fed_rag/base/fl_task.py</code> <pre><code>class BaseFLTaskConfig(BaseModel):\n    pass\n</code></pre>"},{"location":"api_reference/fl_tasks/huggingface/","title":"Huggingface","text":"<p>HuggingFace FL Task.</p> <p>NOTE: Using this module requires the <code>huggingface</code> extra to be installed.</p>"},{"location":"api_reference/fl_tasks/huggingface/#src.fed_rag.fl_tasks.huggingface.HuggingFaceFlowerClient","title":"<code>HuggingFaceFlowerClient</code>","text":"<p>               Bases: <code>NumPyClient</code></p> Source code in <code>src/fed_rag/fl_tasks/huggingface.py</code> <pre><code>class HuggingFaceFlowerClient(NumPyClient):\n    def __init__(\n        self,\n        task_bundle: BaseFLTaskBundle,\n    ) -&gt; None:\n        super().__init__()\n        self.task_bundle = task_bundle\n\n    def __getattr__(self, name: str) -&gt; Any:\n        if name in self.task_bundle.model_fields:\n            return getattr(self.task_bundle, name)\n        else:\n            return super().__getattr__(name)\n\n    def get_weights(self) -&gt; NDArrays:\n        return _get_weights(self.net)\n\n    def set_weights(self, parameters: NDArrays) -&gt; None:\n        if isinstance(self.net, PeftModel):\n            # get state dict\n            state_dict = get_peft_model_state_dict(self.net)\n            state_dict = cast(Dict[str, Any], state_dict)\n\n            # update state dict with supplied parameters\n            params_dict = zip(state_dict.keys(), parameters)\n            state_dict = OrderedDict(\n                {k: torch.tensor(v) for k, v in params_dict}\n            )\n            set_peft_model_state_dict(self.net, state_dict)\n        else:  # SentenceTransformer | PreTrainedModel\n            # get state dict\n            state_dict = self.net.state_dict()\n\n            # update state dict with supplied parameters\n            params_dict = zip(state_dict.keys(), parameters)\n            state_dict = OrderedDict(\n                {k: torch.tensor(v) for k, v in params_dict}\n            )\n            self.net.load_state_dict(state_dict, strict=True)\n\n    def fit(\n        self, parameters: NDArrays, config: dict[str, Scalar]\n    ) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n        self.set_weights(parameters)\n\n        result: TrainResult = self.trainer(\n            self.net,\n            self.train_dataset,\n            self.val_dataset,\n            **self.task_bundle.extra_train_kwargs,\n        )\n        return (\n            self.get_weights(),\n            len(self.train_dataset),\n            {\"loss\": result.loss},\n        )\n\n    def evaluate(\n        self, parameters: NDArrays, config: dict[str, Scalar]\n    ) -&gt; tuple[float, int, dict[str, Scalar]]:\n        self.set_weights(parameters)\n        result: TestResult = self.tester(\n            self.net, self.val_dataset, **self.extra_test_kwargs\n        )\n        return result.loss, len(self.val_dataset), result.metrics\n</code></pre>"},{"location":"api_reference/fl_tasks/huggingface/#src.fed_rag.fl_tasks.huggingface.HuggingFaceFLTask","title":"<code>HuggingFaceFLTask</code>","text":"<p>               Bases: <code>BaseFLTask</code></p> Source code in <code>src/fed_rag/fl_tasks/huggingface.py</code> <pre><code>class HuggingFaceFLTask(BaseFLTask):\n    _client: NumPyClient = PrivateAttr()\n    _trainer: Callable = PrivateAttr()\n    _trainer_spec: TrainerSignatureSpec = PrivateAttr()\n    _tester: Callable = PrivateAttr()\n    _tester_spec: TesterSignatureSpec = PrivateAttr()\n\n    def __init__(\n        self,\n        trainer: Callable,\n        trainer_spec: TrainerSignatureSpec,\n        tester: Callable,\n        tester_spec: TesterSignatureSpec,\n        **kwargs: Any,\n    ) -&gt; None:\n        if not _has_huggingface:\n            msg = (\n                f\"`{self.__class__.__name__}` requires `huggingface` extra to be installed. \"\n                \"To fix please run `pip install fed-rag[huggingface]`.\"\n            )\n            raise ValueError(msg)\n\n        if (\n            trainer_spec.net_parameter_class_name\n            != tester_spec.net_parameter_class_name\n        ):\n            msg = (\n                \"`trainer`'s model class is not the same as that for `tester`.\"\n            )\n            raise NetTypeMismatch(msg)\n\n        if trainer_spec.net_parameter != tester_spec.net_parameter:\n            msg = (\n                \"`trainer`'s model parameter name is not the same as that for `tester`. \"\n                \"Will use the name supplied in `trainer`.\"\n            )\n            warnings.warn(msg, UnequalNetParamWarning)\n\n        super().__init__(**kwargs)\n        self._trainer = trainer\n        self._trainer_spec = trainer_spec\n        self._tester = tester\n        self._tester_spec = tester_spec\n\n    @property\n    def training_loop(self) -&gt; Callable:\n        return self._trainer\n\n    @classmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        # extract trainer spec\n        try:\n            trainer_spec: TrainerSignatureSpec = getattr(\n                trainer, \"__fl_task_trainer_config\"\n            )\n        except AttributeError:\n            msg = \"Cannot extract `TrainerSignatureSpec` from supplied `trainer`.\"\n            raise MissingTrainerSpec(msg)\n\n        # extract tester spec\n        try:\n            tester_spec: TesterSignatureSpec = getattr(\n                tester, \"__fl_task_tester_config\"\n            )\n        except AttributeError:\n            msg = (\n                \"Cannot extract `TesterSignatureSpec` from supplied `tester`.\"\n            )\n            raise MissingTesterSpec(msg)\n\n        return cls(\n            trainer=trainer,\n            trainer_spec=trainer_spec,\n            tester=tester,\n            tester_spec=tester_spec,\n        )\n\n    def server(\n        self,\n        strategy: Strategy | None = None,\n        client_manager: ClientManager | None = None,\n        **kwargs: Any,\n    ) -&gt; HuggingFaceFlowerServer | None:\n        if strategy is None:\n            if self._trainer_spec.net_parameter not in kwargs:\n                msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n                raise MissingRequiredNetParam(msg)\n\n            model = kwargs.pop(self._trainer_spec.net_parameter)\n\n            ndarrays = _get_weights(model)\n            parameters = ndarrays_to_parameters(ndarrays)\n            strategy = FedAvg(\n                fraction_evaluate=1.0,\n                initial_parameters=parameters,\n            )\n\n        if client_manager is None:\n            client_manager = SimpleClientManager()\n\n        return HuggingFaceFlowerServer(\n            client_manager=client_manager, strategy=strategy\n        )\n\n    def client(self, **kwargs: Any) -&gt; Client | None:\n        # validate kwargs\n        if self._trainer_spec.net_parameter not in kwargs:\n            msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n            raise MissingRequiredNetParam(msg)\n        # build bundle\n        net = kwargs.pop(self._trainer_spec.net_parameter)\n        train_dataset = kwargs.pop(self._trainer_spec.train_data_param)\n        val_dataset = kwargs.pop(self._trainer_spec.val_data_param)\n\n        bundle = BaseFLTaskBundle(\n            net=net,\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            extra_train_kwargs=kwargs,\n            extra_test_kwargs={},  # TODO make this functional or get rid of it\n            trainer=self._trainer,\n            tester=self._tester,\n        )\n        return HuggingFaceFlowerClient(task_bundle=bundle)\n\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        raise NotImplementedError\n\n    @classmethod\n    def from_configs(cls, trainer_cfg: Any, tester_cfg: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/fl_tasks/pytorch/","title":"Pytorch","text":"<p>PyTorch FL Task</p>"},{"location":"api_reference/fl_tasks/pytorch/#src.fed_rag.fl_tasks.pytorch.PyTorchFLTask","title":"<code>PyTorchFLTask</code>","text":"<p>               Bases: <code>BaseFLTask</code></p> Source code in <code>src/fed_rag/fl_tasks/pytorch.py</code> <pre><code>class PyTorchFLTask(BaseFLTask):\n    _client: NumPyClient = PrivateAttr()\n    _trainer: Callable = PrivateAttr()\n    _trainer_spec: TrainerSignatureSpec = PrivateAttr()\n    _tester: Callable = PrivateAttr()\n    _tester_spec: TesterSignatureSpec = PrivateAttr()\n\n    def __init__(\n        self,\n        trainer: Callable,\n        trainer_spec: TrainerSignatureSpec,\n        tester: Callable,\n        tester_spec: TesterSignatureSpec,\n        **kwargs: Any,\n    ) -&gt; None:\n        if trainer_spec.net_parameter != tester_spec.net_parameter:\n            msg = (\n                \"`trainer`'s model parameter name is not the same as that for `tester`. \"\n                \"Will use the name supplied in `trainer`.\"\n            )\n            warnings.warn(msg, UnequalNetParamWarning)\n\n        super().__init__(**kwargs)\n        self._trainer = trainer\n        self._trainer_spec = trainer_spec\n        self._tester = tester\n        self._tester_spec = tester_spec\n\n    @property\n    def training_loop(self) -&gt; Callable:\n        return self._trainer\n\n    @classmethod\n    def from_trainer_and_tester(\n        cls, trainer: Callable, tester: Callable\n    ) -&gt; Self:\n        # extract trainer spec\n        try:\n            trainer_spec: TrainerSignatureSpec = getattr(\n                trainer, \"__fl_task_trainer_config\"\n            )\n        except AttributeError:\n            msg = \"Cannot extract `TrainerSignatureSpec` from supplied `trainer`.\"\n            raise MissingTrainerSpec(msg)\n\n        # extract tester spec\n        try:\n            tester_spec: TesterSignatureSpec = getattr(\n                tester, \"__fl_task_tester_config\"\n            )\n        except AttributeError:\n            msg = (\n                \"Cannot extract `TesterSignatureSpec` from supplied `tester`.\"\n            )\n            raise MissingTesterSpec(msg)\n\n        return cls(\n            trainer=trainer,\n            trainer_spec=trainer_spec,\n            tester=tester,\n            tester_spec=tester_spec,\n        )\n\n    @classmethod\n    def from_configs(cls, trainer_cfg: Any, tester_cfg: Any) -&gt; Any:\n        return super().from_configs(trainer_cfg, tester_cfg)\n\n    def server(\n        self,\n        strategy: Strategy | None = None,\n        client_manager: ClientManager | None = None,\n        **kwargs: Any,\n    ) -&gt; PyTorchFlowerServer | None:\n        if strategy is None:\n            if self._trainer_spec.net_parameter not in kwargs:\n                msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n                raise MissingRequiredNetParam(msg)\n\n            model = kwargs.pop(self._trainer_spec.net_parameter)\n\n            ndarrays = _get_weights(model)\n            parameters = ndarrays_to_parameters(ndarrays)\n            strategy = FedAvg(\n                fraction_evaluate=1.0,\n                initial_parameters=parameters,\n            )\n\n        if client_manager is None:\n            client_manager = SimpleClientManager()\n\n        return PyTorchFlowerServer(\n            client_manager=client_manager, strategy=strategy\n        )\n\n    def client(self, **kwargs: Any) -&gt; Client | None:\n        # validate kwargs\n        if self._trainer_spec.net_parameter not in kwargs:\n            msg = f\"Please pass in a model using the model param name {self._trainer_spec.net_parameter}.\"\n            raise MissingRequiredNetParam(msg)\n        # build bundle\n        net = kwargs.pop(self._trainer_spec.net_parameter)\n        trainloader = kwargs.pop(self._trainer_spec.train_data_param)\n        valloader = kwargs.pop(self._trainer_spec.val_data_param)\n\n        bundle = BaseFLTaskBundle(\n            net=net,\n            trainloader=trainloader,\n            valloader=valloader,\n            extra_train_kwargs=kwargs,\n            extra_test_kwargs={},  # TODO make this functional or get rid of it\n            trainer=self._trainer,\n            tester=self._tester,\n        )\n        return PyTorchFlowerClient(task_bundle=bundle)\n\n    def simulate(self, num_clients: int, **kwargs: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"}]}